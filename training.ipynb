{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Hierarchical Training v2.0 - Educational Architecture\n",
    "\n",
    "**Purpose**: Train hierarchical concept learner with transparent, layer-based API.\n",
    "\n",
    "## Key Changes in v2.0\n",
    "\n",
    "### Architecture\n",
    "- **Full text processing**: Take complete text ‚Üí tokenize ‚Üí chunk ‚Üí feed to node0\n",
    "- **Natural abstraction**: Let hierarchy learn naturally (no forced segmentation)\n",
    "- **Pattern name flow**: Explicit flow between layers visible to users\n",
    "\n",
    "### API Design\n",
    "- **TensorFlow/PyTorch-style**: Use `add_layer()` to build hierarchy\n",
    "- **Explicit KATO calls**: Show `observe()`, `observe_sequence()`, `learn()`, `get_predictions()`\n",
    "- **Educational focus**: Users see exactly what's happening\n",
    "\n",
    "### Configuration\n",
    "- **Flexible metadata**: Configurable which layers capture source metadata\n",
    "- **Per-layer settings**: Chunk size, max predictions, recall threshold, STM mode\n",
    "- **Transparent**: All settings visible in notebook\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to configure hierarchical layers explicitly\n",
    "2. How pattern names flow between layers\n",
    "3. How KATO API calls work (observe, learn, predict)\n",
    "4. How to handle metadata at specific layers\n",
    "5. How to process full documents through the hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets transformers requests numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from tools.hierarchical_builder import (\n",
    "    HierarchicalBuilder,\n",
    "    process_chunk_at_layer,\n",
    "    accumulate_in_stm,\n",
    "    learn_from_stm,\n",
    "    extract_prediction_field\n",
    ")\n",
    "\n",
    "# For profiling and analysis\n",
    "from tools import (\n",
    "    ProfilingEngine,\n",
    "    StreamingDatasetLoader\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì All modules imported successfully\")\n",
    "print(\"‚úì Ready for hierarchical training v2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Service Configuration\n",
    "\n",
    "Configure KATO server URL.\n",
    "\n",
    "**Note**: KATO has migrated from MongoDB to ClickHouse + Redis (Nov 2025). The KATO API remains backward compatible, so training works as before. Post-training analysis tools are being updated.\n",
    "\n",
    "**Multi-machine support**: Change KATO_URL if running KATO on a separate machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SERVICE CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "KATO_URL = 'http://kato:8000'  # KATO server\n",
    "\n",
    "# NOTE: MongoDB has been removed from KATO (migrated to ClickHouse + Redis)\n",
    "# KATO API remains backward compatible - training will work as before\n",
    "# Post-training analysis tools are being updated for the new storage backend\n",
    "\n",
    "# For multi-machine setups:\n",
    "# KATO_URL = 'http://192.168.1.100:8000'\n",
    "\n",
    "print(\"‚úì Service URLs configured\")\n",
    "print(f\"  KATO: {KATO_URL}\")\n",
    "print(\"  Note: KATO now uses ClickHouse + Redis (MongoDB removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VERIFY KATO SERVER CONNECTION\n",
    "# ========================================\n",
    "\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def check_kato_server(url):\n",
    "    \"\"\"Check if KATO server is responding.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úì KATO server is healthy at {url}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  KATO server responded with status {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"‚úó Cannot connect to KATO server at {url}\")\n",
    "        print(f\"  Make sure KATO is running (check: docker ps | grep kato)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error checking KATO server: {e}\")\n",
    "        return False\n",
    "\n",
    "print(f\"Checking KATO server at {KATO_URL}...\")\n",
    "if check_kato_server(KATO_URL):\n",
    "    print(f\"\\n‚úì Ready to begin training!\")\n",
    "    print(f\"  Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Training will fail without KATO server\")\n",
    "    print(f\"  Please start KATO before continuing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure dataset and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Dataset\n",
    "DATASET_KEY = 'wikitext'  # Options: 'c4', 'refinedweb', 'wikitext', 'openwebtext'\n",
    "MAX_SAMPLES = 100000  # Start small for testing, then scale up\n",
    "CHUNK_SIZES = [8, 8, 8, 8]      # [node0, node1, node2, node3]\n",
    "\n",
    "# Workers (for parallel training)\n",
    "NUM_WORKERS = 1  # Start with 1 for educational single-threaded mode\n",
    "\n",
    "# Checkpoint configuration\n",
    "CHECKPOINT_INTERVAL = 1000  # Save checkpoint every N samples\n",
    "RESUME_FROM_CHECKPOINT = False  # Set True to resume interrupted training\n",
    "\n",
    "print(\"‚úì Training configuration set\")\n",
    "print(f\"  Dataset: {DATASET_KEY}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES:,}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "print(f\"  Checkpoint interval: {CHECKPOINT_INTERVAL:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Layer Configuration (NEW!)\n",
    "\n",
    "**TensorFlow/PyTorch-style API**: Build hierarchy by adding layers.\n",
    "\n",
    "**Key Parameters**:\n",
    "- `chunk_size`: How many inputs per chunk\n",
    "- `max_predictions`: Top N predictions to pass to next layer\n",
    "- `prediction_field`: Which field to extract ('name')\n",
    "- `recall_threshold`: Pattern matching strictness (0.0-1.0)\n",
    "- `capture_metadata`: Whether this layer captures source metadata (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hierarchy with explicit layer configuration\n",
    "hierarchy = HierarchicalBuilder(\n",
    "    tokenizer_name='gpt2',\n",
    "    base_url=KATO_URL\n",
    ")\n",
    "\n",
    "# Add node0: Chunk-level patterns\n",
    "hierarchy.add_layer(\n",
    "    name='node0',\n",
    "    chunk_size=CHUNK_SIZES[0],\n",
    "    max_predictions=10,\n",
    "    prediction_field='name',\n",
    "    recall_threshold=0.6,\n",
    "    stm_mode='CLEAR',\n",
    "    max_pattern_length=0,\n",
    "    process_predictions=False,\n",
    "    capture_metadata=False  # Don't capture metadata here\n",
    ")\n",
    "\n",
    "# Add node1: Paragraph-level patterns\n",
    "hierarchy.add_layer(\n",
    "    name='node1',\n",
    "    chunk_size=CHUNK_SIZES[1],\n",
    "    max_predictions=8,\n",
    "    prediction_field='name',\n",
    "    recall_threshold=0.6,\n",
    "    stm_mode='CLEAR',\n",
    "    max_pattern_length=0,\n",
    "    process_predictions=False,\n",
    "    capture_metadata=False  # Don't capture metadata here either\n",
    ")\n",
    "\n",
    "# Add node2: Chapter-level patterns (capture metadata)\n",
    "hierarchy.add_layer(\n",
    "    name='node2',\n",
    "    chunk_size=CHUNK_SIZES[2],\n",
    "    max_predictions=6,\n",
    "    prediction_field='name',\n",
    "    recall_threshold=0.6,\n",
    "    stm_mode='CLEAR',\n",
    "    max_pattern_length=0,\n",
    "    process_predictions=False,\n",
    "    capture_metadata=True  # START capturing metadata at this layer\n",
    ")\n",
    "\n",
    "# Add node3: Book-level patterns (capture metadata)\n",
    "hierarchy.add_layer(\n",
    "    name='node3',\n",
    "    chunk_size=CHUNK_SIZES[3],\n",
    "    max_predictions=4,\n",
    "    prediction_field='name',\n",
    "    recall_threshold=0.6,\n",
    "    stm_mode='CLEAR',\n",
    "    max_pattern_length=0,\n",
    "    process_predictions=False,\n",
    "    capture_metadata=True  # Capture metadata here too\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = hierarchy.build(verbose=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explicit KATO Helper Functions (Educational)\n",
    "\n",
    "These functions show the EXACT KATO API calls being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_sample(text, metadata=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Process one text sample through the hierarchy.\n",
    "    \n",
    "    Shows explicit KATO calls at each step.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING SAMPLE\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Step 1: Tokenize\n",
    "    tokens = model.tokenize(text)\n",
    "    if verbose:\n",
    "        print(f\"\\n1. Tokenized: {len(tokens)} tokens\")\n",
    "        print(f\"   First 10: {tokens[:10]}\")\n",
    "    \n",
    "    # Step 2: Chunk tokens for node0\n",
    "    chunks = model.chunk_tokens(tokens, model.layers[0].chunk_size)\n",
    "    if verbose:\n",
    "        print(f\"\\n2. Chunked into {len(chunks)} chunks (size={model.layers[0].chunk_size})\")\n",
    "    \n",
    "    # Step 3: Process each chunk at node0\n",
    "    if verbose:\n",
    "        print(f\"\\n3. Processing chunks at node0...\")\n",
    "    \n",
    "    node0_patterns = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # EXPLICIT KATO API CALL\n",
    "        pattern = process_chunk_at_layer(\n",
    "            chunk,\n",
    "            model.layers[0].client,\n",
    "            metadata=None,  # No metadata at node0\n",
    "            verbose=False\n",
    "        )\n",
    "        node0_patterns.append(pattern)\n",
    "        if verbose and i < 3:  # Show first 3\n",
    "            print(f\"   Chunk {i+1}: {pattern[:40]}...\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   ‚Üí node0 produced {len(node0_patterns)} patterns\")\n",
    "    \n",
    "    # Step 4: Send node0 patterns to node1\n",
    "    if verbose:\n",
    "        print(f\"\\n4. Sending {len(node0_patterns)} patterns to node1...\")\n",
    "    \n",
    "    # EXPLICIT KATO API CALL\n",
    "    count = accumulate_in_stm(\n",
    "        node0_patterns,\n",
    "        model.layers[1].client,\n",
    "        metadata=None,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Learn at node1 (sample complete)\n",
    "    # EXPLICIT KATO API CALL\n",
    "    node1_pattern = learn_from_stm(model.layers[1].client, verbose=False)\n",
    "    if verbose:\n",
    "        print(f\"   ‚Üí node1 learned: {node1_pattern[:40]}...\")\n",
    "    \n",
    "    # Step 5: Send node1 pattern to node2 (with metadata if configured)\n",
    "    should_capture_metadata = model.layers[1].should_capture_metadata()\n",
    "    meta = metadata if should_capture_metadata else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n5. Sending node1 pattern to node2...\")\n",
    "        if meta:\n",
    "            print(f\"   ‚Üí Metadata attached: {meta}\")\n",
    "    \n",
    "    # EXPLICIT KATO API CALL\n",
    "    accumulate_in_stm([node1_pattern], model.layers[2].client, metadata=meta, verbose=False)\n",
    "    \n",
    "    # Learn at node2\n",
    "    # EXPLICIT KATO API CALL\n",
    "    node2_pattern = learn_from_stm(model.layers[2].client, verbose=False)\n",
    "    if verbose:\n",
    "        print(f\"   ‚Üí node2 learned: {node2_pattern[:40]}...\")\n",
    "    \n",
    "    # Step 6: Send node2 pattern to node3 (with metadata if configured)\n",
    "    should_capture_metadata = model.layers[2].should_capture_metadata()\n",
    "    meta = metadata if should_capture_metadata else None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n6. Sending node2 pattern to node3...\")\n",
    "    \n",
    "    # EXPLICIT KATO API CALL\n",
    "    accumulate_in_stm([node2_pattern], model.layers[3].client, metadata=meta, verbose=False)\n",
    "    \n",
    "    # Learn at node3\n",
    "    # EXPLICIT KATO API CALL\n",
    "    node3_pattern = learn_from_stm(model.layers[3].client, verbose=False)\n",
    "    if verbose:\n",
    "        print(f\"   ‚Üí node3 learned: {node3_pattern[:40]}...\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SAMPLE COMPLETE\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'node0_patterns': len(node0_patterns),\n",
    "        'node1_pattern': node1_pattern,\n",
    "        'node2_pattern': node2_pattern,\n",
    "        'node3_pattern': node3_pattern\n",
    "    }\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Single-Sample Educational Demo\n",
    "\n",
    "Process ONE sample to see exactly what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one sample for demonstration using static method API\n",
    "stream_iterator = StreamingDatasetLoader.load_streaming(\n",
    "    dataset_key=DATASET_KEY,\n",
    "    max_samples=1\n",
    ")\n",
    "\n",
    "# Get first sample\n",
    "sample = next(iter(stream_iterator))\n",
    "\n",
    "print(f\"Sample text preview:\")\n",
    "print(f\"{sample['text'][:200]}...\")\n",
    "print()\n",
    "\n",
    "# Process with verbose output\n",
    "result = process_text_sample(\n",
    "    sample['text'],\n",
    "    metadata={'source': DATASET_KEY, 'sample_id': 0},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nResult summary:\")\n",
    "print(f\"  node0 patterns: {result['node0_patterns']}\")\n",
    "print(f\"  node1 pattern: {result['node1_pattern'][:50]}...\")\n",
    "print(f\"  node2 pattern: {result['node2_pattern'][:50]}...\")\n",
    "print(f\"  node3 pattern: {result['node3_pattern'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Training (Resilient to Service Restarts)\n",
    "\n",
    "### Automatic Retry Behavior\n",
    "\n",
    "The `KATOClient` (v3.6.0+) now includes **production-grade retry logic** that handles KATO service restarts automatically:\n",
    "\n",
    "**What Happens**:\n",
    "1. **Service Restarts**: KATO restarts every ~10,000 requests (uvicorn `--limit-max-requests`) to prevent memory leaks\n",
    "2. **Auto-Detection**: Client detects `ConnectionError` when service is restarting\n",
    "3. **Health Check**: Waits for service to become healthy (up to 30 seconds)\n",
    "4. **Session Recreation**: Creates new session with same configuration\n",
    "5. **Transparent Retry**: Retries failed request automatically\n",
    "\n",
    "**You'll See** (when restart occurs):\n",
    "```\n",
    "‚ö†Ô∏è  KATO service connection lost (attempt 1/3)\n",
    "   Likely cause: Service restart (uvicorn --limit-max-requests)\n",
    "   Waiting for service to become healthy...\n",
    "   ‚úì Service healthy, recreating session and retrying...\n",
    "```\n",
    "\n",
    "**No Action Required**: Training continues automatically!\n",
    "\n",
    "**Configuration**:\n",
    "- Max retry attempts: 3\n",
    "- Health check timeout: 30 seconds\n",
    "- Exponential backoff: 0.5s ‚Üí 1s ‚Üí 2s\n",
    "\n",
    "**Technical Details**:\n",
    "- See `kato_client.py::_request()` for implementation\n",
    "- See `kato_client.py::_wait_for_kato_healthy()` for health check logic\n",
    "- Connection retry is separate from session recreation (404 errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all STM before batch training\n",
    "model.clear_all_stm()\n",
    "\n",
    "# Training statistics\n",
    "stats = {\n",
    "    'samples_processed': 0,\n",
    "    'total_tokens': 0,\n",
    "    'node0_patterns': 0,\n",
    "    'service_restarts': 0,\n",
    "    'errors': 0\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BATCH TRAINING WITH AUTO-RETRY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(f\"‚ÑπÔ∏è  KATOClient now handles service restarts automatically!\")\n",
    "print(f\"   - Detects connection failures (service restart)\")\n",
    "print(f\"   - Waits for service health check (up to 30s)\")\n",
    "print(f\"   - Recreates session and retries transparently\")\n",
    "print(f\"   - You'll see status messages if restarts occur\\n\")\n",
    "\n",
    "# Load streaming dataset using static method API\n",
    "# Note: Initial streaming info will be displayed, then clean progress bar\n",
    "stream_iterator = StreamingDatasetLoader.load_streaming(\n",
    "    dataset_key=DATASET_KEY,\n",
    "    max_samples=MAX_SAMPLES\n",
    ")\n",
    "\n",
    "# Process samples with clean progress bar\n",
    "# NOTE: Retry logic is now built into KATOClient - no manual handling needed!\n",
    "for i, sample in enumerate(tqdm(stream_iterator, total=MAX_SAMPLES, desc=\"Training\", unit=\"sample\")):\n",
    "    try:\n",
    "        # Process sample (verbose=False for batch mode)\n",
    "        # If KATO restarts (every ~10k requests), KATOClient will handle it automatically\n",
    "        result = process_text_sample(\n",
    "            sample['text'],\n",
    "            metadata={'source': DATASET_KEY, 'sample_id': i},\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Update stats\n",
    "        stats['samples_processed'] += 1\n",
    "        stats['node0_patterns'] += result['node0_patterns']\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Catch any errors that couldn't be auto-recovered\n",
    "        print(f\"\\n‚ö†Ô∏è  Error processing sample {i+1}: {str(e)[:150]}\")\n",
    "        stats['errors'] += 1\n",
    "        \n",
    "        # If too many errors, abort\n",
    "        if stats['errors'] > 10:\n",
    "            print(f\"\\n‚úó Too many errors ({stats['errors']}), aborting training\")\n",
    "            print(f\"  Check KATO server: docker logs kato --tail 50\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Samples processed: {stats['samples_processed']:,}\")\n",
    "print(f\"  Node0 patterns: {stats['node0_patterns']:,}\")\n",
    "if stats['samples_processed'] > 0:\n",
    "    print(f\"  Avg patterns/sample: {stats['node0_patterns'] / stats['samples_processed']:.1f}\")\n",
    "if stats['errors'] > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Failed samples: {stats['errors']}\")\n",
    "    \n",
    "print(f\"\\n‚ÑπÔ∏è  Note: KATO service restarts every ~10,000 requests (uvicorn limit)\")\n",
    "print(f\"   This is normal and handled automatically by KATOClient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Complete\n",
    "\n",
    "Training is done! Patterns have been learned and stored in KATO.\n",
    "\n",
    "**Note**: Pattern analysis temporarily unavailable during KATO storage migration (MongoDB ‚Üí ClickHouse + Redis). New analysis tools coming soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Pattern Analysis Status:\")\n",
    "print(\"   ‚Ä¢ Patterns successfully stored in KATO\")\n",
    "print(\"   ‚Ä¢ KATO storage: ClickHouse + Redis (MongoDB removed)\")\n",
    "print(\"   ‚Ä¢ Post-training analysis tools coming soon\")\n",
    "\n",
    "print(\"\\nüîç Next Steps:\")\n",
    "print(\"   ‚Ä¢ Use generation.ipynb to test text generation with learned patterns\")\n",
    "print(\"   ‚Ä¢ Try different prompts to see hierarchical predictions in action\")\n",
    "print(\"   ‚Ä¢ Experiment with generation parameters\")\n",
    "\n",
    "print(\"\\nüìù Note:\")\n",
    "print(\"   Pattern counting and frequency analysis temporarily unavailable.\")\n",
    "print(\"   New analysis tools for ClickHouse + Redis are being developed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting: KATO Server Issues\n",
    "\n",
    "### ‚úÖ Good News: Most Issues Auto-Resolve!\n",
    "\n",
    "**KATOClient (v3.6.0+)** automatically handles:\n",
    "- ‚úÖ Service restarts (every ~10k requests)\n",
    "- ‚úÖ Connection failures (detects and waits for healthy service)\n",
    "- ‚úÖ Session recreation (recreates with same config)\n",
    "- ‚úÖ Exponential backoff retry (3 attempts, 30s health check)\n",
    "\n",
    "**You should see** automatic recovery messages like:\n",
    "```\n",
    "‚ö†Ô∏è  KATO service connection lost (attempt 1/3)\n",
    "   Waiting for service to become healthy...\n",
    "   ‚úì Service healthy, recreating session and retrying...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîç When You Still Need to Troubleshoot\n",
    "\n",
    "**If training fails AFTER retry attempts**, try these steps:\n",
    "\n",
    "#### 1. Check KATO Server Status\n",
    "\n",
    "```bash\n",
    "# In terminal:\n",
    "docker ps | grep kato\n",
    "```\n",
    "\n",
    "If KATO is not running:\n",
    "```bash\n",
    "cd /path/to/kato\n",
    "docker-compose up -d kato\n",
    "```\n",
    "\n",
    "#### 2. Check KATO Logs\n",
    "\n",
    "```bash\n",
    "docker logs kato --tail 50\n",
    "```\n",
    "\n",
    "Look for errors like:\n",
    "- `OOM` (out of memory)\n",
    "- `Exception` or `Error` (application crash)\n",
    "- `Connection refused` (network issues)\n",
    "\n",
    "#### 3. Verify Service Health\n",
    "\n",
    "```bash\n",
    "# Should return {\"status\": \"healthy\"}\n",
    "curl http://kato:8000/health\n",
    "```\n",
    "\n",
    "#### 4. Restart KATO Server (Last Resort)\n",
    "\n",
    "```bash\n",
    "docker restart kato\n",
    "```\n",
    "\n",
    "Wait 10-20 seconds for it to become healthy, then re-run training.\n",
    "\n",
    "#### 5. Check Resource Usage\n",
    "\n",
    "```bash\n",
    "# Check memory/CPU usage\n",
    "docker stats kato --no-stream\n",
    "```\n",
    "\n",
    "If KATO is using >90% memory:\n",
    "- Reduce `MAX_SAMPLES` in training config\n",
    "- Reduce `chunk_size` in layer configuration\n",
    "- Increase Docker memory limit\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Common Issues (Now Auto-Resolved)\n",
    "\n",
    "| Issue | Old Behavior | New Behavior (v3.6.0+) |\n",
    "|-------|--------------|------------------------|\n",
    "| **Service restart (10k requests)** | ‚ùå Training fails with ConnectionError | ‚úÖ Auto-detects, waits, recreates session, continues |\n",
    "| **Session expiration** | ‚ùå 404 error, training stops | ‚úÖ Auto-recreates session, restores STM, continues |\n",
    "| **Network blip** | ‚ùå ConnectionError, immediate failure | ‚úÖ Retries with exponential backoff (3 attempts) |\n",
    "| **Service temporarily unhealthy** | ‚ùå Immediate failure | ‚úÖ Waits up to 30s for health check |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Understanding Service Restart Behavior\n",
    "\n",
    "**Why does KATO restart?**\n",
    "- Configured with `uvicorn --limit-max-requests 10000`\n",
    "- Prevents memory leaks from accumulating\n",
    "- Forces periodic cleanup (production best practice)\n",
    "\n",
    "**What happens during restart?**\n",
    "1. KATO processes 10,000th request\n",
    "2. Uvicorn logs: `WARNING: Maximum request limit of 10000 exceeded. Terminating process.`\n",
    "3. Container restarts (takes ~5-10 seconds)\n",
    "4. Health check passes\n",
    "5. Service accepts new connections\n",
    "\n",
    "**Training impact?**\n",
    "- ‚ùå **Before (v3.5.0)**: Training would fail at ~424 samples\n",
    "- ‚úÖ **After (v3.6.0)**: Training continues seamlessly through restart\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ When to Contact Support\n",
    "\n",
    "Contact KATO developers if:\n",
    "1. **Persistent failures**: Retry logic fails all 3 attempts repeatedly\n",
    "2. **Service crashes**: KATO container keeps restarting (check `docker logs kato`)\n",
    "3. **OOM errors**: Service running out of memory despite normal workload\n",
    "4. **Data corruption**: Patterns not being stored correctly\n",
    "\n",
    "Otherwise, the automatic retry logic should handle all transient issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "### Analysis\n",
    "- Open `analysis.ipynb` to visualize frequency distributions\n",
    "- Inspect high-frequency patterns\n",
    "- Compare training runs\n",
    "\n",
    "### Scaling Up\n",
    "- Increase `MAX_SAMPLES` (100 ‚Üí 1000 ‚Üí 10000)\n",
    "- Add parallel workers (`NUM_WORKERS = 3`)\n",
    "- Use larger datasets (C4, RefinedWeb)\n",
    "\n",
    "### Advanced Features\n",
    "- Add profiling with `ProfilingEngine`\n",
    "- Enable checkpointing for long runs\n",
    "- Experiment with different `chunk_size` values\n",
    "- Try different recall thresholds per layer\n",
    "\n",
    "### Generation\n",
    "- Open `generation.ipynb` to generate text using learned patterns\n",
    "- See how hierarchical predictions work\n",
    "- Experiment with different generation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Migration Note: KATO Storage Update\n",
    "\n",
    "**Date**: November 2025  \n",
    "**Change**: KATO migrated from MongoDB to ClickHouse + Redis\n",
    "\n",
    "### What Changed\n",
    "\n",
    "**Storage Backend**:\n",
    "- ‚ùå **Removed**: MongoDB (deprecated)\n",
    "- ‚úÖ **Added**: ClickHouse (pattern data) + Redis (metadata)\n",
    "- üéØ **Result**: 100-300x faster pattern queries\n",
    "\n",
    "**KATO API**:\n",
    "- ‚úÖ **No changes** - HTTP API remains backward compatible\n",
    "- ‚úÖ Training works exactly as before\n",
    "- ‚úÖ Pattern learning and storage fully functional\n",
    "\n",
    "### What Works Now\n",
    "\n",
    "‚úÖ **Training Pipeline**: Fully functional\n",
    "- Tokenization, chunking, and pattern learning work\n",
    "- All KATO API calls (`observe`, `learn`, `predict`) work\n",
    "- Hierarchical pattern flow intact\n",
    "\n",
    "‚úÖ **Text Generation**: Use `generation.ipynb`\n",
    "- Pattern retrieval works\n",
    "- Hierarchical predictions work\n",
    "- Text generation fully functional\n",
    "\n",
    "### What's Temporarily Unavailable\n",
    "\n",
    "‚è≥ **Post-Training Analysis**: Being updated\n",
    "- Pattern counting (previously used MongoDB queries)\n",
    "- Frequency distributions\n",
    "- Pattern inspection tools\n",
    "\n",
    "These features will be restored with new ClickHouse + Redis analysis tools.\n",
    "\n",
    "### For Developers\n",
    "\n",
    "**If you need pattern statistics now**:\n",
    "- Option 1: Use `generation.ipynb` to verify patterns work\n",
    "- Option 2: Access ClickHouse/Redis directly (see KATO server docs)\n",
    "- Option 3: Wait for new analysis tools (recommended)\n",
    "\n",
    "**Future enhancements**:\n",
    "- New `tools/clickhouse_redis_analyzer.py` module\n",
    "- Updated `analysis.ipynb` with new storage queries\n",
    "- Restored pattern counting and frequency analysis\n",
    "\n",
    "### Resources\n",
    "\n",
    "- KATO migration docs: `/Users/sevakavakians/PROGRAMMING/kato/planning-docs/completed/features/2025-11-13-mongodb-removal-complete.md`\n",
    "- This notebook focuses on **training** (fully working)\n",
    "- Use `generation.ipynb` for **testing** (fully working)\n",
    "- Watch for updates to `analysis.ipynb` (coming soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
