{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Hierarchical Training v2.0 - Educational Architecture\n",
    "\n",
    "**Purpose**: Train hierarchical concept learner with transparent, layer-based API.\n",
    "\n",
    "## Key Changes in v2.0\n",
    "\n",
    "### Architecture\n",
    "- **Full text processing**: Take complete text ‚Üí tokenize ‚Üí chunk ‚Üí feed to node0\n",
    "- **Natural abstraction**: Let hierarchy learn naturally (no forced segmentation)\n",
    "- **Pattern name flow**: Explicit flow between layers visible to users\n",
    "\n",
    "### API Design\n",
    "- **TensorFlow/PyTorch-style**: Use `add_layer()` to build hierarchy\n",
    "- **Explicit KATO calls**: Show `observe()`, `observe_sequence()`, `learn()`, `get_predictions()`\n",
    "- **Educational focus**: Users see exactly what's happening\n",
    "\n",
    "### Configuration\n",
    "- **Flexible metadata**: Configurable which layers capture source metadata\n",
    "- **Per-layer settings**: Chunk size, max predictions, recall threshold, STM mode\n",
    "- **Transparent**: All settings visible in notebook\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. How to configure hierarchical layers explicitly\n",
    "2. How pattern names flow between layers\n",
    "3. How KATO API calls work (observe, learn, predict)\n",
    "4. How to handle metadata at specific layers\n",
    "5. How to process full documents through the hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets transformers requests numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All modules imported successfully\n",
      "‚úì Ready for hierarchical training v2.0\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "from tools.hierarchical_builder import (\n",
    "    HierarchicalBuilder,\n",
    "    process_chunk_at_layer,\n",
    "    accumulate_in_stm,\n",
    "    learn_from_stm,\n",
    "    extract_prediction_field\n",
    ")\n",
    "\n",
    "# For profiling and analysis\n",
    "from tools import (\n",
    "    ProfilingEngine,\n",
    "    StreamingDatasetLoader\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì All modules imported successfully\")\n",
    "print(\"‚úì Ready for hierarchical training v2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Service Configuration\n",
    "\n",
    "Configure KATO server URL.\n",
    "\n",
    "**Note**: KATO has migrated from MongoDB to ClickHouse + Redis (Nov 2025). The KATO API remains backward compatible, so training works as before. Post-training analysis tools are being updated.\n",
    "\n",
    "**Multi-machine support**: Change KATO_URL if running KATO on a separate machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Service URLs configured\n",
      "  KATO: http://kato:8000\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# SERVICE CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "KATO_URL = 'http://kato:8000'  # KATO server\n",
    "\n",
    "# For multi-machine setups:\n",
    "# KATO_URL = 'http://192.168.1.100:8000'\n",
    "\n",
    "print(\"‚úì Service URLs configured\")\n",
    "print(f\"  KATO: {KATO_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking KATO server at http://kato:8000...\n",
      "‚úì KATO server is healthy at http://kato:8000\n",
      "\n",
      "‚úì Ready to begin training!\n",
      "  Timestamp: 2026-01-19 20:36:53\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# VERIFY KATO SERVER CONNECTION\n",
    "# ========================================\n",
    "\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def check_kato_server(url):\n",
    "    \"\"\"Check if KATO server is responding.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{url}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úì KATO server is healthy at {url}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  KATO server responded with status {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"‚úó Cannot connect to KATO server at {url}\")\n",
    "        print(f\"  Make sure KATO is running (check: docker ps | grep kato)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error checking KATO server: {e}\")\n",
    "        return False\n",
    "\n",
    "print(f\"Checking KATO server at {KATO_URL}...\")\n",
    "if check_kato_server(KATO_URL):\n",
    "    print(f\"\\n‚úì Ready to begin training!\")\n",
    "    print(f\"  Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Training will fail without KATO server\")\n",
    "    print(f\"  Please start KATO before continuing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "Configure dataset and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Training configuration set\n",
      "  Dataset: wikitext\n",
      "  Max samples: 100,000\n",
      "  Workers: 1\n",
      "  Checkpoint interval: 1,000\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Dataset\n",
    "DATASET_KEY = 'wikitext'  # Options: 'c4', 'refinedweb', 'wikitext', 'openwebtext'\n",
    "MAX_SAMPLES = 100000  # Start small for testing, then scale up\n",
    "CHUNK_SIZES = [8, 8, 8, 8]      # [node0, node1, node2, node3]\n",
    "\n",
    "# Workers (for parallel training)\n",
    "NUM_WORKERS = 1  # Start with 1 for educational single-threaded mode\n",
    "\n",
    "# Checkpoint configuration\n",
    "CHECKPOINT_INTERVAL = 1000  # Save checkpoint every N samples\n",
    "RESUME_FROM_CHECKPOINT = True  # Set True to resume interrupted training\n",
    "\n",
    "print(\"‚úì Training configuration set\")\n",
    "print(f\"  Dataset: {DATASET_KEY}\")\n",
    "print(f\"  Max samples: {MAX_SAMPLES:,}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "print(f\"  Checkpoint interval: {CHECKPOINT_INTERVAL:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Layer Configuration (NEW!)\n",
    "\n",
    "**TensorFlow/PyTorch-style API**: Build hierarchy by adding layers.\n",
    "\n",
    "**Key Parameters**:\n",
    "- `chunk_size`: How many inputs per chunk\n",
    "- `max_predictions`: Top N predictions to pass to next layer\n",
    "- `prediction_field`: Which field to extract ('name')\n",
    "- `recall_threshold`: Pattern matching strictness (0.0-1.0)\n",
    "- `capture_metadata`: Whether this layer captures source metadata (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BUILDING HIERARCHICAL MODEL\n",
      "================================================================================\n",
      "Tokenizer: gpt2\n",
      "Layers: 4\n",
      "Base URL: http://kato:8000\n",
      "\n",
      "‚úì Tokenizer loaded: gpt2\n",
      "‚úì Layer 0 (node0): KATO client initialized\n",
      "‚úì Layer 1 (node1): KATO client initialized\n",
      "‚úì Layer 2 (node2): KATO client initialized\n",
      "‚úì Layer 3 (node3): KATO client initialized\n",
      "\n",
      "================================================================================\n",
      "MODEL BUILD COMPLETE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "HIERARCHICAL MODEL SUMMARY\n",
      "================================================================================\n",
      "Tokenizer: gpt2\n",
      "Total Layers: 4\n",
      "\n",
      "Layer      Name       Chunk    MaxPred  Recall   STM Mode   Metadata  \n",
      "--------------------------------------------------------------------------------\n",
      "0          node0      8        10       0.60     CLEAR      No        \n",
      "1          node1      8        8        0.60     CLEAR      No        \n",
      "2          node2      8        6        0.60     CLEAR      Yes       \n",
      "3          node3      8        4        0.60     CLEAR      Yes       \n",
      "================================================================================\n",
      "\n",
      "Receptive Fields (token coverage):\n",
      "  node0: 8 tokens\n",
      "  node1: 64 tokens\n",
      "  node2: 512 tokens\n",
      "  node3: 4,096 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build hierarchy with explicit layer configuration\n",
    "hierarchy = HierarchicalBuilder(\n",
    "    tokenizer_name='gpt2',\n",
    "    base_url=KATO_URL\n",
    ")\n",
    "\n",
    "# Add node0: Chunk-level patterns\n",
    "hierarchy.add_layer(\n",
    "    name='node0',\n",
    "    chunk_size=CHUNK_SIZES[0],\n",
    "    max_predictions=10,\n",
    "    prediction_field='name',\n",
    "    recall_threshold=0.6,\n",
    "    stm_mode='CLEAR',\n",
    "    max_pattern_length=0,\n",
    "    process_predictions=False,\n",
    "    capture_metadata=False  # Don't capture metadata here\n",
    ")\n",
    "\n",
    "# Add node1: Paragraph-level patterns\n",
    "hierarchy.add_layer(\n",
    "    name='node1',\n",
    "    chunk_size=CHUNK_SIZES[1],\n",
    "    max_predictions=8,\n",
    "    prediction_field='name',\n",
    "    recall_threshold=0.6,\n",
    "    stm_mode='CLEAR',\n",
    "    max_pattern_length=0,\n",
    "    process_predictions=False,\n",
    "    capture_metadata=False  # Don't capture metadata here either\n",
    ")\n",
    "\n",
    "# Add node2: Chapter-level patterns (capture metadata)\n",
    "hierarchy.add_layer(\n",
    "    name='node2',\n",
    "    chunk_size=CHUNK_SIZES[2],\n",
    "    max_predictions=6,\n",
    "    prediction_field='name',\n",
    "    recall_threshold=0.6,\n",
    "    stm_mode='CLEAR',\n",
    "    max_pattern_length=0,\n",
    "    process_predictions=False,\n",
    "    capture_metadata=True  # START capturing metadata at this layer\n",
    ")\n",
    "\n",
    "# Add node3: Book-level patterns (capture metadata)\n",
    "hierarchy.add_layer(\n",
    "    name='node3',\n",
    "    chunk_size=CHUNK_SIZES[3],\n",
    "    max_predictions=4,\n",
    "    prediction_field='name',\n",
    "    recall_threshold=0.6,\n",
    "    stm_mode='CLEAR',\n",
    "    max_pattern_length=0,\n",
    "    process_predictions=False,\n",
    "    capture_metadata=True  # Capture metadata here too\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "model = hierarchy.build(verbose=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_pattern_length': 0,\n",
       " 'persistence': 5,\n",
       " 'recall_threshold': 0.6,\n",
       " 'indexer_type': 'VI',\n",
       " 'max_predictions': 8,\n",
       " 'sort_symbols': True,\n",
       " 'process_predictions': False,\n",
       " 'use_token_matching': True,\n",
       " 'stm_mode': 'CLEAR',\n",
       " 'rank_sort_algo': 'potential',\n",
       " 'filter_pipeline': [],\n",
       " 'length_min_ratio': 0.5,\n",
       " 'length_max_ratio': 2.0,\n",
       " 'jaccard_threshold': 0.3,\n",
       " 'jaccard_min_overlap': 2,\n",
       " 'minhash_threshold': 0.7,\n",
       " 'minhash_bands': 20,\n",
       " 'minhash_rows': 5,\n",
       " 'minhash_num_hashes': 100,\n",
       " 'bloom_false_positive_rate': 0.01,\n",
       " 'max_candidates_per_stage': 100000,\n",
       " 'enable_filter_metrics': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].client.get_session_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì node0 use_token_matching: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CONFIGURE TOKEN MATCHING FOR NODE0\n",
    "# ========================================\n",
    "\n",
    "# print(\"\\nConfiguring token matching for node0...\")\n",
    "\n",
    "# # Update node0 session config to enable token-level matching\n",
    "# result = model.layers[0].client.update_session_config({\n",
    "#     'use_token_matching': True\n",
    "# })\n",
    "\n",
    "# Verify it's set\n",
    "config = model.layers[0].client.get_session_config()\n",
    "print(f\"‚úì node0 use_token_matching: {config.get('use_token_matching')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explicit KATO Helper Functions (Educational)\n",
    "\n",
    "These functions show the EXACT KATO API calls being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def process_text_sample(text, metadata=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Process one text sample through the hierarchy.\n",
    "\n",
    "    Shows explicit KATO calls at each step.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PROCESSING SAMPLE\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    # Step 1: Tokenize\n",
    "    tokens = model.tokenize(text)\n",
    "    if verbose:\n",
    "        print(f\"\\n1. Tokenized: {len(tokens)} tokens\")\n",
    "        print(f\"   First 10: {tokens[:10]}\")\n",
    "\n",
    "    # Step 2: Chunk tokens for node0\n",
    "    chunks = model.chunk_tokens(tokens, model.layers[0].chunk_size)\n",
    "    if verbose:\n",
    "        print(f\"\\n2. Chunked into {len(chunks)} chunks (size={model.layers[0].chunk_size})\")\n",
    "\n",
    "    # Step 3: Process each chunk at node0\n",
    "    if verbose:\n",
    "        print(f\"\\n3. Processing chunks at node0...\")\n",
    "\n",
    "    node0_patterns = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # EXPLICIT KATO API CALL\n",
    "        pattern = process_chunk_at_layer(\n",
    "            chunk,\n",
    "            model.layers[0].client,\n",
    "            metadata=None,  # No metadata at node0\n",
    "            verbose=False\n",
    "        )\n",
    "        node0_patterns.append(pattern)\n",
    "        if verbose and i < 3:  # Show first 3\n",
    "            print(f\"   Chunk {i+1}: {pattern[:40]}...\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"   ‚Üí node0 produced {len(node0_patterns)} patterns\")\n",
    "\n",
    "    # Step 4: Chunk node0 patterns and process at node1\n",
    "    if verbose:\n",
    "        print(f\"\\n4. Chunking {len(node0_patterns)} patterns for node1...\")\n",
    "\n",
    "    # Chunk node0 patterns using node1's chunk_size\n",
    "    node1_chunks = model.chunk_tokens(node0_patterns, model.layers[1].chunk_size)\n",
    "    if verbose:\n",
    "        print(f\"   Created {len(node1_chunks)} chunks (size={model.layers[1].chunk_size})\")\n",
    "\n",
    "    node1_patterns = []\n",
    "    for i, chunk in enumerate(node1_chunks):\n",
    "        # EXPLICIT KATO API CALL\n",
    "        accumulate_in_stm(\n",
    "            chunk,\n",
    "            model.layers[1].client,\n",
    "            metadata=None,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Learn from this chunk\n",
    "        # EXPLICIT KATO API CALL\n",
    "        pattern = learn_from_stm(model.layers[1].client, verbose=False)\n",
    "        node1_patterns.append(pattern)\n",
    "\n",
    "        if verbose and i < 3:  # Show first 3\n",
    "            print(f\"   Chunk {i+1}: {pattern[:40]}...\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"   ‚Üí node1 produced {len(node1_patterns)} patterns\")\n",
    "\n",
    "    # Step 5: Chunk node1 patterns and process at node2 (with metadata if configured)\n",
    "    should_capture_metadata = model.layers[1].should_capture_metadata()\n",
    "    meta = metadata if should_capture_metadata else None\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n5. Chunking {len(node1_patterns)} patterns for node2...\")\n",
    "        if meta:\n",
    "            print(f\"   ‚Üí Metadata attached: {meta}\")\n",
    "\n",
    "    # Chunk node1 patterns using node2's chunk_size\n",
    "    node2_chunks = model.chunk_tokens(node1_patterns, model.layers[2].chunk_size)\n",
    "    if verbose:\n",
    "        print(f\"   Created {len(node2_chunks)} chunks (size={model.layers[2].chunk_size})\")\n",
    "\n",
    "    node2_patterns = []\n",
    "    for i, chunk in enumerate(node2_chunks):\n",
    "        # EXPLICIT KATO API CALL\n",
    "        accumulate_in_stm(chunk, model.layers[2].client, metadata=meta, verbose=False)\n",
    "\n",
    "        # Learn from this chunk\n",
    "        # EXPLICIT KATO API CALL\n",
    "        pattern = learn_from_stm(model.layers[2].client, verbose=False)\n",
    "        node2_patterns.append(pattern)\n",
    "\n",
    "        if verbose and i < 3:  # Show first 3\n",
    "            print(f\"   Chunk {i+1}: {pattern[:40]}...\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"   ‚Üí node2 produced {len(node2_patterns)} patterns\")\n",
    "\n",
    "    # Step 6: Chunk node2 patterns and process at node3 (with metadata if configured)\n",
    "    should_capture_metadata = model.layers[2].should_capture_metadata()\n",
    "    meta = metadata if should_capture_metadata else None\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n6. Chunking {len(node2_patterns)} patterns for node3...\")\n",
    "\n",
    "    # Chunk node2 patterns using node3's chunk_size\n",
    "    node3_chunks = model.chunk_tokens(node2_patterns, model.layers[3].chunk_size)\n",
    "    if verbose:\n",
    "        print(f\"   Created {len(node3_chunks)} chunks (size={model.layers[3].chunk_size})\")\n",
    "\n",
    "    node3_patterns = []\n",
    "    for i, chunk in enumerate(node3_chunks):\n",
    "        # EXPLICIT KATO API CALL\n",
    "        accumulate_in_stm(chunk, model.layers[3].client, metadata=meta, verbose=False)\n",
    "\n",
    "        # Learn from this chunk\n",
    "        # EXPLICIT KATO API CALL\n",
    "        pattern = learn_from_stm(model.layers[3].client, verbose=False)\n",
    "        node3_patterns.append(pattern)\n",
    "\n",
    "        if verbose and i < 3:  # Show first 3\n",
    "            print(f\"   Chunk {i+1}: {pattern[:40]}...\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"   ‚Üí node3 produced {len(node3_patterns)} patterns\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SAMPLE COMPLETE\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "    return {\n",
    "        'node0_patterns': len(node0_patterns),\n",
    "        'node1_patterns': len(node1_patterns),\n",
    "        'node2_patterns': len(node2_patterns),\n",
    "        'node3_patterns': len(node3_patterns)\n",
    "    }\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Single-Sample Educational Demo\n",
    "\n",
    "Process ONE sample to see exactly what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one sample for demonstration using static method API\n",
    "stream_iterator = StreamingDatasetLoader.load_streaming(\n",
    "    dataset_key=DATASET_KEY,\n",
    "    max_samples=1\n",
    ")\n",
    "\n",
    "# Get first sample\n",
    "sample = next(iter(stream_iterator))\n",
    "\n",
    "print(f\"Sample text preview:\")\n",
    "print(f\"{sample['text'][:200]}...\")\n",
    "print()\n",
    "\n",
    "# Process with verbose output\n",
    "result = process_text_sample(\n",
    "    sample['text'],\n",
    "    metadata={'source': DATASET_KEY, 'sample_id': 0},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nResult summary:\")\n",
    "print(f\"  node0 patterns: {result['node0_patterns']}\")\n",
    "print(f\"  node1 pattern: {result['node1_pattern'][:50]}...\")\n",
    "print(f\"  node2 pattern: {result['node2_pattern'][:50]}...\")\n",
    "print(f\"  node3 pattern: {result['node3_pattern'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Training (Resilient to Service Restarts)\n",
    "\n",
    "### Automatic Retry Behavior\n",
    "\n",
    "The `KATOClient` (v3.6.0+) now includes **production-grade retry logic** that handles KATO service restarts automatically:\n",
    "\n",
    "**What Happens**:\n",
    "1. **Service Restarts**: KATO restarts every ~10,000 requests (uvicorn `--limit-max-requests`) to prevent memory leaks\n",
    "2. **Auto-Detection**: Client detects `ConnectionError` when service is restarting\n",
    "3. **Health Check**: Waits for service to become healthy (up to 30 seconds)\n",
    "4. **Session Recreation**: Creates new session with same configuration\n",
    "5. **Transparent Retry**: Retries failed request automatically\n",
    "\n",
    "**You'll See** (when restart occurs):\n",
    "```\n",
    "‚ö†Ô∏è  KATO service connection lost (attempt 1/3)\n",
    "   Likely cause: Service restart (uvicorn --limit-max-requests)\n",
    "   Waiting for service to become healthy...\n",
    "   ‚úì Service healthy, recreating session and retrying...\n",
    "```\n",
    "\n",
    "**No Action Required**: Training continues automatically!\n",
    "\n",
    "**Configuration**:\n",
    "- Max retry attempts: 3\n",
    "- Health check timeout: 30 seconds\n",
    "- Exponential backoff: 0.5s ‚Üí 1s ‚Üí 2s\n",
    "\n",
    "**Technical Details**:\n",
    "- See `kato_client.py::_request()` for implementation\n",
    "- See `kato_client.py::_wait_for_kato_healthy()` for health check logic\n",
    "- Connection retry is separate from session recreation (404 errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# CHECKPOINT LOAD LOGIC\n# ========================================\nimport json\nimport os\nimport time\nfrom pathlib import Path\n\n# Checkpoint configuration\nCHECKPOINT_DIR = './checkpoints'\ncheckpoint_path = Path(CHECKPOINT_DIR) / f\"{DATASET_KEY}_v2_checkpoint.json\"\n\n# Initialize tracking variables\nstart_sample = 0\nsamples_completed = 0\nsamples_errored = 0\n\n# Create checkpoint directory\nPath(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n\n# Resume from checkpoint if requested\nif RESUME_FROM_CHECKPOINT and checkpoint_path.exists():\n    try:\n        with open(checkpoint_path, 'r') as f:\n            checkpoint_data = json.load(f)\n        \n        start_sample = checkpoint_data.get('samples_completed', 0)\n        samples_completed = start_sample\n        samples_errored = checkpoint_data.get('samples_errored', 0)\n        \n        # Validate configuration matches checkpoint\n        if 'model_config' in checkpoint_data:\n            saved_config = checkpoint_data['model_config']\n            current_chunk_sizes = CHUNK_SIZES\n            \n            mismatches = []\n            if saved_config.get('chunk_sizes') != current_chunk_sizes:\n                mismatches.append(f\"chunk_sizes: checkpoint={saved_config.get('chunk_sizes')}, current={current_chunk_sizes}\")\n            if saved_config.get('dataset_key') != DATASET_KEY:\n                mismatches.append(f\"dataset_key: checkpoint={saved_config.get('dataset_key')}, current={DATASET_KEY}\")\n            if saved_config.get('tokenizer') != 'gpt2':\n                mismatches.append(f\"tokenizer: checkpoint={saved_config.get('tokenizer')}, current=gpt2\")\n            \n            if mismatches:\n                error_msg = (\n                    \"\\n‚ùå CONFIGURATION MISMATCH - Cannot resume training!\\n\\n\"\n                    \"The checkpoint was created with different configuration.\\n\"\n                    \"Mismatches detected:\\n\"\n                )\n                for mismatch in mismatches:\n                    error_msg += f\"  - {mismatch}\\n\"\n                error_msg += (\n                    \"\\nTo fix:\\n\"\n                    \"  1. Use the EXACT same configuration as checkpoint\\n\"\n                    \"  2. Or delete checkpoint and start fresh\\n\"\n                    f\"  3. Or rename checkpoint file: {checkpoint_path}\\n\"\n                )\n                raise ValueError(error_msg)\n        \n        print(f\"\\nüìÇ Resuming from checkpoint:\")\n        print(f\"   Samples completed: {samples_completed:,}\")\n        print(f\"   Samples errored: {samples_errored:,}\")\n        print(f\"   ‚úì Configuration validated\")\n        print()\n    except ValueError:\n        # Re-raise configuration mismatch errors\n        raise\n    except Exception as e:\n        print(f\"\\n‚ö†Ô∏è  Failed to load checkpoint: {e}\")\n        print(\"Starting from beginning...\\n\")\n        start_sample = 0\n        samples_completed = 0\n        samples_errored = 0\n\nprint(f\"{'='*60}\")\nprint(f\"TRAINING START\")\nprint(f\"{'='*60}\")\nprint(f\"Dataset: {DATASET_KEY}\")\nprint(f\"Max samples: {MAX_SAMPLES:,}\")\nprint(f\"Starting from: sample {start_sample:,}\")\nprint(f\"Checkpoint interval: {CHECKPOINT_INTERVAL:,}\")\nprint(f\"{'='*60}\\n\")\n\n# ========================================\n# TRAINING LOOP WITH CHECKPOINTING\n# ========================================\n\n# Clear all STM before batch training\nif RESUME_FROM_CHECKPOINT and start_sample > 0:\n    print(\"Resuming training - clearing STM only...\")\n    model.clear_all_stm()\nelse:\n    print(\"Starting fresh - clearing all memory...\")\n    model.clear_all_memory()\n\n# Training statistics\nstats = {\n    'samples_processed': 0,\n    'total_tokens': 0,\n    'node0_patterns': 0,\n    'service_restarts': 0,\n    'errors': samples_errored\n}\n\nprint(f\"\\n{'='*60}\")\nprint(f\"BATCH TRAINING WITH AUTO-RETRY AND CHECKPOINTING\")\nprint(f\"{'='*60}\\n\")\nprint(f\"‚ÑπÔ∏è  KATOClient now handles service restarts automatically!\")\nprint(f\"   - Detects connection failures (service restart)\")\nprint(f\"   - Waits for service health check (up to 60s)\")\nprint(f\"   - Recreates session and retries transparently\")\nprint(f\"   - You'll see status messages if restarts occur\\n\")\n\n# Load streaming dataset using static method API with efficient skip\nstream_iterator = StreamingDatasetLoader.load_streaming(\n    dataset_key=DATASET_KEY,\n    max_samples=MAX_SAMPLES,\n    skip=start_sample  # Efficiently skip already-processed samples\n)\n\n# Start time for checkpoint tracking\nstart_time = time.time()\n\n# Process samples with clean progress bar\nfor i, sample in enumerate(tqdm(stream_iterator, total=MAX_SAMPLES-start_sample, desc=\"Training\", unit=\"sample\", initial=0)):\n    # Calculate actual sample index in full dataset\n    actual_sample_idx = start_sample + i\n    \n    try:\n        # Process sample (verbose=False for batch mode)\n        result = process_text_sample(\n            sample['text'],\n            metadata={'source': DATASET_KEY, 'sample_id': actual_sample_idx},\n            verbose=False\n        )\n        \n        # Update stats\n        stats['samples_processed'] += 1\n        samples_completed += 1\n        stats['node0_patterns'] += result['node0_patterns']\n        \n        # Save checkpoint periodically\n        if samples_completed > 0 and samples_completed % CHECKPOINT_INTERVAL == 0:\n            checkpoint_data = {\n                'dataset_key': DATASET_KEY,\n                'samples_completed': samples_completed,\n                'samples_errored': stats['errors'],\n                'timestamp': time.time(),\n                'elapsed_seconds': time.time() - start_time,\n                'model_config': {\n                    'chunk_sizes': CHUNK_SIZES,\n                    'dataset_key': DATASET_KEY,\n                    'tokenizer': 'gpt2',\n                    'num_layers': 4\n                }\n            }\n            \n            # Atomic write\n            temp_path = checkpoint_path.with_suffix('.tmp')\n            with open(temp_path, 'w') as f:\n                json.dump(checkpoint_data, f, indent=2)\n            temp_path.replace(checkpoint_path)\n            \n            print(f\"\\nüíæ Checkpoint saved: {samples_completed:,} samples completed\")\n        \n    except Exception as e:\n        # Catch any errors that couldn't be auto-recovered\n        print(f\"\\n‚ö†Ô∏è  Error processing sample {actual_sample_idx + 1}: {str(e)[:150]}\")\n        stats['errors'] += 1\n        samples_errored += 1\n        \n        # Save emergency checkpoint on error\n        if stats['errors'] % 10 == 0:\n            checkpoint_data = {\n                'dataset_key': DATASET_KEY,\n                'samples_completed': samples_completed,\n                'samples_errored': stats['errors'],\n                'timestamp': time.time(),\n                'elapsed_seconds': time.time() - start_time,\n                'model_config': {\n                    'chunk_sizes': CHUNK_SIZES,\n                    'dataset_key': DATASET_KEY,\n                    'tokenizer': 'gpt2',\n                    'num_layers': 4\n                },\n                'last_error': str(e)[:200]\n            }\n            with open(checkpoint_path, 'w') as f:\n                json.dump(checkpoint_data, f, indent=2)\n            print(f\"üíæ Emergency checkpoint saved\")\n        \n        # If too many errors, abort\n        if stats['errors'] > 10:\n            print(f\"\\n‚úó Too many errors ({stats['errors']}), aborting training\")\n            print(f\"  Check KATO server: docker logs kato --tail 50\")\n            break\n\n# Save final checkpoint\ncheckpoint_data = {\n    'dataset_key': DATASET_KEY,\n    'samples_completed': samples_completed,\n    'samples_errored': stats['errors'],\n    'timestamp': time.time(),\n    'elapsed_seconds': time.time() - start_time,\n    'model_config': {\n        'chunk_sizes': CHUNK_SIZES,\n        'dataset_key': DATASET_KEY,\n        'tokenizer': 'gpt2',\n        'num_layers': 4\n    }\n}\nwith open(checkpoint_path, 'w') as f:\n    json.dump(checkpoint_data, f, indent=2)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"TRAINING COMPLETE\")\nprint(f\"{'='*60}\\n\")\n\nprint(f\"Statistics:\")\nprint(f\"  Samples processed: {stats['samples_processed']:,}\")\nprint(f\"  Total samples completed: {samples_completed:,}\")\nprint(f\"  Node0 patterns: {stats['node0_patterns']:,}\")\nif stats['samples_processed'] > 0:\n    print(f\"  Avg patterns/sample: {stats['node0_patterns'] / stats['samples_processed']:.1f}\")\nif stats['errors'] > 0:\n    print(f\"  ‚ö†Ô∏è  Failed samples: {stats['errors']}\")\n    \nprint(f\"\\nüíæ Final checkpoint saved: {checkpoint_path}\")\nprint(f\"\\n‚ÑπÔ∏è  Note: KATO service restarts every ~100,000 requests\")\nprint(f\"   This is normal and handled automatically by KATOClient\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Complete\n",
    "\n",
    "Training is done! Patterns have been learned and stored in KATO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Pattern Analysis Status:\")\n",
    "print(\"   ‚Ä¢ Patterns successfully stored in KATO\")\n",
    "print(\"   ‚Ä¢ KATO storage: ClickHouse + Redis (MongoDB removed)\")\n",
    "print(\"   ‚Ä¢ Post-training analysis tools coming soon\")\n",
    "\n",
    "print(\"\\nüîç Next Steps:\")\n",
    "print(\"   ‚Ä¢ Use generation.ipynb to test text generation with learned patterns\")\n",
    "print(\"   ‚Ä¢ Try different prompts to see hierarchical predictions in action\")\n",
    "print(\"   ‚Ä¢ Experiment with generation parameters\")\n",
    "\n",
    "print(\"\\nüìù Note:\")\n",
    "print(\"   Pattern counting and frequency analysis temporarily unavailable.\")\n",
    "print(\"   New analysis tools for ClickHouse + Redis are being developed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting: KATO Server Issues\n",
    "\n",
    "### ‚úÖ Good News: Most Issues Auto-Resolve!\n",
    "\n",
    "**KATOClient (v3.6.0+)** automatically handles:\n",
    "- ‚úÖ Service restarts (every ~10k requests)\n",
    "- ‚úÖ Connection failures (detects and waits for healthy service)\n",
    "- ‚úÖ Session recreation (recreates with same config)\n",
    "- ‚úÖ Exponential backoff retry (3 attempts, 30s health check)\n",
    "\n",
    "**You should see** automatic recovery messages like:\n",
    "```\n",
    "‚ö†Ô∏è  KATO service connection lost (attempt 1/3)\n",
    "   Waiting for service to become healthy...\n",
    "   ‚úì Service healthy, recreating session and retrying...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîç When You Still Need to Troubleshoot\n",
    "\n",
    "**If training fails AFTER retry attempts**, try these steps:\n",
    "\n",
    "#### 1. Check KATO Server Status\n",
    "\n",
    "```bash\n",
    "# In terminal:\n",
    "docker ps | grep kato\n",
    "```\n",
    "\n",
    "If KATO is not running:\n",
    "```bash\n",
    "cd /path/to/kato\n",
    "docker-compose up -d kato\n",
    "```\n",
    "\n",
    "#### 2. Check KATO Logs\n",
    "\n",
    "```bash\n",
    "docker logs kato --tail 50\n",
    "```\n",
    "\n",
    "Look for errors like:\n",
    "- `OOM` (out of memory)\n",
    "- `Exception` or `Error` (application crash)\n",
    "- `Connection refused` (network issues)\n",
    "\n",
    "#### 3. Verify Service Health\n",
    "\n",
    "```bash\n",
    "# Should return {\"status\": \"healthy\"}\n",
    "curl http://kato:8000/health\n",
    "```\n",
    "\n",
    "#### 4. Restart KATO Server (Last Resort)\n",
    "\n",
    "```bash\n",
    "docker restart kato\n",
    "```\n",
    "\n",
    "Wait 10-20 seconds for it to become healthy, then re-run training.\n",
    "\n",
    "#### 5. Check Resource Usage\n",
    "\n",
    "```bash\n",
    "# Check memory/CPU usage\n",
    "docker stats kato --no-stream\n",
    "```\n",
    "\n",
    "If KATO is using >90% memory:\n",
    "- Reduce `MAX_SAMPLES` in training config\n",
    "- Reduce `chunk_size` in layer configuration\n",
    "- Increase Docker memory limit\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Common Issues (Now Auto-Resolved)\n",
    "\n",
    "| Issue | Old Behavior | New Behavior (v3.6.0+) |\n",
    "|-------|--------------|------------------------|\n",
    "| **Service restart (10k requests)** | ‚ùå Training fails with ConnectionError | ‚úÖ Auto-detects, waits, recreates session, continues |\n",
    "| **Session expiration** | ‚ùå 404 error, training stops | ‚úÖ Auto-recreates session, restores STM, continues |\n",
    "| **Network blip** | ‚ùå ConnectionError, immediate failure | ‚úÖ Retries with exponential backoff (3 attempts) |\n",
    "| **Service temporarily unhealthy** | ‚ùå Immediate failure | ‚úÖ Waits up to 30s for health check |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Understanding Service Restart Behavior\n",
    "\n",
    "**Why does KATO restart?**\n",
    "- Configured with `uvicorn --limit-max-requests 10000`\n",
    "- Prevents memory leaks from accumulating\n",
    "- Forces periodic cleanup (production best practice)\n",
    "\n",
    "**What happens during restart?**\n",
    "1. KATO processes 10,000th request\n",
    "2. Uvicorn logs: `WARNING: Maximum request limit of 10000 exceeded. Terminating process.`\n",
    "3. Container restarts (takes ~5-10 seconds)\n",
    "4. Health check passes\n",
    "5. Service accepts new connections\n",
    "\n",
    "**Training impact?**\n",
    "- ‚ùå **Before (v3.5.0)**: Training would fail at ~424 samples\n",
    "- ‚úÖ **After (v3.6.0)**: Training continues seamlessly through restart\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ When to Contact Support\n",
    "\n",
    "Contact KATO developers if:\n",
    "1. **Persistent failures**: Retry logic fails all 3 attempts repeatedly\n",
    "2. **Service crashes**: KATO container keeps restarting (check `docker logs kato`)\n",
    "3. **OOM errors**: Service running out of memory despite normal workload\n",
    "4. **Data corruption**: Patterns not being stored correctly\n",
    "\n",
    "Otherwise, the automatic retry logic should handle all transient issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "### Analysis\n",
    "- Open `analysis.ipynb` to visualize frequency distributions\n",
    "- Inspect high-frequency patterns\n",
    "- Compare training runs\n",
    "\n",
    "### Scaling Up\n",
    "- Increase `MAX_SAMPLES` (100 ‚Üí 1000 ‚Üí 10000)\n",
    "- Add parallel workers (`NUM_WORKERS = 3`)\n",
    "- Use larger datasets (C4, RefinedWeb)\n",
    "\n",
    "### Advanced Features\n",
    "- Add profiling with `ProfilingEngine`\n",
    "- Enable checkpointing for long runs\n",
    "- Experiment with different `chunk_size` values\n",
    "- Try different recall thresholds per layer\n",
    "\n",
    "### Generation\n",
    "- Open `generation.ipynb` to generate text using learned patterns\n",
    "- See how hierarchical predictions work\n",
    "- Experiment with different generation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What this is\n",
    "- This notebook focuses on **training** (fully working)\n",
    "- Use `generation.ipynb` for **testing** (fully working)\n",
    "\n",
    "‚úÖ **Training Pipeline**: Fully functional\n",
    "- Tokenization, chunking, and pattern learning work\n",
    "- All KATO API calls (`observe`, `learn`, `predict`) work\n",
    "- Hierarchical pattern flow intact\n",
    "\n",
    "‚úÖ **Text Generation**: Use `generation.ipynb`\n",
    "- Pattern retrieval works\n",
    "- Hierarchical predictions work\n",
    "- Text generation fully functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}