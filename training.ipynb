{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Hierarchical Training - Real Data Workflow\n",
    "\n",
    "**Purpose**: Train hierarchical concept learner on real datasets with performance profiling.\n",
    "\n",
    "**This notebook**:\n",
    "- ‚úÖ Trains on **real data** from HuggingFace datasets (WikiText, C4, RefinedWeb, etc.)\n",
    "- ‚úÖ Uses **parallel workers** for optimal speed (2-3x faster)\n",
    "- ‚úÖ Profiles **hardware resources** (CPU, RAM, disk I/O) during training\n",
    "- ‚úÖ Tracks **training history** in SQLite for later analysis\n",
    "- ‚úÖ Tests different **chunk_size and layer configurations** to find optimal settings\n",
    "\n",
    "**Key Concepts**:\n",
    "- node0 learns token chunks (e.g., 8 tokens ‚Üí phrase patterns)\n",
    "- node1 learns sequences of node0 patterns (e.g., 64 tokens ‚Üí sentence patterns)\n",
    "- node2 learns sequences of node1 patterns (e.g., 512 tokens ‚Üí paragraph patterns)\n",
    "- node3 learns sequences of node2 patterns (e.g., 4,096 tokens ‚Üí chapter patterns)\n",
    "\n",
    "**After training**: Use `analysis.ipynb` to analyze learned patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets transformers requests numpy matplotlib tqdm pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All modules imported successfully\n",
      "‚úì Ready for hierarchical training with real data\n"
     ]
    }
   ],
   "source": [
    "# Import hierarchical training modules\n",
    "from tools import (\n",
    "    # Core training\n",
    "    HierarchicalConceptLearner,\n",
    "    HierarchicalNode,\n",
    "    train_from_streaming_dataset_parallel,\n",
    "    \n",
    "    # Profiling and analysis\n",
    "    ProfilingEngine,\n",
    "    HardwareAnalyzerV2,\n",
    "    StorageEstimator,\n",
    "    TrainingHistory,\n",
    "    TrainingEstimator,  # NEW: Data-driven training time estimator\n",
    "    \n",
    "    # Dataset loading\n",
    "    StreamingDatasetLoader,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì All modules imported successfully\")\n",
    "print(\"‚úì Ready for hierarchical training with real data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KATO Node & Training Config\n",
    "\n",
    "**Configuration**:\n",
    "- `dataset_key`: 'wikitext', 'c4', 'refinedweb', 'openwebtext', etc.\n",
    "- `max_samples`: Start small (100) to test, then scale up (10K, 100K, 1M+)\n",
    "- `num_workers`: **3 recommended** (was 6, reduced to prevent deadlocks)\n",
    "  - Rule: workers √ó nodes ‚â§ 30 connections (for stability)\n",
    "  - 3 workers √ó 5 nodes = 15 connections ‚úì SAFE\n",
    "- `checkpoint_interval`: Save progress every N samples (default: 5000)\n",
    "- `resume_from_checkpoint`: Resume from last checkpoint if training was interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration set\n",
      "  Workers: 3 (3 workers √ó 5 nodes = 15 connections - SAFE)\n",
      "  Checkpoint interval: 5,000 samples\n",
      "  Resume: True\n"
     ]
    }
   ],
   "source": [
    "# Chunk sizes per node.\n",
    "\n",
    "# cs_array = [3, 5, 3, 3, 3]\n",
    "# cs_array = [3, 5, 5, 8, 3]\n",
    "# cs_array = [3, 3, 3, 3, 3]\n",
    "# cs_array = [4, 4, 4, 4, 4]\n",
    "# cs_array = [5, 5, 5, 5, 5]\n",
    "# cs_array = [6, 6, 6, 6, 6]\n",
    "# cs_array = [7, 7, 7, 7, 7]\n",
    "cs_array = [8, 8, 8, 8, 8]\n",
    "# cs_array = [8, 6, 5, 4, 3]\n",
    "# cs_array = [5, 4, 4, 3, 3]\n",
    "## after iterating these above, change node0_batch_size to 100 and redo the above - Doing now\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Configure dataset and training parameters\n",
    "DATASET_KEY = 'wikitext'  # Options: 'c4', 'refinedweb', 'wikitext', 'openwebtext'\n",
    "MAX_SAMPLES = 100000  # Start small to test, then scale up\n",
    "NUM_WORKERS = 3    # REDUCED from 6 ‚Üí safer, prevents deadlocks (recommended: 2-4)\n",
    "\n",
    "# Checkpoint configuration (NEW!)\n",
    "CHECKPOINT_INTERVAL = 5000  # Save checkpoint every 5K samples\n",
    "RESUME_FROM_CHECKPOINT = True  # Set True to resume interrupted training\n",
    "\n",
    "print(\"‚úì Configuration set\")\n",
    "print(f\"  Workers: {NUM_WORKERS} (3 workers √ó 5 nodes = 15 connections - SAFE)\")\n",
    "print(f\"  Checkpoint interval: {CHECKPOINT_INTERVAL:,} samples\")\n",
    "print(f\"  Resume: {RESUME_FROM_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Crash Recovery Guide (Kernel Restart / Interruption)\n",
    "\n",
    "**What happens if Jupyter crashes or kernel restarts mid-training?**\n",
    "\n",
    "‚úÖ **Good news**: Learned patterns persist in MongoDB (not lost!)\n",
    "\n",
    "‚úÖ **Checkpoint system**: Progress saved every 5,000 samples\n",
    "\n",
    "**To resume after crash:**\n",
    "\n",
    "1. **Restart kernel** (if needed)\n",
    "\n",
    "2. **Re-run setup cells** with **EXACT SAME configuration**:\n",
    "   - ‚úì Cell 1: Imports\n",
    "   - ‚úì Cell 2: Additional imports\n",
    "   - ‚úì Cell 4: **Configuration** (must match original!)\n",
    "     - Set `RESUME_FROM_CHECKPOINT = True`\n",
    "     - Keep same `cs_array`, `batch_size`, `NUM_WORKERS`\n",
    "   - ‚úì Cell 10: Create learner (**same nodes, chunk_sizes, tokenizer!**)\n",
    "   - ‚úì Cell 14: Create profiler\n",
    "\n",
    "3. **Run training cell** (Cell 16):\n",
    "   - System validates configuration matches checkpoint\n",
    "   - Skips already-processed samples\n",
    "   - Continues from where it left off\n",
    "\n",
    "**‚ö†Ô∏è Configuration Validation**:\n",
    "\n",
    "The system now validates your configuration matches the checkpoint:\n",
    "- ‚úì If match ‚Üí Resume safely\n",
    "- ‚ùå If mismatch ‚Üí Clear error message explaining the problem\n",
    "\n",
    "**Example error if config changed**:\n",
    "```\n",
    "‚ùå CONFIGURATION MISMATCH - Cannot resume training!\n",
    "\n",
    "Mismatches detected:\n",
    "  - num_nodes: checkpoint=5, current=4\n",
    "  - chunk_sizes: checkpoint=[8,8,8,8,8], current=[10,10,10,10]\n",
    "\n",
    "To fix:\n",
    "  1. Recreate learner with EXACT same configuration\n",
    "  2. Or delete checkpoint (./checkpoints/) and start fresh\n",
    "  3. Or use different checkpoint_dir for new configuration\n",
    "```\n",
    "\n",
    "**üí° Pro tip**: Take a screenshot of your configuration before long training runs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hardware Analysis (Optional but Recommended)\n",
    "\n",
    "Analyze your system to:\n",
    "- Understand hardware capabilities\n",
    "- Estimate training time for different dataset sizes\n",
    "- Identify performance bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current hardware\n",
    "hw_analyzer = HardwareAnalyzerV2(verbose=True)\n",
    "\n",
    "# Example training config for accurate throughput estimate\n",
    "# (adjust these to match your planned training configuration)\n",
    "example_config = {\n",
    "    'chunk_sizes': cs_array,\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': 6\n",
    "}\n",
    "\n",
    "hw_report = hw_analyzer.analyze_system(\n",
    "    mongodb_uri='mongodb://kato-mongodb:27017/',\n",
    "    kato_url='http://localhost:8000',\n",
    "    training_config=example_config,  # Config-aware estimation\n",
    "    num_samples=10000\n",
    ")\n",
    "\n",
    "hw_report.print_summary()\n",
    "\n",
    "# Save hardware baseline for reference\n",
    "hw_report.export_json('hardware_baseline.json')\n",
    "\n",
    "# Extract key metrics\n",
    "BASELINE_THROUGHPUT = hw_report.estimated_samples_per_sec\n",
    "HARDWARE_TIER = hw_report.tier\n",
    "\n",
    "print(f\"\\nüéØ HARDWARE BASELINE\")\n",
    "print(f\"  Estimated throughput: {BASELINE_THROUGHPUT:.1f} samples/sec\")\n",
    "print(f\"  (for chunk_size={example_config['chunk_sizes'][0]}, batch={example_config['batch_size']}, 10K samples)\")\n",
    "print(f\"  Hardware tier: {HARDWARE_TIER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Storage Estimation (Optional)\n",
    "\n",
    "Estimate MongoDB storage requirements using Zipfian distribution modeling.\n",
    "\n",
    "This helps you plan disk space before training large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create storage estimator with auto-calibration\n",
    "# Auto-calibration uses historical training data to refine Zipfian parameters\n",
    "storage_est = StorageEstimator(verbose=True, auto_calibrate=True)\n",
    "\n",
    "# Example configuration (adjust to match your training config)\n",
    "config = {\n",
    "    'num_levels': 4,\n",
    "    'chunk_sizes': [3,5,8],  # Uniform chunk_size=8\n",
    "    'tokenizer': 'gpt2'\n",
    "}\n",
    "\n",
    "dataset_stats = {\n",
    "    'avg_tokens_per_sample': 500,\n",
    "    'dataset_name': 'wikitext'\n",
    "}\n",
    "\n",
    "# Estimate for your planned training size\n",
    "print(\"\\nüìä STORAGE ESTIMATES\\n\")\n",
    "\n",
    "# for num_samples in [1_000, 10_000, 100_000]:\n",
    "for num_samples in [100_000, 1_000_000]:\n",
    "    estimate = storage_est.estimate_storage(\n",
    "        num_samples=num_samples,\n",
    "        config=config,\n",
    "        dataset_stats=dataset_stats\n",
    "    )\n",
    "    \n",
    "    print(f\"{num_samples:>10,} samples: {estimate.estimated_storage_with_overhead_gb:>8.2f} GB \")\n",
    "    print(f\"             Total patterns: {estimate.total_patterns:,}\")\n",
    "    if storage_est.calibrated_zipf_alpha:\n",
    "        print(f\"             Zipfian Œ±: {storage_est.calibrated_zipf_alpha:.3f} (calibrated)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Hierarchical Learner\n",
    "\n",
    "**Key Decision**: Choose `chunk_size` based on your dataset.\n",
    "\n",
    "**Recommended configurations**:\n",
    "- **WikiText (500-2K tokens/sample)**: `chunk_size=8` with 4 levels ‚Üí covers 8‚Üí64‚Üí512‚Üí4K tokens\n",
    "- **C4/RefinedWeb (300-3K tokens)**: `chunk_size=6` with 4 levels ‚Üí covers 6‚Üí36‚Üí216‚Üí1.3K tokens\n",
    "- **BookCorpus (50K+ tokens)**: `chunk_size=8` with 5-6 levels for book-length coverage\n",
    "\n",
    "**See PROJECT_OVERVIEW.md Section 7** for detailed hierarchy sizing guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure hierarchical nodes\n# RECOMMENDED: Use chunk_size=8 for WikiText (exponential semantic scaling)\n\nnodes = [\n    HierarchicalNode('node0', chunk_size=cs_array[0], mode='chunking', base_url='http://kato:8000'),\n    HierarchicalNode('node1', chunk_size=cs_array[1], mode='chunking', base_url='http://kato:8000'),\n    HierarchicalNode('node2', chunk_size=cs_array[2], mode='chunking', base_url='http://kato:8000'),\n    HierarchicalNode('node3', chunk_size=cs_array[3], mode='chunking', base_url='http://kato:8000')\n]\n\nlearner = HierarchicalConceptLearner(\n    nodes=nodes,\n    tokenizer_name='gpt2',\n    node0_batch_size=batch_size  # Batching for 4-7x speedup\n)\n\nprint(f\"‚úì Created hierarchical learner with {learner.num_nodes} nodes\")\nprint(f\"  Chunk size: {learner.node_configs[0].chunk_size}\")\nprint(f\"  Node0 batch size: {learner.node0_batch_size} (batching ENABLED)\")\nprint(f\"\\n  Semantic coverage:\")\ncoverage = learner.node_configs[0].chunk_size\nfor i in range(learner.num_nodes):\n    print(f\"    node{i}: {coverage:,} tokens\")\n    coverage *= learner.node_configs[0].chunk_size\n\n# Clear all node knowledgebases ONLY if starting fresh (not resuming)\nif RESUME_FROM_CHECKPOINT:\n    print(f\"\\nüìÇ RESUME MODE: Keeping existing MongoDB data\")\n    print(f\"   ‚úì Patterns from previous training will be preserved\")\n    print(f\"   ‚úì Training will continue from checkpoint\")\nelse:\n    print(\"\\nüßπ Clearing all node knowledgebases...\")\n    for i, node in enumerate(learner.nodes.values()):\n        node.clear_all_memory()\n        print(f\"  ‚úì node{i} cleared\")\n    print(\"‚úì All nodes cleared and ready for fresh training\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Training History\n",
    "\n",
    "Training history tracks all runs in SQLite for comparison and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training history database\n",
    "history = TrainingHistory(db_path='./training_history.db', verbose=True)\n",
    "\n",
    "# Show current state\n",
    "history.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Training Time Estimator (NEW!)\n",
    "\n",
    "**Predict training time** before you start, based on 29 historical training runs.\n",
    "\n",
    "The TrainingEstimator uses real data to provide accurate estimates that account for:\n",
    "- **chunk_size** (exponential impact - most important factor)\n",
    "- **batch_size** (linear speedup)\n",
    "- **scale** (logarithmic slowdown at larger datasets)\n",
    "- **workers** (sub-linear scaling)\n",
    "- **hardware tier** (existing multipliers)\n",
    "\n",
    "**Key insight**: Performance is dominated by minimum chunk_size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calibrated chunk effect: rate ‚àù chunk^0.80, base=1.96\n",
      "  Calibrated batch effect: rate ‚àù batch^0.50\n",
      "  Calibrated scale effect: coefficient=0.020\n",
      "  Throughput variance: CV=0.35\n",
      "‚úì TrainingEstimator initialized with 29 runs\n",
      "\n",
      "================================================================================\n",
      "ESTIMATOR ACCURACY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ESTIMATOR VALIDATION\n",
      "================================================================================\n",
      "Historical runs: 29\n",
      "MAE: 0.635 samples/sec\n",
      "MAPE: 24.6%\n",
      "RMSE: 0.909 samples/sec\n",
      "R¬≤: 0.147\n",
      "================================================================================\n",
      "\n",
      "Estimator is 75.4% accurate on average\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING TIME PREDICTION\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING TIME ESTIMATE\n",
      "================================================================================\n",
      "\n",
      "üìã CONFIGURATION\n",
      "  Samples: 100,000\n",
      "  Chunk sizes: [8, 8, 8, 8, 8]\n",
      "  Batch size: 100\n",
      "  Workers: 6\n",
      "\n",
      "‚ö° PERFORMANCE ESTIMATE\n",
      "  Throughput: 5.82 samples/sec\n",
      "  Estimated time: 286.2 minutes (4.77 hours)\n",
      "  Confidence interval (80%): 158.5 - 413.9 minutes\n",
      "\n",
      "üîç BREAKDOWN\n",
      "  Base rate: 1.96 samples/sec\n",
      "  Chunk size multiplier: 2.19x\n",
      "  Batch size multiplier: 1.41x\n",
      "  Scale multiplier: 0.96x\n",
      "  Worker multiplier: 1.00x\n",
      "  Hardware multiplier: 1.00x\n",
      "\n",
      "üìä CONFIDENCE\n",
      "  Estimation confidence: HIGH\n",
      "  Based on 29 historical runs\n",
      "\n",
      "‚ö†Ô∏è  WARNINGS\n",
      "  ‚Ä¢ 100,000 samples is 2x larger than largest historical run\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CHUNK SIZE COMPARISON (for 10K samples)\n",
      "================================================================================\n",
      "\n",
      "chunk_size=8: 286.2 min (5.82 samples/sec)\n",
      "chunk_size=8: 286.2 min (5.82 samples/sec)\n",
      "chunk_size=8: 286.2 min (5.82 samples/sec)\n",
      "chunk_size=8: 286.2 min (5.82 samples/sec)\n",
      "chunk_size=8: 286.2 min (5.82 samples/sec)\n",
      "\n",
      "üí° TIP: Larger chunk sizes train MUCH faster (exponential speedup)\n",
      "   chunk_size=3 ‚Üí chunk_size=8 gives ~3x speedup!\n"
     ]
    }
   ],
   "source": [
    "# Initialize training time estimator (calibrated from historical runs)\n",
    "time_estimator = TrainingEstimator(verbose=True)\n",
    "\n",
    "# Validate estimator accuracy against historical data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTIMATOR ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "validation_metrics = time_estimator.validate_against_history(verbose=True)\n",
    "print(f\"\\nEstimator is {100 - validation_metrics['mape']:.1f}% accurate on average\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define your planned training configuration\n",
    "planned_config = {\n",
    "    'chunk_sizes': cs_array,  # Adjust to match section 4\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': 6\n",
    "}\n",
    "\n",
    "# Get time estimate\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING TIME PREDICTION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "time_estimate = time_estimator.estimate_training(\n",
    "    config=planned_config,\n",
    "    num_samples=MAX_SAMPLES,\n",
    "    hardware_tier=HARDWARE_TIER if 'HARDWARE_TIER' in dir() else 'medium'\n",
    ")\n",
    "\n",
    "time_estimate.print_summary()\n",
    "\n",
    "# Compare different chunk sizes\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CHUNK SIZE COMPARISON (for 10K samples)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for chunk_size in cs_array:\n",
    "    test_config = {\n",
    "        'chunk_sizes': [chunk_size] * 5,\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': 6\n",
    "    }\n",
    "    est = time_estimator.estimate_training(\n",
    "        config=test_config,\n",
    "        num_samples=MAX_SAMPLES,\n",
    "        hardware_tier=HARDWARE_TIER if 'HARDWARE_TIER' in dir() else 'medium'\n",
    "    )\n",
    "    print(f\"chunk_size={chunk_size}: {est.estimated_time_minutes:.1f} min ({est.estimated_samples_per_sec:.2f} samples/sec)\")\n",
    "\n",
    "print(f\"\\nüí° TIP: Larger chunk sizes train MUCH faster (exponential speedup)\")\n",
    "print(f\"   chunk_size=3 ‚Üí chunk_size=8 gives ~3x speedup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train with Real Data (Parallel + Profiling)\n",
    "\n",
    "**This is the main training step**.\n",
    "\n",
    "**‚ö†Ô∏è Requires**:\n",
    "- KATO server running at localhost:8000\n",
    "- MongoDB running at localhost:27017\n",
    "\n",
    "**üìä Note about Pattern Counts**:\n",
    "- Pattern counts will show as 0 after training (MongoDB connection limit with parallel workers)\n",
    "- **Patterns are successfully stored** via KATO API\n",
    "- Use `analysis.ipynb` after training for accurate pattern counts and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If training stalls, re-run this cell (make sure RESUME_FROM_CHECKPOINT = True in settings of cell 2.)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dataset: {DATASET_KEY}\")\n",
    "print(f\"Samples: {MAX_SAMPLES:,}\")\n",
    "print(f\"Workers: {NUM_WORKERS}\")\n",
    "print(f\"Connections: {NUM_WORKERS * learner.num_nodes} (workers √ó nodes)\")\n",
    "print(f\"Nodes: {learner.num_nodes}\")\n",
    "print(f\"Chunk size: {nodes[0].chunk_size}\")\n",
    "print(f\"Batch size: {learner.node0_batch_size}\")\n",
    "print(f\"Checkpoint interval: {CHECKPOINT_INTERVAL:,} samples\")\n",
    "print(f\"Resume from checkpoint: {RESUME_FROM_CHECKPOINT}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Start profiling\n",
    "profiler = ProfilingEngine(sampling_interval_seconds=1.0, verbose=True)\n",
    "profiler.start()\n",
    "\n",
    "# Train with parallel workers (profiler is REQUIRED for performance analysis)\n",
    "stats = train_from_streaming_dataset_parallel(\n",
    "    dataset_key=DATASET_KEY,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    learner=learner,\n",
    "    profiler=profiler,  # REQUIRED - tracks samples/sec, CPU, memory for analysis.ipynb\n",
    "    num_levels=learner.num_nodes,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    segment_method='simple',\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL,  # NEW: Auto-checkpoint\n",
    "    checkpoint_dir='./checkpoints',           # NEW: Checkpoint directory\n",
    "    resume_from_checkpoint=RESUME_FROM_CHECKPOINT,  # NEW: Resume support\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Stop profiling\n",
    "profiler.stop()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nPerformance Statistics:\")\n",
    "print(f\"  Samples processed: {stats['samples_processed']:,}\")\n",
    "print(f\"  Samples attempted: {stats.get('samples_attempted', stats['samples_processed']):,}\")\n",
    "print(f\"  Total time: {stats['total_time_seconds']:.2f}s\")\n",
    "print(f\"  Rate: {stats['rate_samples_per_sec']:.2f} samples/sec\")\n",
    "print(f\"  Workers: {stats.get('num_workers', 'N/A')}\")\n",
    "print(f\"  Checkpoints saved: {stats.get('checkpoints_saved', 0)}\")\n",
    "print(f\"\\nüìä For pattern counts and analysis: Open analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Profiling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record this training run for later comparison\n",
    "config = {\n",
    "    'num_levels': learner.num_nodes,\n",
    "    'chunk_sizes': [n.chunk_size for n in learner.node_configs],\n",
    "    'batch_size': learner.node0_batch_size,\n",
    "    'num_workers': NUM_WORKERS\n",
    "}\n",
    "\n",
    "# Calculate estimated time if estimator was used\n",
    "if 'time_estimate' in dir() and time_estimate is not None:\n",
    "    estimated_time_sec = time_estimate.estimated_time_seconds\n",
    "    estimated_storage = None  # Could add storage estimate here too\n",
    "else:\n",
    "    estimated_time_sec = None\n",
    "    estimated_storage = None\n",
    "\n",
    "run_id = history.record_run(\n",
    "    config=config,\n",
    "    estimated_time=estimated_time_sec,  # Use prediction from Section 5a\n",
    "    actual_time=stats['total_time_seconds'],\n",
    "    estimated_storage_gb=estimated_storage,\n",
    "    actual_storage_gb=profiling_report.total_disk_write_mb / 1024,\n",
    "    samples_processed=MAX_SAMPLES,\n",
    "    patterns_learned={\n",
    "        f'node{i}_patterns': stats.get(f'node{i}_patterns', 0)\n",
    "        for i in range(learner.num_nodes)\n",
    "    },\n",
    "    profiling_report=profiling_report,\n",
    "    dataset_key=DATASET_KEY,\n",
    "    hardware_tier=HARDWARE_TIER if 'HARDWARE_TIER' in dir() else 'unknown',\n",
    "    notes=f'Parallel training with {NUM_WORKERS} workers on {MAX_SAMPLES} samples'\n",
    ")\n",
    "\n",
    "# Show estimation accuracy if we had a prediction\n",
    "if estimated_time_sec:\n",
    "    actual_time_min = stats['total_time_seconds'] / 60\n",
    "    estimated_time_min = estimated_time_sec / 60\n",
    "    error_pct = abs(estimated_time_sec - stats['total_time_seconds']) / stats['total_time_seconds'] * 100\n",
    "    \n",
    "    print(f\"\\nüìä ESTIMATION ACCURACY\")\n",
    "    print(f\"  Estimated: {estimated_time_min:.1f} minutes\")\n",
    "    print(f\"  Actual: {actual_time_min:.1f} minutes\")\n",
    "    print(f\"  Error: {error_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úì Training run recorded in history: {run_id}\")\n",
    "print(f\"\\nüéâ TRAINING SESSION COMPLETE!\")\n",
    "print(f\"\\nüìä Next step: Open analysis.ipynb to analyze learned patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Record Training Run in History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record this training run for later comparison\n",
    "config = {\n",
    "    'num_levels': learner.num_nodes,\n",
    "    'chunk_sizes': [n.chunk_size for n in learner.node_configs],\n",
    "    'batch_size': learner.node0_batch_size,\n",
    "    'num_workers': NUM_WORKERS\n",
    "}\n",
    "\n",
    "run_id = history.record_run(\n",
    "    config=config,\n",
    "    estimated_time=None,  # We didn't pre-estimate\n",
    "    actual_time=stats['total_time_seconds'],\n",
    "    estimated_storage_gb=None,\n",
    "    actual_storage_gb=profiling_report.total_disk_write_mb / 1024,\n",
    "    samples_processed=MAX_SAMPLES,\n",
    "    patterns_learned={\n",
    "        f'node{i}_patterns': stats.get(f'node{i}_patterns', 0)\n",
    "        for i in range(learner.num_nodes)\n",
    "    },\n",
    "    profiling_report=profiling_report,\n",
    "    dataset_key=DATASET_KEY,\n",
    "    hardware_tier=HARDWARE_TIER if 'HARDWARE_TIER' in dir() else 'unknown',\n",
    "    notes=f'Parallel training with {NUM_WORKERS} workers on {MAX_SAMPLES} samples'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Training run recorded in history: {run_id}\")\n",
    "print(f\"\\nüéâ TRAINING SESSION COMPLETE!\")\n",
    "print(f\"\\nüìä Next step: Open analysis.ipynb to analyze learned patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture training snapshot (BEFORE clearing for next run!)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CAPTURING TRAINING SNAPSHOT\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "run_snapshot = history.capture_snapshot(\n",
    "    learner=learner,\n",
    "    run_id=run_id,\n",
    "    mongo_uri='mongodb://kato-mongodb:27017/',\n",
    "    snapshots_dir='./snapshots',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüì∏ SNAPSHOT SUMMARY:\")\n",
    "print(f\"  Total patterns: {run_snapshot.total_patterns:,}\")\n",
    "print(f\"  Total storage: {run_snapshot.total_storage_mb:.2f} MB\")\n",
    "print(f\"  Total observations: {run_snapshot.total_observations:,}\")\n",
    "\n",
    "print(f\"\\n  Per-node breakdown:\")\n",
    "for node_name in sorted(run_snapshot.nodes.keys()):\n",
    "    ns = run_snapshot.nodes[node_name]\n",
    "    print(f\"    {node_name}: {ns.total_patterns:,} patterns, {ns.db_size_mb:.2f} MB\")\n",
    "    if ns.zipf_alpha:\n",
    "        print(f\"             Zipf Œ±={ns.zipf_alpha:.3f}, mean_freq={ns.mean_frequency:.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úì Snapshot captured and saved\")\n",
    "print(f\"  Use analysis.ipynb to compare with other runs\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### üìä Hierarchy Metrics (NEW!)\n",
    "Open **`hierarchy_dashboard.ipynb`** for:\n",
    "- **Quick health check** (5-tier scoring system)\n",
    "- At-a-glance hierarchy quality assessment\n",
    "- Actionable recommendations\n",
    "- Immediate issue detection\n",
    "\n",
    "Open **`hierarchy_metrics.ipynb`** for:\n",
    "- **Comprehensive analysis** of all 15 metrics\n",
    "- Graph topology evaluation\n",
    "- Information-theoretic analysis\n",
    "- Training dynamics visualization\n",
    "- Detailed interpretation guide\n",
    "\n",
    "**15 Metrics Across 6 Categories**:\n",
    "1. Compression (ratios, counts, effectiveness)\n",
    "2. Connectivity (reusability, coverage, branching)\n",
    "3. Information Theory (MI, entropy, constraints)\n",
    "4. Prediction (fan-out)\n",
    "5. Context (alignment, diversity)\n",
    "6. Training Dynamics (growth, reusability trends)\n",
    "\n",
    "### üìä Traditional Analysis\n",
    "Open **`analysis.ipynb`** to:\n",
    "- Visualize frequency distributions\n",
    "- Inspect high-frequency patterns\n",
    "- Compare multiple training runs\n",
    "- Clean up low-frequency noise\n",
    "\n",
    "### üî¨ Experimentation\n",
    "To find optimal configurations:\n",
    "1. Try different `chunk_size` values (5, 8, 10, 15, 20)\n",
    "2. Test different number of levels (3, 4, 5, 6)\n",
    "3. Compare training runs using TrainingHistory\n",
    "4. **Use hierarchy metrics** to validate improvements\n",
    "\n",
    "### üìà Scale Up\n",
    "Once you've found good settings (via hierarchy metrics):\n",
    "- Increase `MAX_SAMPLES` (10K ‚Üí 100K ‚Üí 1M+)\n",
    "- Use larger datasets (C4, RefinedWeb)\n",
    "- Monitor hierarchy health over time\n",
    "\n",
    "### üìö Documentation\n",
    "- **hierarchy_metrics/README.md**: Complete metrics guide\n",
    "- **PROJECT_OVERVIEW.md**: Core concepts and philosophy\n",
    "- **TRAINING_RUN_COMPARISON.md**: How to compare experiments\n",
    "- **README.md**: Full feature list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Capture Hierarchy Metrics (Graph-Based Analysis)\n",
    "\n",
    "**IMPORTANT**: This must run BEFORE clearing MongoDB for the next run.\n",
    "\n",
    "Captures comprehensive graph-based metrics including:\n",
    "- Compression ratios and pattern counts\n",
    "- Connectivity (reusability, coverage, branching)\n",
    "- Graph topology and relationships\n",
    "\n",
    "Use `hierarchy_metrics.ipynb` or `hierarchy_dashboard.ipynb` to analyze results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### üìä Analysis\n",
    "Open **`analysis.ipynb`** to:\n",
    "- Visualize frequency distributions\n",
    "- Inspect high-frequency patterns\n",
    "- Compare multiple training runs\n",
    "- Clean up low-frequency noise\n",
    "\n",
    "### üî¨ Experimentation\n",
    "To find optimal configurations:\n",
    "1. Try different `chunk_size` values (5, 8, 10, 15, 20)\n",
    "2. Test different number of levels (3, 4, 5, 6)\n",
    "3. Compare training runs using TrainingHistory\n",
    "4. Analyze which configurations produce best patterns\n",
    "\n",
    "### üìà Scale Up\n",
    "Once you've found good settings:\n",
    "- Increase `MAX_SAMPLES` (10K ‚Üí 100K ‚Üí 1M+)\n",
    "- Use larger datasets (C4, RefinedWeb)\n",
    "- Monitor storage growth with estimates\n",
    "\n",
    "### üìö Documentation\n",
    "- **PROJECT_OVERVIEW.md**: Core concepts and philosophy\n",
    "- **TRAINING_RUN_COMPARISON.md**: How to compare experiments\n",
    "- **README.md**: Full feature list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}