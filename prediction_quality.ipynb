{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Prediction Quality Analysis\n",
    "\n",
    "**Purpose**: Estimate text generation quality from training snapshot statistics WITHOUT running actual generation tests.\n",
    "\n",
    "**This notebook analyzes:**\n",
    "- âœ… **Zipf Quality**: Level-specific alpha targets (node0: 1.0-1.5, node1: 0.8-1.2, etc.)\n",
    "- âœ… **Composition Quality**: Orphan rates and coverage metrics\n",
    "- âœ… **Prediction Quality**: Fan-out and predictive_information scores\n",
    "- âœ… **Hierarchical Consistency**: Frequency correlation validation\n",
    "- âœ… **Overall HGR Score**: 0-100 Hierarchical Generation Readiness score\n",
    "\n",
    "**Key Innovation**: Predict generation quality from pattern statistics alone, enabling:\n",
    "- Configuration optimization before generation implementation\n",
    "- Multi-run comparison and ranking\n",
    "- Actionable recommendations for improvement\n",
    "\n",
    "**Prerequisites**: \n",
    "- Completed training run with enhanced snapshot capture\n",
    "- Snapshot file in `./snapshots/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import (\n",
    "    TrainingHistory,\n",
    "    TrainingRunSnapshot,\n",
    ")\n",
    "\n",
    "from tools.prediction_quality_estimator import (\n",
    "    compute_zipf_quality_score,\n",
    "    compute_fanout_quality_score,\n",
    "    compute_composition_quality_score,\n",
    "    compute_hierarchical_consistency_score,\n",
    "    compute_predictive_information_quality,\n",
    "    compute_generation_readiness_score,\n",
    "    compare_snapshots,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"âœ“ All modules imported successfully\")\n",
    "print(\"âœ“ Ready for prediction quality analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Training Snapshots\n",
    "\n",
    "Load snapshots from previous training runs for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training history\n",
    "history = TrainingHistory(db_path='./training_history.db', verbose=True)\n",
    "history.print_summary()\n",
    "\n",
    "# Get recent runs\n",
    "recent_runs = history.get_recent_runs(n=10)\n",
    "\n",
    "print(f\"\\nðŸ“‹ Found {len(recent_runs)} recent training runs:\")\n",
    "for run in recent_runs:\n",
    "    print(f\"  - {run['run_id']}: {run['samples_processed']:,} samples, \"\n",
    "          f\"chunk_size={run['config'].get('chunk_sizes', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load snapshots for analysis\n",
    "# Option 1: Load specific run by ID\n",
    "# run_id = 'run_YYYYMMDD_HHMMSS'\n",
    "# snapshot = TrainingRunSnapshot.load(f'./snapshots/{run_id}_snapshot.json')\n",
    "\n",
    "# Option 2: Load most recent run\n",
    "if recent_runs:\n",
    "    latest_run = recent_runs[0]\n",
    "    snapshot_path = f\"./snapshots/{latest_run['run_id']}_snapshot.json\"\n",
    "    \n",
    "    if Path(snapshot_path).exists():\n",
    "        snapshot = TrainingRunSnapshot.load(snapshot_path)\n",
    "        print(f\"\\nâœ“ Loaded snapshot: {snapshot.run_id}\")\n",
    "        print(f\"  Total patterns: {snapshot.total_patterns:,}\")\n",
    "        print(f\"  Node count: {len(snapshot.nodes)}\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Snapshot file not found: {snapshot_path}\")\n",
    "        print(f\"  Please run training.ipynb Cell 24 to capture snapshot\")\n",
    "        snapshot = None\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No training runs found. Please run training.ipynb first.\")\n",
    "    snapshot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pattern Coverage Analysis\n",
    "\n",
    "Analyze how well patterns participate in the hierarchy:\n",
    "- **Vocabulary richness**: unique_patterns / total_observations\n",
    "- **Orphan rate**: % patterns with no parents\n",
    "- **Coverage**: % patterns used by parent level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshot:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PATTERN COVERAGE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    node_names = sorted(snapshot.nodes.keys())\n",
    "    \n",
    "    # Collect coverage metrics\n",
    "    coverage_data = []\n",
    "    \n",
    "    for node_name in node_names:\n",
    "        ns = snapshot.nodes[node_name]\n",
    "        comp_result = compute_composition_quality_score(snapshot, node_name)\n",
    "        \n",
    "        vocab_richness = ns.total_patterns / ns.total_observations if ns.total_observations > 0 else 0\n",
    "        \n",
    "        coverage_data.append({\n",
    "            'node': node_name,\n",
    "            'patterns': ns.total_patterns,\n",
    "            'vocab_richness': vocab_richness,\n",
    "            'orphan_rate': comp_result['orphan_rate'] or 0,\n",
    "            'coverage': comp_result['coverage'] or 0,\n",
    "            'status': comp_result['status']\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{node_name}:\")\n",
    "        print(f\"  Patterns: {ns.total_patterns:,}\")\n",
    "        print(f\"  Vocab richness: {vocab_richness:.3f} (target: 0.3-0.7)\")\n",
    "        print(f\"  Orphan rate: {comp_result['orphan_rate']:.1%} (target: <10%)\")\n",
    "        print(f\"  Coverage: {comp_result['coverage']:.1%} (target: >70%)\")\n",
    "        print(f\"  Quality: {comp_result['score']:.2f}/1.0 [{comp_result['status'].upper()}]\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Vocabulary richness\n",
    "    ax = axes[0]\n",
    "    vocab_vals = [d['vocab_richness'] for d in coverage_data]\n",
    "    ax.bar(range(len(node_names)), vocab_vals, color='skyblue', edgecolor='black')\n",
    "    ax.axhline(0.3, color='green', linestyle='--', label='Target min', alpha=0.7)\n",
    "    ax.axhline(0.7, color='green', linestyle='--', label='Target max', alpha=0.7)\n",
    "    ax.set_xticks(range(len(node_names)))\n",
    "    ax.set_xticklabels(node_names)\n",
    "    ax.set_ylabel('Vocabulary Richness')\n",
    "    ax.set_title('Vocabulary Richness by Level')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Orphan rates\n",
    "    ax = axes[1]\n",
    "    orphan_vals = [d['orphan_rate'] * 100 for d in coverage_data]\n",
    "    colors = ['red' if v > 30 else 'orange' if v > 20 else 'yellow' if v > 10 else 'green' \n",
    "              for v in orphan_vals]\n",
    "    ax.bar(range(len(node_names)), orphan_vals, color=colors, edgecolor='black')\n",
    "    ax.axhline(10, color='green', linestyle='--', label='Target (<10%)', alpha=0.7)\n",
    "    ax.set_xticks(range(len(node_names)))\n",
    "    ax.set_xticklabels(node_names)\n",
    "    ax.set_ylabel('Orphan Rate (%)')\n",
    "    ax.set_title('Orphan Rate by Level (Lower is Better)')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Coverage\n",
    "    ax = axes[2]\n",
    "    coverage_vals = [d['coverage'] * 100 for d in coverage_data]\n",
    "    colors = ['green' if v > 70 else 'yellow' if v > 50 else 'orange' if v > 30 else 'red'\n",
    "              for v in coverage_vals]\n",
    "    ax.bar(range(len(node_names)), coverage_vals, color=colors, edgecolor='black')\n",
    "    ax.axhline(70, color='green', linestyle='--', label='Target (>70%)', alpha=0.7)\n",
    "    ax.set_xticks(range(len(node_names)))\n",
    "    ax.set_xticklabels(node_names)\n",
    "    ax.set_ylabel('Coverage (%)')\n",
    "    ax.set_title('Coverage by Level (Higher is Better)')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Frequency Distribution Quality\n",
    "\n",
    "Analyze Zipfian power-law quality with **level-specific targets**:\n",
    "- **node0**: Î± â‰ˆ 1.0-1.5 (classic Zipfian)\n",
    "- **node1**: Î± â‰ˆ 0.8-1.2 (moderate power-law)\n",
    "- **node2**: Î± â‰ˆ 0.5-1.0 (weaker power-law)\n",
    "- **node3**: Î± â‰ˆ 0.2-0.5 (nearly uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshot:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FREQUENCY DISTRIBUTION QUALITY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    zipf_results = []\n",
    "    \n",
    "    for node_name in node_names:\n",
    "        zipf_result = compute_zipf_quality_score(snapshot, node_name)\n",
    "        zipf_results.append(zipf_result)\n",
    "        \n",
    "        if zipf_result['alpha'] is not None:\n",
    "            print(f\"\\n{node_name}:\")\n",
    "            print(f\"  Measured Î±: {zipf_result['alpha']:.3f}\")\n",
    "            print(f\"  Target range: {zipf_result['target_range']}\")\n",
    "            print(f\"  Ideal Î±: {zipf_result['ideal']:.2f}\")\n",
    "            print(f\"  Deviation: {zipf_result['deviation']:.3f}\")\n",
    "            print(f\"  Score: {zipf_result['score']:.2f}/1.0 [{zipf_result['status'].upper()}]\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Alpha values with target ranges\n",
    "    ax = axes[0]\n",
    "    alphas = [r['alpha'] for r in zipf_results if r['alpha'] is not None]\n",
    "    ideals = [r['ideal'] for r in zipf_results if r['alpha'] is not None]\n",
    "    mins = [r['target_range'][0] for r in zipf_results if r['alpha'] is not None]\n",
    "    maxs = [r['target_range'][1] for r in zipf_results if r['alpha'] is not None]\n",
    "    valid_nodes = [node_names[i] for i, r in enumerate(zipf_results) if r['alpha'] is not None]\n",
    "    \n",
    "    x_pos = range(len(valid_nodes))\n",
    "    ax.plot(x_pos, alphas, 'o-', color='blue', markersize=10, linewidth=2, label='Measured Î±')\n",
    "    ax.plot(x_pos, ideals, 's--', color='green', markersize=8, linewidth=1.5, label='Ideal Î±')\n",
    "    ax.fill_between(x_pos, mins, maxs, alpha=0.2, color='green', label='Target range')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(valid_nodes)\n",
    "    ax.set_ylabel('Zipfian Alpha (Î±)')\n",
    "    ax.set_title('Zipfian Alpha by Level')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Quality scores\n",
    "    ax = axes[1]\n",
    "    scores = [r['score'] * 100 for r in zipf_results if r['alpha'] is not None]\n",
    "    colors = ['green' if s > 80 else 'yellow' if s > 60 else 'orange' if s > 40 else 'red'\n",
    "              for s in scores]\n",
    "    ax.bar(x_pos, scores, color=colors, edgecolor='black')\n",
    "    ax.axhline(80, color='green', linestyle='--', alpha=0.7, label='Excellent (>80)')\n",
    "    ax.axhline(60, color='yellow', linestyle='--', alpha=0.7, label='Good (>60)')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(valid_nodes)\n",
    "    ax.set_ylabel('Quality Score')\n",
    "    ax.set_title('Zipf Quality Score by Level')\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Consistency\n",
    "\n",
    "Validate that parent frequencies correlate with sum of child frequencies.\n",
    "\n",
    "**Expected**: Strong positive correlation (>0.7)\n",
    "- If parent appears 10 times, its children should sum to ~10 occurrences\n",
    "- Low correlation indicates structural issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshot:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL CONSISTENCY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    consistency_results = []\n",
    "    \n",
    "    # Skip node0 (no children)\n",
    "    for node_name in node_names[1:]:\n",
    "        consistency_result = compute_hierarchical_consistency_score(snapshot, node_name)\n",
    "        consistency_results.append((node_name, consistency_result))\n",
    "        \n",
    "        print(f\"\\n{node_name} vs {node_names[node_names.index(node_name) - 1]}:\")\n",
    "        if consistency_result['freq_correlation'] is not None:\n",
    "            print(f\"  Frequency correlation: {consistency_result['freq_correlation']:.3f} (target: >0.7)\")\n",
    "            print(f\"  Score: {consistency_result['score']:.2f}/1.0 [{consistency_result['status'].upper()}]\")\n",
    "        else:\n",
    "            print(f\"  Frequency correlation: N/A (insufficient data)\")\n",
    "        \n",
    "        if consistency_result['freq_compression_ratio'] is not None:\n",
    "            print(f\"  Compression ratio: {consistency_result['freq_compression_ratio']:.2f}x\")\n",
    "    \n",
    "    # Visualize\n",
    "    valid_results = [(n, r) for n, r in consistency_results if r['freq_correlation'] is not None]\n",
    "    \n",
    "    if valid_results:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Correlation values\n",
    "        ax = axes[0]\n",
    "        nodes_list = [n for n, r in valid_results]\n",
    "        corrs = [r['freq_correlation'] for n, r in valid_results]\n",
    "        colors = ['green' if c >= 0.7 else 'yellow' if c >= 0.5 else 'orange' if c >= 0.3 else 'red'\n",
    "                  for c in corrs]\n",
    "        ax.bar(range(len(nodes_list)), corrs, color=colors, edgecolor='black')\n",
    "        ax.axhline(0.7, color='green', linestyle='--', alpha=0.7, label='Target (>0.7)')\n",
    "        ax.set_xticks(range(len(nodes_list)))\n",
    "        ax.set_xticklabels(nodes_list)\n",
    "        ax.set_ylabel('Spearman Correlation')\n",
    "        ax.set_title('Parent-Child Frequency Correlation')\n",
    "        ax.set_ylim(-0.1, 1.0)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Quality scores\n",
    "        ax = axes[1]\n",
    "        scores = [r['score'] * 100 for n, r in valid_results]\n",
    "        colors = ['green' if s > 80 else 'yellow' if s > 60 else 'orange' if s > 40 else 'red'\n",
    "                  for s in scores]\n",
    "        ax.bar(range(len(nodes_list)), scores, color=colors, edgecolor='black')\n",
    "        ax.axhline(80, color='green', linestyle='--', alpha=0.7, label='Excellent (>80)')\n",
    "        ax.set_xticks(range(len(nodes_list)))\n",
    "        ax.set_xticklabels(nodes_list)\n",
    "        ax.set_ylabel('Quality Score')\n",
    "        ax.set_title('Consistency Quality Score')\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Quality Estimation\n",
    "\n",
    "Analyze sampled predictions to estimate generation quality:\n",
    "- **Predictive information**: How reliably patterns predict their futures\n",
    "- **Potential scores**: similarity Ã— predictive_information\n",
    "- **Fan-out**: Number of predictions per query (level-specific targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshot:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDICTION QUALITY ESTIMATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check if prediction samples were captured\n",
    "    has_predictions = any(\n",
    "        snapshot.nodes[n].prediction_samples is not None \n",
    "        for n in node_names\n",
    "    )\n",
    "    \n",
    "    if has_predictions:\n",
    "        for node_name in node_names:\n",
    "            ns = snapshot.nodes[node_name]\n",
    "            \n",
    "            if ns.prediction_samples:\n",
    "                print(f\"\\n{node_name}:\")\n",
    "                \n",
    "                # Predictive information\n",
    "                pi_result = compute_predictive_information_quality(snapshot, node_name)\n",
    "                if pi_result['mean_pi'] is not None:\n",
    "                    print(f\"  Predictive information: {pi_result['mean_pi']:.3f} (target: >0.5)\")\n",
    "                    print(f\"  Mean potential: {pi_result['mean_potential']:.3f}\")\n",
    "                    print(f\"  PI Quality: [{pi_result['status'].upper()}]\")\n",
    "                \n",
    "                # Fan-out\n",
    "                fanout_result = compute_fanout_quality_score(snapshot, node_name)\n",
    "                if fanout_result['mean_fanout'] is not None:\n",
    "                    print(f\"  Mean fan-out: {fanout_result['mean_fanout']:.1f}\")\n",
    "                    print(f\"  Target range: {fanout_result['target_range']}\")\n",
    "                    print(f\"  Fan-out Quality: [{fanout_result['status'].upper()}]\")\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Predictive information\n",
    "        ax = axes[0]\n",
    "        pi_vals = []\n",
    "        valid_nodes_pi = []\n",
    "        for node_name in node_names:\n",
    "            pi_result = compute_predictive_information_quality(snapshot, node_name)\n",
    "            if pi_result['mean_pi'] is not None:\n",
    "                pi_vals.append(pi_result['mean_pi'])\n",
    "                valid_nodes_pi.append(node_name)\n",
    "        \n",
    "        if pi_vals:\n",
    "            ax.bar(range(len(valid_nodes_pi)), pi_vals, color='skyblue', edgecolor='black')\n",
    "            ax.axhline(0.5, color='green', linestyle='--', alpha=0.7, label='Target (>0.5)')\n",
    "            ax.set_xticks(range(len(valid_nodes_pi)))\n",
    "            ax.set_xticklabels(valid_nodes_pi)\n",
    "            ax.set_ylabel('Mean Predictive Information')\n",
    "            ax.set_title('Predictive Information by Level')\n",
    "            ax.set_ylim(0, 1.0)\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Fan-out\n",
    "        ax = axes[1]\n",
    "        fanout_vals = []\n",
    "        valid_nodes_fo = []\n",
    "        for node_name in node_names:\n",
    "            fo_result = compute_fanout_quality_score(snapshot, node_name)\n",
    "            if fo_result['mean_fanout'] is not None:\n",
    "                fanout_vals.append(fo_result['mean_fanout'])\n",
    "                valid_nodes_fo.append(node_name)\n",
    "        \n",
    "        if fanout_vals:\n",
    "            colors = ['green' if 50 < v < 150 else 'yellow' for v in fanout_vals]\n",
    "            ax.bar(range(len(valid_nodes_fo)), fanout_vals, color=colors, edgecolor='black')\n",
    "            ax.set_xticks(range(len(valid_nodes_fo)))\n",
    "            ax.set_xticklabels(valid_nodes_fo)\n",
    "            ax.set_ylabel('Mean Fan-out')\n",
    "            ax.set_title('Prediction Fan-out by Level')\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No prediction samples found in snapshot.\")\n",
    "        print(\"   Snapshot was captured without prediction sampling.\")\n",
    "        print(\"   Re-run training.ipynb Cell 24 with capture_prediction_samples=True\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hierarchical Generation Readiness (HGR) Score\n",
    "\n",
    "**Overall quality score (0-100)** that predicts text generation quality.\n",
    "\n",
    "**Categories**:\n",
    "1. Zipf Quality (25%)\n",
    "2. Composition Quality (25%)\n",
    "3. Prediction Quality (25%)\n",
    "4. Hierarchical Consistency (20%)\n",
    "5. Diversity (5%)\n",
    "\n",
    "**Interpretation**:\n",
    "- **90-100**: Excellent - ready for generation\n",
    "- **70-89**: Good - minor improvements possible\n",
    "- **50-69**: Warning - review specific categories\n",
    "- **30-49**: Poor - significant issues\n",
    "- **0-29**: Critical - not ready for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshot:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HIERARCHICAL GENERATION READINESS (HGR) SCORE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    hgr_result = compute_generation_readiness_score(snapshot)\n",
    "    \n",
    "    # Overall score\n",
    "    print(f\"ðŸŽ¯ OVERALL HGR SCORE: {hgr_result['overall_score']:.1f}/100\")\n",
    "    print(f\"   Health Status: {hgr_result['health_status']}\\n\")\n",
    "    \n",
    "    # Category breakdown\n",
    "    print(\"ðŸ“Š CATEGORY SCORES:\")\n",
    "    for category, score in hgr_result['category_scores'].items():\n",
    "        score_pct = score * 100\n",
    "        status_icon = 'ðŸŸ¢' if score > 0.8 else 'ðŸŸ¡' if score > 0.6 else 'ðŸŸ ' if score > 0.4 else 'ðŸ”´'\n",
    "        print(f\"  {status_icon} {category.replace('_', ' ').title()}: {score_pct:.1f}/100\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(hgr_result['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    # Visualize category scores\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Radar chart\n",
    "    ax = axes[0]\n",
    "    categories = list(hgr_result['category_scores'].keys())\n",
    "    values = [hgr_result['category_scores'][c] * 100 for c in categories]\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    values += values[:1]  # Complete the circle\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    ax = plt.subplot(121, projection='polar')\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, color='blue')\n",
    "    ax.fill(angles, values, alpha=0.25, color='blue')\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels([c.replace('_', '\\n').title() for c in categories], size=8)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_title('HGR Category Breakdown', pad=20)\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Bar chart\n",
    "    ax = axes[1]\n",
    "    colors = ['green' if v > 80 else 'yellow' if v > 60 else 'orange' if v > 40 else 'red'\n",
    "              for v in values[:-1]]\n",
    "    ax.barh(range(len(categories)), values[:-1], color=colors, edgecolor='black')\n",
    "    ax.set_yticks(range(len(categories)))\n",
    "    ax.set_yticklabels([c.replace('_', ' ').title() for c in categories])\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.axvline(80, color='green', linestyle='--', alpha=0.5, label='Excellent')\n",
    "    ax.axvline(60, color='yellow', linestyle='--', alpha=0.5, label='Good')\n",
    "    ax.set_title('Category Scores')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Run Comparison\n",
    "\n",
    "Compare multiple training runs to identify optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all available snapshots\n",
    "snapshot_dir = Path('./snapshots')\n",
    "snapshot_files = list(snapshot_dir.glob('*_snapshot.json'))\n",
    "\n",
    "print(f\"Found {len(snapshot_files)} snapshot files\\n\")\n",
    "\n",
    "if len(snapshot_files) > 1:\n",
    "    # Load all snapshots\n",
    "    snapshots = []\n",
    "    for snapshot_file in snapshot_files:\n",
    "        try:\n",
    "            s = TrainingRunSnapshot.load(str(snapshot_file))\n",
    "            snapshots.append(s)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Failed to load {snapshot_file.name}: {e}\")\n",
    "    \n",
    "    if len(snapshots) > 1:\n",
    "        # Compare snapshots\n",
    "        comparison = compare_snapshots(snapshots, metric='overall_score')\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"MULTI-RUN COMPARISON\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        print(\"ðŸ“Š RANKINGS BY HGR SCORE:\\n\")\n",
    "        for i, (run_id, score) in enumerate(comparison['rankings'], 1):\n",
    "            medal = 'ðŸ¥‡' if i == 1 else 'ðŸ¥ˆ' if i == 2 else 'ðŸ¥‰' if i == 3 else '  '\n",
    "            print(f\"{medal} {i}. {run_id}: {score:.1f}/100\")\n",
    "        \n",
    "        print(f\"\\nðŸ† Best configuration: {comparison['best_run']}\")\n",
    "        print(f\"   Score: {comparison['rankings'][0][1]:.1f}/100\")\n",
    "        \n",
    "        # Visualize comparison\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        run_ids = [rid for rid, _ in comparison['rankings']]\n",
    "        scores = [score for _, score in comparison['rankings']]\n",
    "        colors = ['green' if s >= 80 else 'yellow' if s >= 60 else 'orange' if s >= 40 else 'red'\n",
    "                  for s in scores]\n",
    "        \n",
    "        ax.barh(range(len(run_ids)), scores, color=colors, edgecolor='black')\n",
    "        ax.set_yticks(range(len(run_ids)))\n",
    "        ax.set_yticklabels([rid[-15:] for rid in run_ids])  # Show last 15 chars\n",
    "        ax.set_xlabel('HGR Score')\n",
    "        ax.set_title('Training Run Comparison (Ranked by HGR Score)')\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.axvline(80, color='green', linestyle='--', alpha=0.5, label='Excellent')\n",
    "        ax.axvline(60, color='yellow', linestyle='--', alpha=0.5, label='Good')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "    else:\n",
    "        print(\"Only one valid snapshot loaded. Need at least 2 for comparison.\")\n",
    "else:\n",
    "    print(\"Only one snapshot found. Run more training sessions for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Metrics Export\n",
    "\n",
    "Export comprehensive metrics table for external analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if snapshot:\n",
    "    # Build comprehensive metrics table\n",
    "    metrics_table = []\n",
    "    \n",
    "    for node_name in node_names:\n",
    "        ns = snapshot.nodes[node_name]\n",
    "        \n",
    "        zipf_result = compute_zipf_quality_score(snapshot, node_name)\n",
    "        comp_result = compute_composition_quality_score(snapshot, node_name)\n",
    "        fanout_result = compute_fanout_quality_score(snapshot, node_name)\n",
    "        pi_result = compute_predictive_information_quality(snapshot, node_name)\n",
    "        \n",
    "        metrics_table.append({\n",
    "            'Node': node_name,\n",
    "            'Patterns': ns.total_patterns,\n",
    "            'Zipf_Alpha': zipf_result['alpha'],\n",
    "            'Zipf_Score': zipf_result['score'],\n",
    "            'Orphan_Rate': comp_result['orphan_rate'],\n",
    "            'Coverage': comp_result['coverage'],\n",
    "            'Comp_Score': comp_result['score'],\n",
    "            'Mean_Fanout': fanout_result.get('mean_fanout'),\n",
    "            'Fanout_Score': fanout_result['score'],\n",
    "            'Mean_PI': pi_result.get('mean_pi'),\n",
    "            'PI_Score': pi_result['score']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(metrics_table)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE METRICS TABLE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Export to CSV\n",
    "    csv_path = f'./metrics/{snapshot.run_id}_metrics.csv'\n",
    "    Path('./metrics').mkdir(exist_ok=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nâœ“ Exported to {csv_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interpretation Guide\n",
    "\n",
    "### Understanding HGR Scores\n",
    "\n",
    "**Overall HGR Score Ranges:**\n",
    "- **90-100 (EXCELLENT)**: System is ready for text generation. All metrics in healthy ranges.\n",
    "- **70-89 (GOOD)**: Minor improvements possible, but generation should work well.\n",
    "- **50-69 (WARNING)**: Review weak categories. Generation may have quality issues.\n",
    "- **30-49 (POOR)**: Significant problems detected. Address before generation.\n",
    "- **0-29 (CRITICAL)**: Not ready for generation. Major configuration issues.\n",
    "\n",
    "### Category Interpretations\n",
    "\n",
    "**1. Zipf Quality (25%):**\n",
    "- Measures if frequency distributions follow expected power-laws\n",
    "- Different targets per level (node0: 1.0-1.5, node1: 0.8-1.2, etc.)\n",
    "- Low score â†’ Adjust chunk_size or increase training data\n",
    "\n",
    "**2. Composition Quality (25%):**\n",
    "- Orphan rate: % patterns not used by higher levels\n",
    "- Coverage: % patterns participating in hierarchy\n",
    "- Low score â†’ Increase training samples or review segmentation\n",
    "\n",
    "**3. Prediction Quality (25%):**\n",
    "- Predictive information: Pattern reliability\n",
    "- Fan-out: Number of predictions (level-specific targets)\n",
    "- Low score â†’ Adjust recall_threshold or improve pattern diversity\n",
    "\n",
    "**4. Hierarchical Consistency (20%):**\n",
    "- Frequency correlation: Parent freq vs sum(child freqs)\n",
    "- Target: >0.7 correlation\n",
    "- Low score â†’ Review hierarchy architecture\n",
    "\n",
    "**5. Diversity (5%):**\n",
    "- Vocabulary richness\n",
    "- Target: 0.3-0.7 (Goldilocks zone)\n",
    "\n",
    "### Common Issues & Solutions\n",
    "\n",
    "**Issue**: Low Zipf quality at node0\n",
    "- **Solution**: Train on more samples (10K â†’ 100K) or adjust chunk_size\n",
    "\n",
    "**Issue**: High orphan rate (>30%)\n",
    "- **Solution**: Increase training data or review document boundaries\n",
    "\n",
    "**Issue**: Low coverage (<50%)\n",
    "- **Solution**: Patterns not being composed effectively; increase chunk_size\n",
    "\n",
    "**Issue**: Poor frequency correlation (<0.5)\n",
    "- **Solution**: Structural issues; review hierarchy design\n",
    "\n",
    "**Issue**: Fan-out too low (<5) or too high (>100)\n",
    "- **Solution**: Adjust recall_threshold in KATO configuration\n",
    "\n",
    "### Next Steps Based on HGR Score\n",
    "\n",
    "**If HGR > 70:**\n",
    "1. System is ready for generation experiments\n",
    "2. Consider implementing text generation module\n",
    "3. Validate HGR predictions with actual generation tests\n",
    "\n",
    "**If 50 < HGR < 70:**\n",
    "1. Review category breakdowns\n",
    "2. Address weak categories with specific solutions\n",
    "3. Re-train and re-analyze\n",
    "4. Target HGR > 70 before generation\n",
    "\n",
    "**If HGR < 50:**\n",
    "1. Fundamental issues detected\n",
    "2. Review all recommendations carefully\n",
    "3. Consider different configuration:\n",
    "   - Try different chunk_sizes\n",
    "   - Increase training data significantly\n",
    "   - Review data quality and segmentation\n",
    "4. Re-train from scratch with new configuration\n",
    "\n",
    "### Validation (Future Work)\n",
    "\n",
    "To validate HGR predictions:\n",
    "1. Implement text generation module\n",
    "2. Generate test outputs from multiple configurations\n",
    "3. Measure actual generation quality (coherence, diversity, faithfulness)\n",
    "4. Correlate HGR scores with actual quality\n",
    "5. Refine metric weights if needed\n",
    "\n",
    "**Success criterion**: Pearson correlation > 0.7 between HGR and actual quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
