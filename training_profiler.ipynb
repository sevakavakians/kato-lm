{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Hierarchical Training - Real Data Workflow\n",
    "\n",
    "**Purpose**: Train hierarchical concept learner on real datasets with performance profiling.\n",
    "\n",
    "**This notebook**:\n",
    "- ‚úÖ Trains on **real data** from HuggingFace datasets (WikiText, C4, RefinedWeb, etc.)\n",
    "- ‚úÖ Uses **parallel workers** for optimal speed (2-3x faster)\n",
    "- ‚úÖ Profiles **hardware resources** (CPU, RAM, disk I/O) during training\n",
    "- ‚úÖ Tracks **training history** in SQLite for later analysis\n",
    "- ‚úÖ Tests different **chunk_size and layer configurations** to find optimal settings\n",
    "\n",
    "**Key Concepts**:\n",
    "- node0 learns token chunks (e.g., 8 tokens ‚Üí phrase patterns)\n",
    "- node1 learns sequences of node0 patterns (e.g., 64 tokens ‚Üí sentence patterns)\n",
    "- node2 learns sequences of node1 patterns (e.g., 512 tokens ‚Üí paragraph patterns)\n",
    "- node3 learns sequences of node2 patterns (e.g., 4,096 tokens ‚Üí chapter patterns)\n",
    "\n",
    "**After training**: Use `analysis.ipynb` to analyze learned patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets transformers requests numpy matplotlib tqdm pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All modules imported successfully\n",
      "‚úì Ready for hierarchical training with real data\n"
     ]
    }
   ],
   "source": [
    "# Import hierarchical training modules\n",
    "from tools import (\n",
    "    # Core training\n",
    "    HierarchicalConceptLearner,\n",
    "    HierarchicalNode,\n",
    "    train_from_streaming_dataset_parallel,\n",
    "    \n",
    "    # Profiling and analysis\n",
    "    ProfilingEngine,\n",
    "    HardwareAnalyzerV2,\n",
    "    StorageEstimator,\n",
    "    TrainingHistory,\n",
    "    TrainingEstimator,  # NEW: Data-driven training time estimator\n",
    "    \n",
    "    # Dataset loading\n",
    "    StreamingDatasetLoader,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì All modules imported successfully\")\n",
    "print(\"‚úì Ready for hierarchical training with real data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KATO Node & Training Config\n",
    "\n",
    "**Configuration**:\n",
    "- `dataset_key`: 'wikitext', 'c4', 'refinedweb', 'openwebtext', etc.\n",
    "- `max_samples`: Start small (100) to test, then scale up (10K, 100K, 1M+)\n",
    "- `num_workers`: **3 recommended** (was 6, reduced to prevent deadlocks)\n",
    "  - Rule: workers √ó nodes ‚â§ 30 connections (for stability)\n",
    "  - 3 workers √ó 5 nodes = 15 connections ‚úì SAFE\n",
    "- `checkpoint_interval`: Save progress every N samples (default: 5000)\n",
    "- `resume_from_checkpoint`: Resume from last checkpoint if training was interrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Service URLs configured\n",
      "  KATO:    http://kato:8000\n",
      "  MongoDB: mongodb://kato-mongodb:27017/\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# SERVICE CONFIGURATION (Multi-Machine Support)\n",
    "# ========================================\n",
    "# Configure these URLs for your deployment environment.\n",
    "# Change these if running KATO and MongoDB on separate machines.\n",
    "\n",
    "KATO_URL = 'http://kato:8000'              # KATO server (use IP if DNS unavailable: 'http://192.168.1.100:8000')\n",
    "MONGODB_URI = 'mongodb://kato-mongodb:27017/'  # MongoDB (use IP if DNS unavailable: 'mongodb://192.168.1.101:27017/')\n",
    "\n",
    "# For single-machine setups (everything on localhost):\n",
    "# KATO_URL = 'http://localhost:8000'\n",
    "# MONGODB_URI = 'mongodb://localhost:27017/'\n",
    "\n",
    "# For multi-machine setups with IP addresses:\n",
    "# KATO_URL = 'http://192.168.1.100:8000'\n",
    "# MONGODB_URI = 'mongodb://192.168.1.101:27017/'\n",
    "\n",
    "print(\"‚úì Service URLs configured\")\n",
    "print(f\"  KATO:    {KATO_URL}\")\n",
    "print(f\"  MongoDB: {MONGODB_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuration set\n",
      "  Workers: 3 (3 workers √ó 5 nodes = 15 connections - SAFE)\n",
      "  Checkpoint interval: 5,000 samples\n",
      "  Resume: False\n"
     ]
    }
   ],
   "source": [
    "# Chunk sizes per node.\n",
    "\n",
    "# cs_array = [3, 5, 3, 3, 3]\n",
    "# cs_array = [3, 5, 5, 8, 3]\n",
    "# cs_array = [3, 3, 3, 3, 3]\n",
    "# cs_array = [4, 4, 4, 4, 4]\n",
    "# cs_array = [5, 5, 5, 5, 5]\n",
    "# cs_array = [6, 6, 6, 6, 6]\n",
    "# cs_array = [7, 7, 7, 7, 7]\n",
    "cs_array = [8, 8, 8, 8, 8]\n",
    "# cs_array = [8, 6, 5, 4, 3]\n",
    "# cs_array = [5, 4, 4, 3, 3]\n",
    "## after iterating these above, change node0_batch_size to 100 and redo the above - Doing now\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Configure dataset and training parameters\n",
    "DATASET_KEY = 'wikitext'  # Options: 'c4', 'refinedweb', 'wikitext', 'openwebtext'\n",
    "MAX_SAMPLES = 100000  # Start small to test, then scale up\n",
    "NUM_WORKERS = 3    # REDUCED from 6 ‚Üí safer, prevents deadlocks (recommended: 2-4)\n",
    "\n",
    "# Checkpoint configuration (NEW!)\n",
    "CHECKPOINT_INTERVAL = 5000  # Save checkpoint every 5K samples\n",
    "RESUME_FROM_CHECKPOINT = False  # Set True to resume interrupted training\n",
    "\n",
    "print(\"‚úì Configuration set\")\n",
    "print(f\"  Workers: {NUM_WORKERS} (3 workers √ó 5 nodes = 15 connections - SAFE)\")\n",
    "print(f\"  Checkpoint interval: {CHECKPOINT_INTERVAL:,} samples\")\n",
    "print(f\"  Resume: {RESUME_FROM_CHECKPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Crash Recovery Guide (Kernel Restart / Interruption)\n",
    "\n",
    "**What happens if Jupyter crashes or kernel restarts mid-training?**\n",
    "\n",
    "‚úÖ **Good news**: Learned patterns persist in MongoDB (not lost!)\n",
    "\n",
    "‚úÖ **Checkpoint system**: Progress saved every 5,000 samples\n",
    "\n",
    "**To resume after crash:**\n",
    "\n",
    "1. **Restart kernel** (if needed)\n",
    "\n",
    "2. **Re-run setup cells** with **EXACT SAME configuration**:\n",
    "   - ‚úì Cell 1: Imports\n",
    "   - ‚úì Cell 2: Additional imports\n",
    "   - ‚úì Cell 4: **Configuration** (must match original!)\n",
    "     - Set `RESUME_FROM_CHECKPOINT = True`\n",
    "     - Keep same `cs_array`, `batch_size`, `NUM_WORKERS`\n",
    "   - ‚úì Cell 10: Create learner (**same nodes, chunk_sizes, tokenizer!**)\n",
    "   - ‚úì Cell 14: Create profiler\n",
    "\n",
    "3. **Run training cell** (Cell 16):\n",
    "   - System validates configuration matches checkpoint\n",
    "   - Skips already-processed samples\n",
    "   - Continues from where it left off\n",
    "\n",
    "**‚ö†Ô∏è Configuration Validation**:\n",
    "\n",
    "The system now validates your configuration matches the checkpoint:\n",
    "- ‚úì If match ‚Üí Resume safely\n",
    "- ‚ùå If mismatch ‚Üí Clear error message explaining the problem\n",
    "\n",
    "**Example error if config changed**:\n",
    "```\n",
    "‚ùå CONFIGURATION MISMATCH - Cannot resume training!\n",
    "\n",
    "Mismatches detected:\n",
    "  - num_nodes: checkpoint=5, current=4\n",
    "  - chunk_sizes: checkpoint=[8,8,8,8,8], current=[10,10,10,10]\n",
    "\n",
    "To fix:\n",
    "  1. Recreate learner with EXACT same configuration\n",
    "  2. Or delete checkpoint (./checkpoints/) and start fresh\n",
    "  3. Or use different checkpoint_dir for new configuration\n",
    "```\n",
    "\n",
    "**üí° Pro tip**: Take a screenshot of your configuration before long training runs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hardware Analysis (Optional but Recommended)\n",
    "\n",
    "Analyze your system to:\n",
    "- Understand hardware capabilities\n",
    "- Estimate training time for different dataset sizes\n",
    "- Identify performance bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current hardware\n",
    "hw_analyzer = HardwareAnalyzerV2(verbose=True)\n",
    "\n",
    "# Example training config for accurate throughput estimate\n",
    "# (adjust these to match your planned training configuration)\n",
    "example_config = {\n",
    "    'chunk_sizes': cs_array,\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': 6\n",
    "}\n",
    "\n",
    "hw_report = hw_analyzer.analyze_system(\n",
    "    mongodb_uri=MONGODB_URI,  # Use configured MongoDB URI\n",
    "    kato_url=KATO_URL,        # Use configured KATO URL\n",
    "    training_config=example_config,  # Config-aware estimation\n",
    "    num_samples=10000\n",
    ")\n",
    "\n",
    "hw_report.print_summary()\n",
    "\n",
    "# Save hardware baseline for reference\n",
    "hw_report.export_json('hardware_baseline.json')\n",
    "\n",
    "# Extract key metrics\n",
    "BASELINE_THROUGHPUT = hw_report.estimated_samples_per_sec\n",
    "HARDWARE_TIER = hw_report.tier\n",
    "\n",
    "print(f\"\\nüéØ HARDWARE BASELINE\")\n",
    "print(f\"  Estimated throughput: {BASELINE_THROUGHPUT:.1f} samples/sec\")\n",
    "print(f\"  (for chunk_size={example_config['chunk_sizes'][0]}, batch={example_config['batch_size']}, 10K samples)\")\n",
    "print(f\"  Hardware tier: {HARDWARE_TIER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Storage Estimation (Optional)\n",
    "\n",
    "Estimate MongoDB storage requirements using Zipfian distribution modeling.\n",
    "\n",
    "This helps you plan disk space before training large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create storage estimator with auto-calibration\n",
    "# Auto-calibration uses historical training data to refine Zipfian parameters\n",
    "storage_est = StorageEstimator(verbose=True, auto_calibrate=True)\n",
    "\n",
    "# Example configuration (adjust to match your training config)\n",
    "config = {\n",
    "    'num_levels': 4,\n",
    "    'chunk_sizes': [3,5,8],  # Uniform chunk_size=8\n",
    "    'tokenizer': 'gpt2'\n",
    "}\n",
    "\n",
    "dataset_stats = {\n",
    "    'avg_tokens_per_sample': 500,\n",
    "    'dataset_name': 'wikitext'\n",
    "}\n",
    "\n",
    "# Estimate for your planned training size\n",
    "print(\"\\nüìä STORAGE ESTIMATES\\n\")\n",
    "\n",
    "# for num_samples in [1_000, 10_000, 100_000]:\n",
    "for num_samples in [100_000, 1_000_000]:\n",
    "    estimate = storage_est.estimate_storage(\n",
    "        num_samples=num_samples,\n",
    "        config=config,\n",
    "        dataset_stats=dataset_stats\n",
    "    )\n",
    "    \n",
    "    print(f\"{num_samples:>10,} samples: {estimate.estimated_storage_with_overhead_gb:>8.2f} GB \")\n",
    "    print(f\"             Total patterns: {estimate.total_patterns:,}\")\n",
    "    if storage_est.calibrated_zipf_alpha:\n",
    "        print(f\"             Zipfian Œ±: {storage_est.calibrated_zipf_alpha:.3f} (calibrated)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Hierarchical Learner\n",
    "\n",
    "**KATO Configuration** (Performance Optimizations):\n",
    "- `process_predictions=False`: Disables prediction computation ‚Üí **2-3x faster training**\n",
    "- `max_pattern_length=0`: Manual learning only (we control when to learn)\n",
    "- `stm_mode='CLEAR'`: Clears short-term memory after each learn (fresh context)\n",
    "\n",
    "These settings are configurable in the cell below for transparency and control.\n",
    "\n",
    "**Key Decision**: Choose `chunk_size` based on your dataset.\n",
    "\n",
    "**Recommended configurations**:\n",
    "- **WikiText (500-2K tokens/sample)**: `chunk_size=8` with 4 levels ‚Üí covers 8‚Üí64‚Üí512‚Üí4K tokens\n",
    "- **C4/RefinedWeb (300-3K tokens)**: `chunk_size=6` with 4 levels ‚Üí covers 6‚Üí36‚Üí216‚Üí1.3K tokens\n",
    "- **BookCorpus (50K+ tokens)**: `chunk_size=8` with 5-6 levels for book-length coverage\n",
    "\n",
    "**See PROJECT_OVERVIEW.md Section 7** for detailed hierarchy sizing guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì KATO Configuration:\n",
      "  process_predictions = False (predictions disabled)\n",
      "  max_pattern_length = 0 (manual learning)\n",
      "  stm_mode = CLEAR\n",
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL CONCEPT LEARNER\n",
      "================================================================================\n",
      "Using custom node configurations (4 nodes)\n",
      "\n",
      "‚úì 4 nodes initialized with:\n",
      "  - max_pattern_length = 0 (manual learning)\n",
      "  - stm_mode = CLEAR (STM clears after learn)\n",
      "  - process_predictions = False (predictions disabled)\n",
      "  - tokenizer = gpt2\n",
      "\n",
      "Per-node configuration:\n",
      "  node0: mode=chunking, chunk_size=8\n",
      "  node1: mode=chunking, chunk_size=8\n",
      "  node2: mode=chunking, chunk_size=8\n",
      "  node3: mode=chunking, chunk_size=8\n",
      "================================================================================\n",
      "\n",
      "‚úì Created hierarchical learner with 4 nodes\n",
      "  Chunk size: 8\n",
      "  Node0 batch size: 100 (batching ENABLED)\n",
      "\n",
      "  Semantic coverage:\n",
      "    node0: 8 tokens\n",
      "    node1: 64 tokens\n",
      "    node2: 512 tokens\n",
      "    node3: 4,096 tokens\n",
      "\n",
      "üßπ Clearing all node knowledgebases...\n",
      "  ‚úì node0 cleared\n",
      "  ‚úì node1 cleared\n",
      "  ‚úì node2 cleared\n",
      "  ‚úì node3 cleared\n",
      "‚úì All nodes cleared and ready for fresh training\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# KATO CONFIGURATION (Performance Optimizations)\n",
    "# ========================================\n",
    "# These settings control KATO's internal behavior during training.\n",
    "# Defaults are optimized for training performance.\n",
    "\n",
    "# process_predictions: Disable prediction computation during training\n",
    "#   - False (recommended): 2-3x faster, predictions not needed during training\n",
    "#   - True: Compute predictions (only for interactive exploration/debugging)\n",
    "PROCESS_PREDICTIONS = False\n",
    "\n",
    "# max_pattern_length: Auto-learning behavior\n",
    "#   - 0 (recommended): Manual learning only (we control when to learn)\n",
    "#   - >0: Auto-learn after N observations (not recommended for training)\n",
    "MAX_PATTERN_LENGTH = 0\n",
    "\n",
    "# stm_mode: Short-term memory management\n",
    "#   - 'CLEAR' (recommended): Clear STM after each learn (fresh context)\n",
    "#   - 'ROLLING': Keep rolling window (for sequential prediction tasks)\n",
    "STM_MODE = 'CLEAR'\n",
    "\n",
    "print(\"‚úì KATO Configuration:\")\n",
    "print(f\"  process_predictions = {PROCESS_PREDICTIONS} ({'predictions disabled' if not PROCESS_PREDICTIONS else 'predictions enabled'})\")\n",
    "print(f\"  max_pattern_length = {MAX_PATTERN_LENGTH} ({'manual learning' if MAX_PATTERN_LENGTH == 0 else 'auto-learning'})\")\n",
    "print(f\"  stm_mode = {STM_MODE}\")\n",
    "\n",
    "# ========================================\n",
    "# HIERARCHICAL NODES\n",
    "# ========================================\n",
    "# Configure hierarchical nodes with KATO settings\n",
    "\n",
    "nodes = [\n",
    "    HierarchicalNode('node0', chunk_size=cs_array[0], mode='chunking', base_url=KATO_URL,\n",
    "                     process_predictions=PROCESS_PREDICTIONS, max_pattern_length=MAX_PATTERN_LENGTH, stm_mode=STM_MODE),\n",
    "    HierarchicalNode('node1', chunk_size=cs_array[1], mode='chunking', base_url=KATO_URL,\n",
    "                     process_predictions=PROCESS_PREDICTIONS, max_pattern_length=MAX_PATTERN_LENGTH, stm_mode=STM_MODE),\n",
    "    HierarchicalNode('node2', chunk_size=cs_array[2], mode='chunking', base_url=KATO_URL,\n",
    "                     process_predictions=PROCESS_PREDICTIONS, max_pattern_length=MAX_PATTERN_LENGTH, stm_mode=STM_MODE),\n",
    "    HierarchicalNode('node3', chunk_size=cs_array[3], mode='chunking', base_url=KATO_URL,\n",
    "                     process_predictions=PROCESS_PREDICTIONS, max_pattern_length=MAX_PATTERN_LENGTH, stm_mode=STM_MODE)\n",
    "]\n",
    "\n",
    "learner = HierarchicalConceptLearner(\n",
    "    nodes=nodes,\n",
    "    tokenizer_name='gpt2',\n",
    "    node0_batch_size=batch_size  # Batching for 4-7x speedup\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Created hierarchical learner with {learner.num_nodes} nodes\")\n",
    "print(f\"  Chunk size: {learner.node_configs[0].chunk_size}\")\n",
    "print(f\"  Node0 batch size: {learner.node0_batch_size} (batching ENABLED)\")\n",
    "print(f\"\\n  Semantic coverage:\")\n",
    "coverage = learner.node_configs[0].chunk_size\n",
    "for i in range(learner.num_nodes):\n",
    "    print(f\"    node{i}: {coverage:,} tokens\")\n",
    "    coverage *= learner.node_configs[0].chunk_size\n",
    "\n",
    "# Clear all node knowledgebases ONLY if starting fresh (not resuming)\n",
    "if RESUME_FROM_CHECKPOINT:\n",
    "    print(f\"\\nüìÇ RESUME MODE: Keeping existing MongoDB data\")\n",
    "    print(f\"   ‚úì Patterns from previous training will be preserved\")\n",
    "    print(f\"   ‚úì Training will continue from checkpoint\")\n",
    "else:\n",
    "    print(\"\\nüßπ Clearing all node knowledgebases...\")\n",
    "    for i, node in enumerate(learner.nodes.values()):\n",
    "        node.clear_all_memory()\n",
    "        print(f\"  ‚úì node{i} cleared\")\n",
    "    print(\"‚úì All nodes cleared and ready for fresh training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Training History\n",
    "\n",
    "Training history tracks all runs in SQLite for comparison and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training history database\n",
    "history = TrainingHistory(db_path='./training_history.db', verbose=True)\n",
    "\n",
    "# Show current state\n",
    "history.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Training Time Estimator (NEW!)\n",
    "\n",
    "**Predict training time** before you start, based on 29 historical training runs.\n",
    "\n",
    "The TrainingEstimator uses real data to provide accurate estimates that account for:\n",
    "- **chunk_size** (exponential impact - most important factor)\n",
    "- **batch_size** (linear speedup)\n",
    "- **scale** (logarithmic slowdown at larger datasets)\n",
    "- **workers** (sub-linear scaling)\n",
    "- **hardware tier** (existing multipliers)\n",
    "\n",
    "**Key insight**: Performance is dominated by minimum chunk_size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training time estimator (calibrated from historical runs)\n",
    "time_estimator = TrainingEstimator(verbose=True)\n",
    "\n",
    "# Validate estimator accuracy against historical data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTIMATOR ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "validation_metrics = time_estimator.validate_against_history(verbose=True)\n",
    "print(f\"\\nEstimator is {100 - validation_metrics['mape']:.1f}% accurate on average\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define your planned training configuration\n",
    "planned_config = {\n",
    "    'chunk_sizes': cs_array,  # Adjust to match section 4\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': 6\n",
    "}\n",
    "\n",
    "# Get time estimate\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING TIME PREDICTION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "time_estimate = time_estimator.estimate_training(\n",
    "    config=planned_config,\n",
    "    num_samples=MAX_SAMPLES,\n",
    "    hardware_tier=HARDWARE_TIER if 'HARDWARE_TIER' in dir() else 'medium'\n",
    ")\n",
    "\n",
    "time_estimate.print_summary()\n",
    "\n",
    "# Compare different chunk sizes\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CHUNK SIZE COMPARISON (for 10K samples)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for chunk_size in cs_array:\n",
    "    test_config = {\n",
    "        'chunk_sizes': [chunk_size] * 5,\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': 6\n",
    "    }\n",
    "    est = time_estimator.estimate_training(\n",
    "        config=test_config,\n",
    "        num_samples=MAX_SAMPLES,\n",
    "        hardware_tier=HARDWARE_TIER if 'HARDWARE_TIER' in dir() else 'medium'\n",
    "    )\n",
    "    print(f\"chunk_size={chunk_size}: {est.estimated_time_minutes:.1f} min ({est.estimated_samples_per_sec:.2f} samples/sec)\")\n",
    "\n",
    "print(f\"\\nüí° TIP: Larger chunk sizes train MUCH faster (exponential speedup)\")\n",
    "print(f\"   chunk_size=3 ‚Üí chunk_size=8 gives ~3x speedup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train with Real Data (Parallel + Profiling)\n",
    "\n",
    "**This is the main training step**.\n",
    "\n",
    "**‚ö†Ô∏è Requires**:\n",
    "- KATO server running at localhost:8000\n",
    "- MongoDB running at localhost:27017\n",
    "\n",
    "**üìä Note about Pattern Counts**:\n",
    "- Pattern counts will show as 0 after training (MongoDB connection limit with parallel workers)\n",
    "- **Patterns are successfully stored** via KATO API\n",
    "- Use `analysis.ipynb` after training for accurate pattern counts and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING CONFIGURATION\n",
      "================================================================================\n",
      "Dataset: wikitext\n",
      "Samples: 100,000\n",
      "Workers: 3\n",
      "Connections: 12 (workers √ó nodes)\n",
      "Nodes: 4\n",
      "Chunk size: 8\n",
      "Batch size: 100\n",
      "Checkpoint interval: 5,000 samples\n",
      "Resume from checkpoint: False\n",
      "================================================================================\n",
      "\n",
      "‚úì ProfilingEngine initialized\n",
      "‚úì Profiling started at 18:41:34\n",
      "\n",
      "================================================================================\n",
      "PARALLEL STREAMING HIERARCHICAL TRAINING\n",
      "================================================================================\n",
      "Dataset: WikiText-103 - Wikipedia articles (script-free)\n",
      "Source: Salesforce/wikitext\n",
      "Samples: 100,000 (target)\n",
      "Workers: 3 (parallel processing)\n",
      "Connections: 12 (workers √ó nodes)\n",
      "Segmentation: simple\n",
      "Node0 batch size: 100\n",
      "Checkpoint interval: 5,000 samples\n",
      "Est. Time (sequential): 1.4h\n",
      "Est. Time (parallel): 37.4m\n",
      "Expected speedup: 2.2x\n",
      "================================================================================\n",
      "\n",
      "üì• Streaming dataset in batches of 1000...\n",
      "‚úì Starting parallel training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples:   0%|                                                                                                                                                      | 0/100000 [00:00<?, ?sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì° Streaming: WikiText-103 - Wikipedia articles (script-free)\n",
      "   Dataset: Salesforce/wikitext\n",
      "   Samples: 100,000\n",
      "   Est. Time: 1.4h\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886638ac6630436689211abc16b7da53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples:  29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                             | 29300/100000 [20:36<1:32:24, 12.75sample/s, trained=4.0/s, errors=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 5,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 5,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 5,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 5,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 5,000 samples completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples:  43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                              | 43400/100000 [47:47<2:25:44,  6.47sample/s, trained=3.5/s, errors=45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 10,000 samples completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                 | 54100/100000 [1:10:09<1:38:36,  7.76sample/s, trained=3.6/s, errors=62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 15,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 15,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 15,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 15,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 15,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 15,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 15,000 samples completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                       | 63300/100000 [1:32:01<1:27:17,  7.01sample/s, trained=3.6/s, errors=83]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 20,000 samples completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                             | 72200/100000 [1:56:24<1:23:47,  5.53sample/s, trained=3.6/s, errors=109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 25,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 25,000 samples completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè             | 87300/100000 [2:41:29<42:32,  4.98sample/s, trained=3.6/s, errors=157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 35,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 35,000 samples completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 94300/100000 [3:04:42<19:08,  4.96sample/s, trained=3.6/s, errors=176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 40,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 40,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 40,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 40,000 samples completed\n",
      "\n",
      "üíæ Checkpoint saved: 40,000 samples completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [3:24:10<00:00,  4.82sample/s, trained=3.6/s, errors=194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Waiting for workers to complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [6:50:25<00:00,  4.06sample/s, trained=3.6/s, errors=194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PARALLEL TRAINING COMPLETE\n",
      "================================================================================\n",
      "‚úì Samples processed: 99,546\n",
      "‚ö†Ô∏è  Errors/skipped: 454\n",
      "‚è±Ô∏è  Total time: 6.8h\n",
      "üìä Rate: 4.0 samples/second\n",
      "üöÄ Workers: 3\n",
      "üíæ Checkpoints saved: 23\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE - PATTERN STATISTICS\n",
      "================================================================================\n",
      "Pattern counts not shown during parallel training (MongoDB connection limit).\n",
      "‚úì Patterns were successfully stored via KATO API.\n",
      "\n",
      "üìä For accurate pattern counts and analysis, open: analysis.ipynb\n",
      "================================================================================\n",
      "\n",
      "‚úì Training manifest saved: manifests/training_20251105_013200.json\n",
      "  (Load later with: TrainingManifest.load('manifests/training_20251105_013200.json'))\n",
      "\n",
      "‚úì Profiling stopped after 24625.31s\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Performance Statistics:\n",
      "  Samples processed: 99,546\n",
      "  Samples attempted: 100,000\n",
      "  Total time: 24625.22s\n",
      "  Rate: 4.04 samples/sec\n",
      "  Workers: 3\n",
      "  Checkpoints saved: 23\n",
      "\n",
      "üìä For pattern counts and analysis: Open analysis.ipynb\n"
     ]
    }
   ],
   "source": [
    "## If training stalls, re-run this cell (make sure RESUME_FROM_CHECKPOINT = True in settings of cell 2.)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dataset: {DATASET_KEY}\")\n",
    "print(f\"Samples: {MAX_SAMPLES:,}\")\n",
    "print(f\"Workers: {NUM_WORKERS}\")\n",
    "print(f\"Connections: {NUM_WORKERS * learner.num_nodes} (workers √ó nodes)\")\n",
    "print(f\"Nodes: {learner.num_nodes}\")\n",
    "print(f\"Chunk size: {nodes[0].chunk_size}\")\n",
    "print(f\"Batch size: {learner.node0_batch_size}\")\n",
    "print(f\"Checkpoint interval: {CHECKPOINT_INTERVAL:,} samples\")\n",
    "print(f\"Resume from checkpoint: {RESUME_FROM_CHECKPOINT}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Start profiling\n",
    "profiler = ProfilingEngine(sampling_interval_seconds=1.0, verbose=True)\n",
    "profiler.start()\n",
    "\n",
    "# Train with parallel workers (profiler is REQUIRED for performance analysis)\n",
    "stats = train_from_streaming_dataset_parallel(\n",
    "    dataset_key=DATASET_KEY,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    learner=learner,\n",
    "    profiler=profiler,  # REQUIRED - tracks samples/sec, CPU, memory for analysis.ipynb\n",
    "    num_levels=learner.num_nodes,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    segment_method='simple',\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL,  # NEW: Auto-checkpoint\n",
    "    checkpoint_dir='./checkpoints',           # NEW: Checkpoint directory\n",
    "    resume_from_checkpoint=RESUME_FROM_CHECKPOINT,  # NEW: Resume support\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Stop profiling and generate report\n",
    "profiler.stop()\n",
    "profiling_report = profiler.generate_report()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nPerformance Statistics:\")\n",
    "print(f\"  Samples processed: {stats['samples_processed']:,}\")\n",
    "print(f\"  Samples attempted: {stats.get('samples_attempted', stats['samples_processed']):,}\")\n",
    "print(f\"  Total time: {stats['total_time_seconds']:.2f}s\")\n",
    "print(f\"  Rate: {stats['rate_samples_per_sec']:.2f} samples/sec\")\n",
    "print(f\"  Workers: {stats.get('num_workers', 'N/A')}\")\n",
    "print(f\"  Checkpoints saved: {stats.get('checkpoints_saved', 0)}\")\n",
    "print(f\"\\nüìä For pattern counts and analysis: Open analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Profiling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROFILING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚è±Ô∏è  TIMING\n",
      "  Total duration: 24625.31s\n",
      "  Samples processed: 99,546\n",
      "  Throughput: 4.04 samples/sec\n",
      "\n",
      "üíæ MEMORY\n",
      "  Peak: 1049.1 MB\n",
      "  Average: 1023.6 MB\n",
      "  Per sample: 0.010 MB/sample\n",
      "  Trend: stable\n",
      "\n",
      "üîß CPU\n",
      "  Average utilization: 53.3%\n",
      "  Peak utilization: 150.7%\n",
      "\n",
      "üíø DISK I/O\n",
      "  Total write: 155742.82 MB\n",
      "  Write speed: 6.32 MB/s\n",
      "  ‚ö†Ô∏è  DISK I/O BOTTLENECK DETECTED\n",
      "\n",
      "üåê NETWORK\n",
      "  Sent: 1237.87 MB\n",
      "  Received: 2862.35 MB\n",
      "\n",
      "üîç BOTTLENECK ANALYSIS\n",
      "  Primary bottleneck: MEMORY\n",
      "  Confidence: 100.0%\n",
      "\n",
      "================================================================================\n",
      "‚úì Profiling report exported to profiling_reports/run_20251105_172734.json\n",
      "\n",
      "‚úì Profiling report exported to profiling_reports/run_20251105_172734.json\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive profiling summary\n",
    "profiler.print_summary()\n",
    "\n",
    "# Optional: Export profiling report to JSON for later analysis\n",
    "import time\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "import os\n",
    "os.makedirs('profiling_reports', exist_ok=True)\n",
    "profiling_report_path = f'profiling_reports/run_{timestamp}.json'\n",
    "profiler.export_json(profiling_report_path)\n",
    "print(f\"\\n‚úì Profiling report exported to {profiling_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Record Training Run in History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     estimated_time_sec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     estimated_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m run_id \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mrecord_run(\n\u001b[1;32m     18\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     19\u001b[0m     estimated_time\u001b[38;5;241m=\u001b[39mestimated_time_sec,  \u001b[38;5;66;03m# Use prediction from Section 5a\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     actual_time\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_time_seconds\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     21\u001b[0m     estimated_storage_gb\u001b[38;5;241m=\u001b[39mestimated_storage,\n\u001b[1;32m     22\u001b[0m     actual_storage_gb\u001b[38;5;241m=\u001b[39mprofiling_report\u001b[38;5;241m.\u001b[39mtotal_disk_write_mb \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     23\u001b[0m     samples_processed\u001b[38;5;241m=\u001b[39mMAX_SAMPLES,\n\u001b[1;32m     24\u001b[0m     patterns_learned\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_patterns\u001b[39m\u001b[38;5;124m'\u001b[39m: stats\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_patterns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(learner\u001b[38;5;241m.\u001b[39mnum_nodes)\n\u001b[1;32m     27\u001b[0m     },\n\u001b[1;32m     28\u001b[0m     profiling_report\u001b[38;5;241m=\u001b[39mprofiling_report,\n\u001b[1;32m     29\u001b[0m     dataset_key\u001b[38;5;241m=\u001b[39mDATASET_KEY,\n\u001b[1;32m     30\u001b[0m     hardware_tier\u001b[38;5;241m=\u001b[39mHARDWARE_TIER \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHARDWARE_TIER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     31\u001b[0m     notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParallel training with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_WORKERS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m workers on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_SAMPLES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Show estimation accuracy if we had a prediction\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimated_time_sec:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Record this training run for later comparison\n",
    "config = {\n",
    "    'num_levels': learner.num_nodes,\n",
    "    'chunk_sizes': [n.chunk_size for n in learner.node_configs],\n",
    "    'batch_size': learner.node0_batch_size,\n",
    "    'num_workers': NUM_WORKERS\n",
    "}\n",
    "\n",
    "# Calculate estimated time if estimator was used\n",
    "if 'time_estimate' in dir() and time_estimate is not None:\n",
    "    estimated_time_sec = time_estimate.estimated_time_seconds\n",
    "    estimated_storage = None  # Could add storage estimate here too\n",
    "else:\n",
    "    estimated_time_sec = None\n",
    "    estimated_storage = None\n",
    "\n",
    "run_id = history.record_run(\n",
    "    config=config,\n",
    "    estimated_time=estimated_time_sec,  # Use prediction from Section 5a\n",
    "    actual_time=stats['total_time_seconds'],\n",
    "    estimated_storage_gb=estimated_storage,\n",
    "    actual_storage_gb=profiling_report.total_disk_write_mb / 1024,\n",
    "    samples_processed=MAX_SAMPLES,\n",
    "    patterns_learned={\n",
    "        f'node{i}_patterns': stats.get(f'node{i}_patterns', 0)\n",
    "        for i in range(learner.num_nodes)\n",
    "    },\n",
    "    profiling_report=profiling_report,\n",
    "    dataset_key=DATASET_KEY,\n",
    "    hardware_tier=HARDWARE_TIER if 'HARDWARE_TIER' in dir() else 'unknown',\n",
    "    notes=f'Parallel training with {NUM_WORKERS} workers on {MAX_SAMPLES} samples'\n",
    ")\n",
    "\n",
    "# Show estimation accuracy if we had a prediction\n",
    "if estimated_time_sec:\n",
    "    actual_time_min = stats['total_time_seconds'] / 60\n",
    "    estimated_time_min = estimated_time_sec / 60\n",
    "    error_pct = abs(estimated_time_sec - stats['total_time_seconds']) / stats['total_time_seconds'] * 100\n",
    "    \n",
    "    print(f\"\\nüìä ESTIMATION ACCURACY\")\n",
    "    print(f\"  Estimated: {estimated_time_min:.1f} minutes\")\n",
    "    print(f\"  Actual: {actual_time_min:.1f} minutes\")\n",
    "    print(f\"  Error: {error_pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚úì Training run recorded in history: {run_id}\")\n",
    "print(f\"\\nüéâ TRAINING SESSION COMPLETE!\")\n",
    "print(f\"\\nüìä Next step: Open analysis.ipynb to analyze learned patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Capture Enhanced Training Snapshot\n",
    "\n",
    "**Purpose:** Save comprehensive statistics BEFORE MongoDB is wiped for next training run.\n",
    "\n",
    "### What's Captured (Enhanced Metrics)\n",
    "\n",
    "**Basic Statistics:**\n",
    "- ‚úÖ Pattern counts and frequency distributions\n",
    "- ‚úÖ Zipfian power-law fits (Œ±, R¬≤)\n",
    "- ‚úÖ Top patterns by frequency\n",
    "- ‚úÖ Storage metrics\n",
    "\n",
    "**NEW: Graph Topology** (for composition analysis):\n",
    "- ‚úÖ Parent-child relationships (which patterns compose into which)\n",
    "- ‚úÖ Orphan rates (% patterns with no parents)\n",
    "- ‚úÖ Coverage metrics (% patterns used by parent level)\n",
    "- ‚úÖ Reusability statistics (patterns referenced by multiple parents)\n",
    "\n",
    "**NEW: Prediction Quality Samples** (for generation readiness):\n",
    "- ‚úÖ Predictive_information scores (prediction reliability)\n",
    "- ‚úÖ Potential scores (similarity √ó predictive_information)\n",
    "- ‚úÖ Fan-out statistics (number of predictions per query)\n",
    "- ‚úÖ Confidence distributions\n",
    "\n",
    "**NEW: Hierarchical Validation** (for integrity checking):\n",
    "- ‚úÖ Frequency correlation (parent freq vs sum of child freqs)\n",
    "- ‚úÖ Frequency compression ratios\n",
    "- ‚úÖ Hierarchical consistency metrics\n",
    "\n",
    "### Why These Metrics Matter\n",
    "\n",
    "**Graph topology becomes critical for:**\n",
    "- **Pruning operations:** Must track orphans when deleting low-frequency patterns\n",
    "- **Post-pruning validation:** Detect dangling references after cleanup\n",
    "- **Composition quality:** Measure how well patterns participate in hierarchy\n",
    "\n",
    "**Prediction quality metrics predict:**\n",
    "- **Text generation quality** WITHOUT running generation tests\n",
    "- **Hierarchical Generation Readiness (HGR)** score (0-100)\n",
    "- **Optimal configuration** for different use cases\n",
    "\n",
    "**Note:** In an unpruned KB, top-down reachability is guaranteed by training design. These metrics become essential for **pruning analysis** and **generation quality prediction**.\n",
    "\n",
    "### Next Steps After Snapshot\n",
    "\n",
    "1. **Analyze generation readiness:** Open `prediction_quality.ipynb`\n",
    "   - Get HGR score (0-100)\n",
    "   - Review category breakdowns\n",
    "   - Get actionable recommendations\n",
    "\n",
    "2. **Compare configurations:** Load multiple snapshots\n",
    "   - Rank by HGR score\n",
    "   - Identify optimal chunk_size\n",
    "   - Track improvements over time\n",
    "\n",
    "3. **Plan pruning (future):** If implementing KB pruning\n",
    "   - Use graph topology for safe cascade cleanup\n",
    "   - Validate no dangling references post-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CAPTURING ENHANCED TRAINING SNAPSHOT\n",
      "================================================================================\n",
      "\n",
      "üì∏ Capturing comprehensive metrics:\n",
      "  ‚úì Basic statistics (patterns, storage, frequencies)\n",
      "  ‚úì Zipfian power-law fits\n",
      "  ‚úì Graph topology (parent-child relationships)\n",
      "  ‚úì Composition quality (orphan rates, coverage)\n",
      "  ‚úì Prediction samples (quality estimation)\n",
      "  ‚úì Hierarchical validation (frequency correlation)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ‚úì Hierarchical validation (frequency correlation)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 15\u001b[0m run_snapshot \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mcapture_snapshot(\n\u001b[1;32m     16\u001b[0m     learner\u001b[38;5;241m=\u001b[39mlearner,\n\u001b[1;32m     17\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[1;32m     18\u001b[0m     mongo_uri\u001b[38;5;241m=\u001b[39mMONGODB_URI,  \u001b[38;5;66;03m# Use configured MongoDB URI\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     snapshots_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./snapshots\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# NEW: Enhanced metrics for prediction quality estimation\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     capture_graph_topology\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,        \u001b[38;5;66;03m# Parent-child relationships, orphan rates\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     capture_prediction_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,    \u001b[38;5;66;03m# Predictive_information, fan-out\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     num_prediction_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,         \u001b[38;5;66;03m# Number of test predictions per node\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     validate_hierarchy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m             \u001b[38;5;66;03m# Frequency correlation validation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müì∏ SNAPSHOT SUMMARY:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Total patterns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_snapshot\u001b[38;5;241m.\u001b[39mtotal_patterns\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Capture training snapshot (BEFORE clearing for next run!)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CAPTURING ENHANCED TRAINING SNAPSHOT\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"üì∏ Capturing comprehensive metrics:\")\n",
    "print(\"  ‚úì Basic statistics (patterns, storage, frequencies)\")\n",
    "print(\"  ‚úì Zipfian power-law fits\")\n",
    "print(\"  ‚úì Graph topology (parent-child relationships)\")\n",
    "print(\"  ‚úì Composition quality (orphan rates, coverage)\")\n",
    "print(\"  ‚úì Prediction samples (quality estimation)\")\n",
    "print(\"  ‚úì Hierarchical validation (frequency correlation)\")\n",
    "print()\n",
    "\n",
    "run_snapshot = history.capture_snapshot(\n",
    "    learner=learner,\n",
    "    run_id=run_id,\n",
    "    mongo_uri=MONGODB_URI,  # Use configured MongoDB URI\n",
    "    snapshots_dir='./snapshots',\n",
    "    verbose=True,\n",
    "    # NEW: Enhanced metrics for prediction quality estimation\n",
    "    capture_graph_topology=True,        # Parent-child relationships, orphan rates\n",
    "    capture_prediction_samples=True,    # Predictive_information, fan-out\n",
    "    num_prediction_samples=100,         # Number of test predictions per node\n",
    "    validate_hierarchy=True             # Frequency correlation validation\n",
    ")\n",
    "\n",
    "print(f\"\\nüì∏ SNAPSHOT SUMMARY:\")\n",
    "print(f\"  Total patterns: {run_snapshot.total_patterns:,}\")\n",
    "print(f\"  Total storage: {run_snapshot.total_storage_mb:.2f} MB\")\n",
    "print(f\"  Total observations: {run_snapshot.total_observations:,}\")\n",
    "\n",
    "print(f\"\\n  Per-node breakdown:\")\n",
    "for node_name in sorted(run_snapshot.nodes.keys()):\n",
    "    ns = run_snapshot.nodes[node_name]\n",
    "    print(f\"    {node_name}: {ns.total_patterns:,} patterns, {ns.db_size_mb:.2f} MB\")\n",
    "    if ns.zipf_alpha:\n",
    "        print(f\"             Zipf Œ±={ns.zipf_alpha:.3f}, mean_freq={ns.mean_frequency:.2f}\")\n",
    "    if ns.orphan_rate is not None:\n",
    "        print(f\"             Orphans: {ns.orphan_rate:.1%}, Coverage: {ns.coverage_to_parent:.1%}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úì Enhanced snapshot captured and saved\")\n",
    "print(f\"  Use prediction_quality.ipynb to estimate generation quality\")\n",
    "print(f\"  Use analysis.ipynb to compare with other runs\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### üìä Hierarchy Metrics (NEW!)\n",
    "Open **`hierarchy_dashboard.ipynb`** for:\n",
    "- **Quick health check** (5-tier scoring system)\n",
    "- At-a-glance hierarchy quality assessment\n",
    "- Actionable recommendations\n",
    "- Immediate issue detection\n",
    "\n",
    "Open **`hierarchy_metrics.ipynb`** for:\n",
    "- **Comprehensive analysis** of all 15 metrics\n",
    "- Graph topology evaluation\n",
    "- Information-theoretic analysis\n",
    "- Training dynamics visualization\n",
    "- Detailed interpretation guide\n",
    "\n",
    "**15 Metrics Across 6 Categories**:\n",
    "1. Compression (ratios, counts, effectiveness)\n",
    "2. Connectivity (reusability, coverage, branching)\n",
    "3. Information Theory (MI, entropy, constraints)\n",
    "4. Prediction (fan-out)\n",
    "5. Context (alignment, diversity)\n",
    "6. Training Dynamics (growth, reusability trends)\n",
    "\n",
    "### üìä Traditional Analysis\n",
    "Open **`analysis.ipynb`** to:\n",
    "- Visualize frequency distributions\n",
    "- Inspect high-frequency patterns\n",
    "- Compare multiple training runs\n",
    "- Clean up low-frequency noise\n",
    "\n",
    "### üî¨ Experimentation\n",
    "To find optimal configurations:\n",
    "1. Try different `chunk_size` values (5, 8, 10, 15, 20)\n",
    "2. Test different number of levels (3, 4, 5, 6)\n",
    "3. Compare training runs using TrainingHistory\n",
    "4. **Use hierarchy metrics** to validate improvements\n",
    "\n",
    "### üìà Scale Up\n",
    "Once you've found good settings (via hierarchy metrics):\n",
    "- Increase `MAX_SAMPLES` (10K ‚Üí 100K ‚Üí 1M+)\n",
    "- Use larger datasets (C4, RefinedWeb)\n",
    "- Monitor hierarchy health over time\n",
    "\n",
    "### üìö Documentation\n",
    "- **hierarchy_metrics/README.md**: Complete metrics guide\n",
    "- **PROJECT_OVERVIEW.md**: Core concepts and philosophy\n",
    "- **TRAINING_RUN_COMPARISON.md**: How to compare experiments\n",
    "- **README.md**: Full feature list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Capture Hierarchy Metrics (Graph-Based Analysis)\n",
    "\n",
    "**IMPORTANT**: This must run BEFORE clearing MongoDB for the next run.\n",
    "\n",
    "Captures comprehensive graph-based metrics including:\n",
    "- Compression ratios and pattern counts\n",
    "- Connectivity (reusability, coverage, branching)\n",
    "- Graph topology and relationships\n",
    "\n",
    "Use `hierarchy_metrics.ipynb` or `hierarchy_dashboard.ipynb` to analyze results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### üìä Analysis\n",
    "Open **`analysis.ipynb`** to:\n",
    "- Visualize frequency distributions\n",
    "- Inspect high-frequency patterns\n",
    "- Compare multiple training runs\n",
    "- Clean up low-frequency noise\n",
    "\n",
    "### üî¨ Experimentation\n",
    "To find optimal configurations:\n",
    "1. Try different `chunk_size` values (5, 8, 10, 15, 20)\n",
    "2. Test different number of levels (3, 4, 5, 6)\n",
    "3. Compare training runs using TrainingHistory\n",
    "4. Analyze which configurations produce best patterns\n",
    "\n",
    "### üìà Scale Up\n",
    "Once you've found good settings:\n",
    "- Increase `MAX_SAMPLES` (10K ‚Üí 100K ‚Üí 1M+)\n",
    "- Use larger datasets (C4, RefinedWeb)\n",
    "- Monitor storage growth with estimates\n",
    "\n",
    "### üìö Documentation\n",
    "- **PROJECT_OVERVIEW.md**: Core concepts and philosophy\n",
    "- **TRAINING_RUN_COMPARISON.md**: How to compare experiments\n",
    "- **README.md**: Full feature list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
