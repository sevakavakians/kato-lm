{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Hierarchical Training - Single-Pass Learning\n",
    "\n",
    "This notebook demonstrates single-pass hierarchical training with KATO nodes.\n",
    "\n",
    "**Key Concepts:**\n",
    "- node0 learns sentences (tokenized)\n",
    "- node1 learns paragraphs (sequences of sentence pattern names)\n",
    "- node2 learns chapters (sequences of paragraph pattern names)\n",
    "- node3 learns books (sequences of chapter pattern names)\n",
    "\n",
    "**Flow:** Text ‚Üí Segment ‚Üí node0 learns ‚Üí pattern_name ‚Üí node1 STM ‚Üí node1 learns ‚Üí pattern_name ‚Üí node2 STM ‚Üí etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets transformers requests numpy matplotlib tqdm pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the hierarchical training module from tools\n",
    "from tools import (\n",
    "    HierarchicalConceptLearner,\n",
    "    HierarchicalNode,  # NEW: Per-node configuration\n",
    "    CorpusSegmenter,\n",
    "    MongoDBAnalyzer,\n",
    "    StandaloneMongoDBAnalyzer,  # NEW: Session-independent analysis\n",
    "    TrainingManifest,  # NEW: Training metadata management\n",
    "    load_latest_manifest,  # NEW: Load saved training manifests\n",
    "    list_all_training_runs,  # NEW: Discover all training runs\n",
    "    create_training_run_nodes,  # NEW: Create unique run IDs\n",
    "    delete_training_run,  # NEW: Cleanup old experiments\n",
    "    HardwareAnalyzer,\n",
    "    StreamingDatasetLoader,\n",
    "    recommend_dataset_configuration,\n",
    "    train_hierarchical_single_pass,\n",
    "    train_from_streaming_dataset,\n",
    "    train_from_streaming_dataset_parallel,  # NEW: Parallel training (Phase 3)\n",
    "    cleanup_all_nodes,\n",
    "    analyze_all_nodes,\n",
    "    transfer_threshold,\n",
    "    transfer_top_n,\n",
    "    transfer_weighted,\n",
    "    transfer_predictions,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Imports complete\")\n",
    "print(\"‚úì Session-independent analysis tools loaded\")\n",
    "print(\"‚úì Training run comparison tools loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parallel Training\n",
    "\n",
    "**NEW! Phase 3 Optimization**: Train with parallel workers for additional 2-3x speedup on top of batching.\n",
    "\n",
    "**Combined Speedup (Phases 1+2+3)**: 15-28x faster than baseline!\n",
    "\n",
    "**How it works**:\n",
    "- Multiple workers process samples concurrently\n",
    "- Each worker gets its own isolated KATO session\n",
    "- No lock contention or race conditions\n",
    "- MongoDB handles concurrent writes safely\n",
    "\n",
    "**Recommended**: Use 4-8 workers depending on your CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines batching + parallel workers for maximum speed\n",
    "\n",
    "# Configure dataset\n",
    "DATASET_KEY = 'wikitext'  # Options: 'c4', 'refinedweb', 'wikitext', 'openwebtext'\n",
    "MAX_SAMPLES = 100  # Start with small number to test\n",
    "\n",
    "# Create learner with optimized configuration (Phase 1 batching enabled)\n",
    "# RECOMMENDED: Use chunk_size=8 for exponential semantic scaling\n",
    "nodes = [\n",
    "    HierarchicalNode('node0', chunk_size=8, mode='chunking', base_url='http://kato:8000'),   # 8 tokens (phrases)\n",
    "    HierarchicalNode('node1', chunk_size=8, mode='chunking', base_url='http://kato:8000'),   # 64 tokens (sentences)\n",
    "    HierarchicalNode('node2', chunk_size=8, mode='chunking', base_url='http://kato:8000'),   # 512 tokens (paragraphs)\n",
    "    HierarchicalNode('node3', chunk_size=8, mode='chunking', base_url='http://kato:8000')    # 4,096 tokens (articles)\n",
    "]\n",
    "\n",
    "learner = HierarchicalConceptLearner(\n",
    "    nodes=nodes,\n",
    "    tokenizer_name='gpt2',\n",
    "    node0_batch_size=50  # Phase 1: Batching for 4-7x speedup\n",
    ")\n",
    "\n",
    "print(f\"‚úì Created hierarchical learner with {learner.num_nodes} nodes\")\n",
    "print(f\"  Chunk size: {learner.node_configs[0].chunk_size} (optimized for WikiText)\")\n",
    "print(f\"  Node0 batch size: {learner.node0_batch_size} (batching ENABLED)\")\n",
    "print(f\"\\n  Semantic coverage:\")\n",
    "coverage = learner.node_configs[0].chunk_size\n",
    "for i in range(learner.num_nodes):\n",
    "    print(f\"    node{i}: {coverage} tokens\")\n",
    "    coverage *= learner.node_configs[0].chunk_size\n",
    "print(f\"  Training with parallel workers...\\n\")\n",
    "\n",
    "# Phase 3: Train with parallel workers for additional 2-3x speedup\n",
    "stats = train_from_streaming_dataset_parallel(\n",
    "    dataset_key=DATASET_KEY,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    learner=learner,\n",
    "    num_levels=4,  # Match number of nodes\n",
    "    num_workers=4,  # Recommended: 4-8 workers (adjust based on CPU cores)\n",
    "    segment_method='simple',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Parallel training complete!\")\n",
    "print(f\"\\nPerformance Statistics:\")\n",
    "print(f\"  Samples processed: {stats['samples_processed']:,}\")\n",
    "print(f\"  Total time: {stats['total_time_seconds']:.2f}s\")\n",
    "print(f\"  Rate: {stats['rate_samples_per_sec']:.2f} samples/sec\")\n",
    "print(f\"  Workers: {stats.get('num_workers', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nPattern Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if 'patterns' in key.lower():\n",
    "        print(f\"  {key}: {value:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Session-Independent Analysis & Training Run Comparison üíæ\n",
    "\n",
    "**NEW! Your training data persists in MongoDB!**\n",
    "\n",
    "After training completes, a **training manifest** is automatically saved to `manifests/` containing metadata about your training run. This enables:\n",
    "\n",
    "### ‚úÖ Session-Independent Analysis\n",
    "Analyze your trained model **without active sessions** - even after kernel restarts!\n",
    "\n",
    "```python\n",
    "from tools import load_latest_manifest\n",
    "\n",
    "# Load the most recent training\n",
    "manifest = load_latest_manifest()\n",
    "analyzers = manifest.get_analyzers(mongo_uri=\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Analyze patterns (no active learner needed!)\n",
    "for node_name, analyzer in analyzers.items():\n",
    "    stats = analyzer.get_stats()\n",
    "    print(f\"{node_name}: {stats['total_patterns']:,} patterns\")\n",
    "```\n",
    "\n",
    "**See [`analysis_only_template.ipynb`](analysis_only_template.ipynb) for complete session-independent analysis workflow!**\n",
    "\n",
    "### üî¨ Comparing Multiple Training Runs\n",
    "By default, training with the same node IDs **overwrites** previous data. To preserve and compare multiple experiments:\n",
    "\n",
    "```python\n",
    "from tools import create_training_run_nodes, HierarchicalConceptLearner\n",
    "\n",
    "# Experiment 1: Small dataset\n",
    "nodes_100 = create_training_run_nodes(run_id='wikitext_100samples')\n",
    "learner_100 = HierarchicalConceptLearner(nodes=nodes_100, tokenizer_name='gpt2')\n",
    "# Train... creates: node0_wikitext_100samples_kato, node1_wikitext_100samples_kato, etc.\n",
    "\n",
    "# Experiment 2: Larger dataset (separate databases!)\n",
    "nodes_500 = create_training_run_nodes(run_id='wikitext_500samples')\n",
    "learner_500 = HierarchicalConceptLearner(nodes=nodes_500, tokenizer_name='gpt2')\n",
    "# Train... creates: node0_wikitext_500samples_kato, node1_wikitext_500samples_kato, etc.\n",
    "\n",
    "# Compare both runs later\n",
    "from tools import list_all_training_runs\n",
    "runs = list_all_training_runs(mongo_uri=\"mongodb://localhost:27017/\")\n",
    "# Returns: {'wikitext_100samples': [...], 'wikitext_500samples': [...]}\n",
    "```\n",
    "\n",
    "**See [`TRAINING_RUN_COMPARISON.md`](TRAINING_RUN_COMPARISON.md) for complete guide!**\n",
    "\n",
    "### üìä Benefits\n",
    "- ‚úÖ Analyze after kernel restarts\n",
    "- ‚úÖ Debug analysis code without retraining\n",
    "- ‚úÖ Compare multiple experiments side-by-side\n",
    "- ‚úÖ Work with historical training data\n",
    "- ‚úÖ No need to keep training sessions active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Frequency Distribution\n",
    "\n",
    "Create histograms to visualize pattern frequency distributions of the learned patterns in each node's knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize node0 (sentence patterns)\n",
    "print(\"Frequency distribution for node0 (sentence patterns):\")\n",
    "analyzer0 = MongoDBAnalyzer(learner.nodes['node0'])\n",
    "analyzer0.visualize_frequency_distribution(max_freq=10)\n",
    "analyzer0.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize node1 (paragraph patterns)\n",
    "print(\"Frequency distribution for node1 (paragraph patterns):\")\n",
    "analyzer1 = MongoDBAnalyzer(learner.nodes['node1'])\n",
    "analyzer1.visualize_frequency_distribution(max_freq=10)\n",
    "analyzer1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Detailed Frequency Histograms\n",
    "\n",
    "View exact frequency counts for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get histograms for all nodes\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FREQUENCY HISTOGRAMS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for node_name in ['node0', 'node1', 'node2', 'node3']:\n",
    "    analyzer = MongoDBAnalyzer(learner.nodes[node_name])\n",
    "    histogram = analyzer.get_frequency_histogram()\n",
    "    analyzer.close()\n",
    "    \n",
    "    if histogram:\n",
    "        print(f\"{node_name}:\")\n",
    "        for freq in sorted(histogram.keys())[:10]:  # Show first 10 frequencies\n",
    "            print(f\"  Frequency {freq}: {histogram[freq]} patterns\")\n",
    "        if len(histogram) > 10:\n",
    "            print(f\"  ... ({len(histogram) - 10} more frequency levels)\")\n",
    "    else:\n",
    "        print(f\"{node_name}: No patterns\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (OPTIONAL) Cleanup Low-Frequency Patterns\n",
    "\n",
    "Remove patterns that appear less than a threshold (e.g., frequency < 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cleanup patterns with frequency < 2\n",
    "# print(\"Cleaning up patterns with frequency < 2...\\n\")\n",
    "\n",
    "# deleted = cleanup_all_nodes(learner, threshold=2, verbose=True)\n",
    "\n",
    "# print(f\"\\nTotal patterns deleted across all nodes: {sum(deleted.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hierarchical text generation \n",
    "\n",
    "Unravel from node1 (paragraph) ‚Üí node0 (sentences) ‚Üí text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical text generation: Unravel from node1 (paragraph) ‚Üí node0 (sentences) ‚Üí text\n",
    "print(f\"{'='*60}\")\n",
    "print(\"HIERARCHICAL GENERATION FROM NODE1 (PARAGRAPH LEVEL)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Sample a paragraph pattern from node1\n",
    "paragraph_patterns = sample_pattern_by_frequency(learner.nodes['node1'], num_samples=1)\n",
    "\n",
    "if paragraph_patterns:\n",
    "    para_pattern = paragraph_patterns[0]\n",
    "    print(f\"Sampled paragraph pattern: {para_pattern['name'][:50]}...\")\n",
    "    print(f\"Contains {len(para_pattern['pattern_data'])} events (sentence pattern names)\\n\")\n",
    "    \n",
    "    # Extract sentence pattern names from the paragraph\n",
    "    sentence_pattern_names = []\n",
    "    for event in para_pattern['pattern_data']:\n",
    "        sentence_pattern_names.extend(event)\n",
    "    \n",
    "    print(f\"Unraveling {len(sentence_pattern_names)} sentence patterns:\\n\")\n",
    "    \n",
    "    # For each sentence pattern name, retrieve it from node0 and decode\n",
    "    analyzer0 = MongoDBAnalyzer(learner.nodes['node0'])\n",
    "    \n",
    "    generated_paragraph = []\n",
    "    for sent_pattern_name in sentence_pattern_names:\n",
    "        # Retrieve the sentence pattern from node0's knowledge base\n",
    "        sent_pattern = analyzer0.patterns_collection.find_one(\n",
    "            {'name': sent_pattern_name},\n",
    "            {'pattern_data': 1, '_id': 0}\n",
    "        )\n",
    "        \n",
    "        if sent_pattern:\n",
    "            # Extract tokens\n",
    "            tokens = []\n",
    "            for event in sent_pattern['pattern_data']:\n",
    "                tokens.extend(event)\n",
    "            tokens = [t for t in tokens if t != '<EOS>']\n",
    "            \n",
    "            # Decode tokens to text\n",
    "            try:\n",
    "                token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                generated_paragraph.append(text)\n",
    "                print(f\"  ‚Ä¢ {text}\")\n",
    "            except:\n",
    "                generated_paragraph.append(' '.join(tokens))\n",
    "                print(f\"  ‚Ä¢ {' '.join(tokens)}\")\n",
    "    \n",
    "    analyzer0.close()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATED PARAGRAPH (combined):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(' '.join(generated_paragraph))\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No paragraph patterns available. Train with more data to enable hierarchical generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get predictions from node0 (requires predictions to be enabled)\n",
    "# Uncomment to test if predictions are enabled:\n",
    "\n",
    "# # Observe some tokens to populate STM\n",
    "# test_tokens = learner.token_processor.tokenize_segment(\"Machine learning is powerful\")\n",
    "# for token in test_tokens:\n",
    "#     learner.nodes['node0'].observe(strings=[token])\n",
    "# \n",
    "# # Get predictions\n",
    "# predictions = learner.nodes['node0'].get_predictions()\n",
    "# \n",
    "# if predictions:\n",
    "#     print(f\"‚úì Retrieved {len(predictions)} predictions\")\n",
    "#     print(\"\\nTop prediction:\")\n",
    "#     print(f\"  Pattern: {predictions[0]['name'][:40]}...\")\n",
    "#     print(f\"  Potential: {predictions[0].get('potential', 0):.3f}\")\n",
    "#     print(f\"  Confidence: {predictions[0].get('confidence', 0):.3f}\")\n",
    "# else:\n",
    "#     print(\"No predictions available. Predictions may not be enabled on this node.\")\n",
    "\n",
    "print(\"Prediction testing code ready (commented out by default)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example custom modeling function\n",
    "def my_custom_filter(predictions, field='name'):\n",
    "    \"\"\"\n",
    "    Custom modeling function: Filter predictions by potential and return top 3.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction dicts\n",
    "        field: Field to extract (default: 'name')\n",
    "    \n",
    "    Returns:\n",
    "        List of filtered pattern names\n",
    "    \"\"\"\n",
    "    # Filter by minimum potential threshold\n",
    "    filtered = [p for p in predictions if p.get('potential', 0) >= 0.3]\n",
    "    \n",
    "    # Sort by confidence\n",
    "    filtered.sort(key=lambda p: p.get('confidence', 0), reverse=True)\n",
    "    \n",
    "    # Return top 3\n",
    "    return [p[field] for p in filtered[:3]]\n",
    "\n",
    "print(\"‚úì Custom modeling function defined\")\n",
    "print(\"\\nExample usage with transfer_predictions:\")\n",
    "print(\"transfer_predictions(\")\n",
    "print(\"    node_source=learner.nodes['node0'],\")\n",
    "print(\"    node_target=learner.nodes['node1'],\")\n",
    "print(\"    field='name',\")\n",
    "print(\"    modeling_function=my_custom_filter\")\n",
    "print(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate built-in modeling functions\n",
    "print(\"Built-in Modeling Functions:\\n\")\n",
    "\n",
    "print(\"1. transfer_threshold:\")\n",
    "print(\"   - Filter predictions by metric threshold\")\n",
    "print(\"   - Example: transfer_threshold(predictions, metric='potential', threshold=0.4)\")\n",
    "\n",
    "print(\"\\n2. transfer_top_n:\")\n",
    "print(\"   - Return top N predictions sorted by metric\")\n",
    "print(\"   - Example: transfer_top_n(predictions, n=5, sort_by='confidence')\")\n",
    "\n",
    "print(\"\\n3. transfer_weighted:\")\n",
    "print(\"   - Weight patterns by metric with repetition\")\n",
    "print(\"   - Example: transfer_weighted(predictions, weight_by='confidence', max_repeats=5)\")\n",
    "\n",
    "print(\"\\n4. transfer_all_names:\")\n",
    "print(\"   - Pass through all predictions without filtering\")\n",
    "print(\"   - Example: transfer_all_names(predictions)\")\n",
    "\n",
    "print(\"\\n‚úì All functions are available in the imported tools module\")\n",
    "print(\"\\nTo use with transfer_predictions:\")\n",
    "print(\"result = transfer_predictions(\")\n",
    "print(\"    node_source=learner.nodes['node0'],\")\n",
    "print(\"    node_target=learner.nodes['node1'],\")\n",
    "print(\"    field='name',\")\n",
    "print(\"    modeling_function=transfer_top_n,  # or any other function\")\n",
    "print(\"    num_predictions=10\")\n",
    "print(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we accomplished\n",
    "print(f\"{'='*80}\")\n",
    "print(\"HIERARCHICAL TRAINING SESSION SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"‚úì What We Did:\")\n",
    "print(\"  1. Created 4-level hierarchical learner (node0 ‚Üí node1 ‚Üí node2 ‚Üí node3)\")\n",
    "print(\"  2. Segmented text into hierarchical structure (books ‚Üí chapters ‚Üí paragraphs ‚Üí sentences)\")\n",
    "print(\"  3. Trained single-pass with pattern names flowing up the hierarchy\")\n",
    "print(\"  4. Analyzed frequency distributions at each level\")\n",
    "print(\"  5. Cleaned up low-frequency patterns (noise removal)\")\n",
    "print(\"  6. Generated new text by sampling and unraveling learned patterns ‚≠ê\")\n",
    "\n",
    "print(\"\\n‚úì Key Insights:\")\n",
    "print(\"  ‚Ä¢ Pattern frequencies follow Zipfian distribution (natural language)\")\n",
    "print(\"  ‚Ä¢ Higher levels have fewer but more abstract patterns\")\n",
    "print(\"  ‚Ä¢ Cleanup improves pattern quality by removing rare/noisy patterns\")\n",
    "print(\"  ‚Ä¢ Text generation works by sampling + unraveling + decoding\")\n",
    "\n",
    "print(\"\\n‚úì Pattern Statistics:\")\n",
    "all_stats_final = analyze_all_nodes(learner)\n",
    "for node_name in ['node0', 'node1', 'node2', 'node3']:\n",
    "    stats = all_stats_final[node_name]\n",
    "    print(f\"  {node_name}: {stats['total_patterns']:,} patterns \"\n",
    "          f\"(avg freq: {stats['average_frequency']:.2f})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Session Complete! You now have a trained hierarchical learning system.\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Prediction testing requires enabling predictions on nodes. This is optional and primarily for advanced research use cases involving prediction-based transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text by sampling from node0 (sentence patterns)\n",
    "print(f\"{'='*60}\")\n",
    "print(\"GENERATING SENTENCES FROM NODE0\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Sample 5 sentence patterns from node0\n",
    "sampled_sentences = sample_pattern_by_frequency(learner.nodes['node0'], num_samples=5)\n",
    "\n",
    "# Decode each pattern back to text\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(learner.token_processor.tokenizer_name)\n",
    "\n",
    "print(\"Generated Sentences:\\n\")\n",
    "for i, pattern in enumerate(sampled_sentences, 1):\n",
    "    # Extract tokens from pattern_data\n",
    "    # Pattern data is a list of events, each event is a list of symbols\n",
    "    tokens = []\n",
    "    for event in pattern['pattern_data']:\n",
    "        tokens.extend(event)\n",
    "    \n",
    "    # Remove special tokens like <EOS>\n",
    "    tokens = [t for t in tokens if t != '<EOS>']\n",
    "    \n",
    "    # Convert token strings to IDs (if needed) and decode\n",
    "    try:\n",
    "        # Try to decode directly if tokens are proper tokenizer tokens\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "        print(f\"{i}. {text}\")\n",
    "    except:\n",
    "        # Fallback: just print the tokens\n",
    "        print(f\"{i}. {' '.join(tokens)}\")\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(sampled_sentences)} sentences from learned patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample patterns from a node based on frequency\n",
    "def sample_pattern_by_frequency(node, num_samples=1):\n",
    "    \"\"\"\n",
    "    Sample pattern(s) from a node's knowledge base using frequency as probability.\n",
    "    Higher frequency patterns are more likely to be sampled.\n",
    "    \n",
    "    Args:\n",
    "        node: KATO node to sample from\n",
    "        num_samples: Number of patterns to sample\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled pattern dictionaries with 'name' and 'pattern_data'\n",
    "    \"\"\"\n",
    "    analyzer = MongoDBAnalyzer(node)\n",
    "    \n",
    "    # Get all patterns with their frequencies\n",
    "    patterns = list(analyzer.patterns_collection.find(\n",
    "        {},\n",
    "        {'name': 1, 'pattern_data': 1, 'frequency': 1, '_id': 0}\n",
    "    ))\n",
    "    analyzer.close()\n",
    "    \n",
    "    if not patterns:\n",
    "        print(f\"No patterns found in {node.node_id}\")\n",
    "        return []\n",
    "    \n",
    "    # Use frequency as sampling weight\n",
    "    import numpy as np\n",
    "    frequencies = np.array([p['frequency'] for p in patterns])\n",
    "    probabilities = frequencies / frequencies.sum()\n",
    "    \n",
    "    # Sample indices\n",
    "    sampled_indices = np.random.choice(len(patterns), size=num_samples, p=probabilities, replace=False)\n",
    "    \n",
    "    return [patterns[i] for i in sampled_indices]\n",
    "\n",
    "print(\"‚úì Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Text Generation from Learned Patterns ‚≠ê\n",
    "\n",
    "**This is the PRIMARY use case of hierarchical learning!**\n",
    "\n",
    "Now that we've learned patterns at multiple levels, we can generate new text by:\n",
    "1. **Sampling** from learned patterns at any level using frequency statistics\n",
    "2. **Unraveling** patterns hierarchically (top-down: book ‚Üí chapter ‚Üí paragraph ‚Üí sentence)\n",
    "3. **Decoding** tokens back to human-readable text\n",
    "\n",
    "This section demonstrates:\n",
    "- Sampling from node0 (sentence level)\n",
    "- Unraveling patterns from higher nodes\n",
    "- Generating coherent text at multiple scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "### üÜï NEW! Session-Independent Analysis & Comparison\n",
    "**Training data persists in MongoDB!** You can now:\n",
    "\n",
    "1. **Analyze after kernel restarts** - See [`analysis_only_template.ipynb`](analysis_only_template.ipynb)\n",
    "2. **Compare multiple training runs** - See [`TRAINING_RUN_COMPARISON.md`](TRAINING_RUN_COMPARISON.md)\n",
    "3. **Load saved training manifests**:\n",
    "   ```python\n",
    "   from tools import load_latest_manifest\n",
    "   manifest = load_latest_manifest()\n",
    "   analyzers = manifest.get_analyzers()\n",
    "   ```\n",
    "\n",
    "### Experimentation Ideas:\n",
    "1. **Train on your own text data**\n",
    "2. **Try different tokenizers** (BERT, RoBERTa, T5, LLaMA)\n",
    "3. **Experiment with different cleanup thresholds**\n",
    "4. **Compare experiments** using `create_training_run_nodes(run_id='...')`\n",
    "5. **Use deeper hierarchies** (5, 10+ nodes)\n",
    "6. **Generate longer text sequences** (paragraphs, chapters, books)\n",
    "7. **Test prediction-based transfers** with `transfer_predictions()`\n",
    "8. **Analyze pattern content** using `StandaloneMongoDBAnalyzer`\n",
    "\n",
    "### Large-Scale Training (Memory-Safe):\n",
    "For datasets >10K samples, use the streaming training approach to avoid memory crashes:\n",
    "```python\n",
    "stats = train_from_streaming_dataset(\n",
    "    dataset_key='wikitext',\n",
    "    max_samples=1500000,\n",
    "    learner=learner,\n",
    "    num_levels=4,\n",
    "    checkpoint_interval=10000,\n",
    "    resume_from_checkpoint=False  # Set True to resume after crash\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits of Streaming Training:**\n",
    "- ‚úÖ Constant memory usage (~1-10 MB per sample)\n",
    "- ‚úÖ Automatic checkpointing every N samples\n",
    "- ‚úÖ Resume from checkpoint after interruption\n",
    "- ‚úÖ Can handle unlimited dataset sizes (millions/billions)\n",
    "- ‚úÖ No OOM crashes!\n",
    "- ‚úÖ **Auto-saves training manifests** for session-independent analysis\n",
    "\n",
    "### Training Run Comparison:\n",
    "Create comparable experiments with unique run IDs:\n",
    "```python\n",
    "# Experiment A: Baseline\n",
    "nodes_baseline = create_training_run_nodes(run_id='baseline_8chunks')\n",
    "# Creates: node0_baseline_8chunks_kato, node1_baseline_8chunks_kato, etc.\n",
    "\n",
    "# Experiment B: Different chunk size\n",
    "nodes_experiment = create_training_run_nodes(run_id='experiment_15chunks', chunk_size=15)\n",
    "# Creates: node0_experiment_15chunks_kato, node1_experiment_15chunks_kato, etc.\n",
    "\n",
    "# Later, compare both runs\n",
    "runs = list_all_training_runs()\n",
    "# Returns: {'baseline_8chunks': [...], 'experiment_15chunks': [...]}\n",
    "```\n",
    "\n",
    "### Training on Different Text Types:\n",
    "- Use `segment_method='simple'` for generic text (default)\n",
    "- Use `segment_method='article'` for article-like text with sections\n",
    "- Use `segment_method='book'` for book-like text with chapters\n",
    "\n",
    "### Supported Tokenizers:\n",
    "- GPT-2: `\"gpt2\"` (default, BPE with ƒ† space markers)\n",
    "- BERT: `\"bert-base-uncased\"`, `\"bert-base-cased\"` (WordPiece with ## continuation)\n",
    "- RoBERTa: `\"roberta-base\"` (byte-level BPE)\n",
    "- T5: `\"t5-small\"`, `\"t5-base\"`, `\"t5-large\"` (SentencePiece)\n",
    "- Others: ALBERT, DistilBERT, XLNet, ELECTRA, DeBERTa, BART, Phi-2, LLaMA-2\n",
    "\n",
    "### Advanced Usage:\n",
    "```python\n",
    "# Use transfer_predictions for prediction-based transfers\n",
    "# (After training and enabling predictions on source node)\n",
    "result = transfer_predictions(\n",
    "    node_source=learner.nodes['node0'],\n",
    "    node_target=learner.nodes['node1'],\n",
    "    field='name',\n",
    "    modeling_function=my_threshold_model,\n",
    "    num_predictions=10\n",
    ")\n",
    "```\n",
    "\n",
    "### Text Generation at Different Scales:\n",
    "- Generate from node0 ‚Üí sentences (token sequences)\n",
    "- Generate from node1 ‚Üí paragraphs (unravel to sentences)\n",
    "- Generate from node2 ‚Üí chapters (unravel to paragraphs ‚Üí sentences)\n",
    "- Generate from node3 ‚Üí books (full hierarchical unraveling)\n",
    "\n",
    "### Granularity Emerges from Hierarchy:\n",
    "- node0: TOKEN-level patterns (from tokenized sentences)\n",
    "- node1: SENTENCE-level patterns (from token pattern sequences)\n",
    "- node2: PARAGRAPH-level patterns (from sentence pattern sequences)\n",
    "- node3: CHAPTER-level patterns (from paragraph pattern sequences)\n",
    "\n",
    "### üìö Related Documentation:\n",
    "- **Session-Independent Analysis**: [`analysis_only_template.ipynb`](analysis_only_template.ipynb)\n",
    "- **Training Run Comparison**: [`TRAINING_RUN_COMPARISON.md`](TRAINING_RUN_COMPARISON.md)\n",
    "- **Project Overview**: [`PROJECT_OVERVIEW.md`](PROJECT_OVERVIEW.md)\n",
    "- **Main README**: [`README.md`](README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "**Experimentation Ideas:**\n",
    "1. Train on your own text data\n",
    "2. Try different tokenizers (BERT, RoBERTa, T5, LLaMA)\n",
    "3. Experiment with different cleanup thresholds\n",
    "4. Create more sophisticated modeling functions\n",
    "5. Use deeper hierarchies (5, 10+ nodes)\n",
    "6. Generate longer text sequences (paragraphs, chapters, books)\n",
    "7. Test prediction-based transfers with `transfer_predictions()`\n",
    "8. Analyze pattern content using `MongoDBAnalyzer.get_patterns_by_frequency()`\n",
    "\n",
    "**Large-Scale Training (NEW! Memory-Safe):**\n",
    "For datasets >10K samples, use the streaming training approach to avoid memory crashes:\n",
    "```python\n",
    "stats = train_from_streaming_dataset(\n",
    "    dataset_key='wikitext',\n",
    "    max_samples=1500000,\n",
    "    learner=learner,\n",
    "    num_levels=4,\n",
    "    checkpoint_interval=10000,\n",
    "    resume_from_checkpoint=False  # Set True to resume after crash\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits of Streaming Training:**\n",
    "- ‚úÖ Constant memory usage (~1-10 MB per sample)\n",
    "- ‚úÖ Automatic checkpointing every N samples\n",
    "- ‚úÖ Resume from checkpoint after interruption\n",
    "- ‚úÖ Can handle unlimited dataset sizes (millions/billions)\n",
    "- ‚úÖ No OOM crashes!\n",
    "\n",
    "**Training on Different Text Types:**\n",
    "- Use `segment_method='simple'` for generic text (default)\n",
    "- Use `segment_method='article'` for article-like text with sections\n",
    "- Use `segment_method='book'` for book-like text with chapters\n",
    "\n",
    "**Supported Tokenizers:**\n",
    "- GPT-2: `\"gpt2\"` (default, BPE with ƒ† space markers)\n",
    "- BERT: `\"bert-base-uncased\"`, `\"bert-base-cased\"` (WordPiece with ## continuation)\n",
    "- RoBERTa: `\"roberta-base\"` (byte-level BPE)\n",
    "- T5: `\"t5-small\"`, `\"t5-base\"`, `\"t5-large\"` (SentencePiece)\n",
    "- Others: ALBERT, DistilBERT, XLNet, ELECTRA, DeBERTa, BART, Phi-2, LLaMA-2\n",
    "\n",
    "**Advanced Usage:**\n",
    "```python\n",
    "# Use transfer_predictions for prediction-based transfers\n",
    "# (After training and enabling predictions on source node)\n",
    "result = transfer_predictions(\n",
    "    node_source=learner.nodes['node0'],\n",
    "    node_target=learner.nodes['node1'],\n",
    "    field='name',\n",
    "    modeling_function=my_threshold_model,\n",
    "    num_predictions=10\n",
    ")\n",
    "```\n",
    "\n",
    "**Text Generation at Different Scales:**\n",
    "- Generate from node0 ‚Üí sentences (token sequences)\n",
    "- Generate from node1 ‚Üí paragraphs (unravel to sentences)\n",
    "- Generate from node2 ‚Üí chapters (unravel to paragraphs ‚Üí sentences)\n",
    "- Generate from node3 ‚Üí books (full hierarchical unraveling)\n",
    "\n",
    "**Granularity Emerges from Hierarchy:**\n",
    "- node0: TOKEN-level patterns (from tokenized sentences)\n",
    "- node1: SENTENCE-level patterns (from token pattern sequences)\n",
    "- node2: PARAGRAPH-level patterns (from sentence pattern sequences)\n",
    "- node3: CHAPTER-level patterns (from paragraph pattern sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Generation from Learned Patterns ‚≠ê\n",
    "\n",
    "**This is the PRIMARY use case of hierarchical learning!**\n",
    "\n",
    "Now that we've learned patterns at multiple levels, we can generate new text by:\n",
    "1. **Sampling** from learned patterns at any level using frequency statistics\n",
    "2. **Unraveling** patterns hierarchically (top-down: book ‚Üí chapter ‚Üí paragraph ‚Üí sentence)\n",
    "3. **Decoding** tokens back to human-readable text\n",
    "\n",
    "This section demonstrates:\n",
    "- Sampling from node0 (sentence level)\n",
    "- Unraveling patterns from higher nodes\n",
    "- Generating coherent text at multiple scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prediction Testing (Optional/Advanced)\n",
    "\n",
    "**Note:** Prediction testing requires enabling predictions on nodes. This is optional and primarily for advanced research use cases involving prediction-based transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Custom Modeling Functions (Optional/Advanced)\n",
    "\n",
    "Create custom functions to model prediction ensembles for transfer between nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "Review what we've accomplished in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "**Experimentation Ideas:**\n",
    "1. Train on your own text data\n",
    "2. Try different tokenizers (BERT, RoBERTa, LLaMA)\n",
    "3. Experiment with different cleanup thresholds\n",
    "4. Create more sophisticated modeling functions\n",
    "5. Use deeper hierarchies (5, 10+ nodes)\n",
    "6. Generate longer text sequences (paragraphs, chapters, books)\n",
    "7. Test prediction-based transfers with `transfer_predictions()`\n",
    "8. Analyze pattern content using `MongoDBAnalyzer.get_patterns_by_frequency()`\n",
    "\n",
    "**Advanced Usage:**\n",
    "```python\n",
    "# Use transfer_predictions for prediction-based transfers\n",
    "# (After training and enabling predictions on source node)\n",
    "result = transfer_predictions(\n",
    "    node_source=learner.nodes['node0'],\n",
    "    node_target=learner.nodes['node1'],\n",
    "    field='name',\n",
    "    modeling_function=my_threshold_model,\n",
    "    num_predictions=10\n",
    ")\n",
    "```\n",
    "\n",
    "**Text Generation at Different Scales:**\n",
    "- Generate from node0 ‚Üí sentences\n",
    "- Generate from node1 ‚Üí paragraphs (unravel to sentences)\n",
    "- Generate from node2 ‚Üí chapters (unravel to paragraphs ‚Üí sentences)\n",
    "- Generate from node3 ‚Üí books (full hierarchical unraveling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
