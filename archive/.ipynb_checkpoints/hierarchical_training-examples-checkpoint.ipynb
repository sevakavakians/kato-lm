{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Hierarchical Training - Single-Pass Learning\n",
    "\n",
    "This notebook demonstrates single-pass hierarchical training with KATO nodes.\n",
    "\n",
    "**Key Concepts:**\n",
    "- node0 learns sentences (tokenized)\n",
    "- node1 learns paragraphs (sequences of sentence pattern names)\n",
    "- node2 learns chapters (sequences of paragraph pattern names)\n",
    "- node3 learns books (sequences of chapter pattern names)\n",
    "\n",
    "**Flow:** Text â†’ Segment â†’ node0 learns â†’ pattern_name â†’ node1 STM â†’ node1 learns â†’ pattern_name â†’ node2 STM â†’ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets transformers requests numpy matplotlib tqdm pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Import the hierarchical training module from tools\n",
    "from tools import (\n",
    "    HierarchicalConceptLearner,\n",
    "    HierarchicalNode,  # NEW: Per-node configuration\n",
    "    CorpusSegmenter,\n",
    "    MongoDBAnalyzer,\n",
    "    HardwareAnalyzer,\n",
    "    StreamingDatasetLoader,\n",
    "    recommend_dataset_configuration,\n",
    "    train_hierarchical_single_pass,\n",
    "    train_from_streaming_dataset,\n",
    "    train_from_streaming_dataset_parallel,  # NEW: Parallel training (Phase 3)\n",
    "    cleanup_all_nodes,\n",
    "    analyze_all_nodes,\n",
    "    transfer_threshold,\n",
    "    transfer_top_n,\n",
    "    transfer_weighted,\n",
    "    transfer_predictions,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hardware Analysis & Dataset Selection\n",
    "\n",
    "Before training, let's analyze your hardware and see which datasets are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hardware capabilities\n",
    "HardwareAnalyzer.display_hardware_info()\n",
    "\n",
    "# Show available datasets\n",
    "print(\"\\n\")\n",
    "StreamingDatasetLoader.list_datasets()\n",
    "\n",
    "# Show time estimates for your hardware\n",
    "StreamingDatasetLoader.show_time_estimates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations based on available time\n",
    "recommend_dataset_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choose Your Training Approach\n",
    "\n",
    "**Option A: Quick Start with Built-in Demo** (recommended for first run)\n",
    "- Uses embedded sample text (3 AI chapters)\n",
    "- Runs in ~1 minute\n",
    "- Great for understanding the workflow\n",
    "\n",
    "**Option B: Large-Scale Dataset Training** \n",
    "- Stream from HuggingFace datasets (C4, RefinedWeb, etc.)\n",
    "- Customize sample size based on available time\n",
    "- Production-ready training\n",
    "\n",
    "Choose one section below to run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Quick Start with Built-in Demo\n",
    "\n",
    "This uses embedded sample text to demonstrate the complete workflow quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Chapter 1: Introduction to Artificial Intelligence\n",
    "\n",
    "Artificial intelligence is transforming the world. Machine learning algorithms \n",
    "can recognize patterns in vast amounts of data. Deep learning networks have \n",
    "achieved remarkable results in image recognition and natural language processing.\n",
    "\n",
    "The field of AI has grown rapidly over the past decade. Researchers continue \n",
    "to push the boundaries of what's possible. New architectures and training \n",
    "methods emerge regularly.\n",
    "\n",
    "Neural networks form the backbone of modern AI systems. These computational \n",
    "models are inspired by biological neurons. They learn through exposure to \n",
    "large datasets and iterative optimization.\n",
    "\n",
    "Chapter 2: Machine Learning Fundamentals\n",
    "\n",
    "Machine learning enables computers to learn from experience. Supervised learning \n",
    "uses labeled data to train predictive models. Classification and regression are \n",
    "two primary types of supervised learning tasks.\n",
    "\n",
    "Unsupervised learning discovers patterns without labeled data. Clustering groups \n",
    "similar data points together. Dimensionality reduction techniques help visualize \n",
    "high-dimensional data.\n",
    "\n",
    "Reinforcement learning trains agents through rewards and penalties. The agent \n",
    "learns optimal behavior by interacting with an environment. This approach has \n",
    "achieved superhuman performance in games like chess and Go.\n",
    "\n",
    "Chapter 3: Applications and Future Directions\n",
    "\n",
    "AI systems are being deployed across many industries. Healthcare applications \n",
    "use machine learning for diagnosis and drug discovery. Financial services employ \n",
    "AI for fraud detection and risk assessment.\n",
    "\n",
    "Autonomous vehicles rely on computer vision and sensor fusion. Natural language \n",
    "processing powers virtual assistants and translation systems. Robotics combines \n",
    "perception, planning, and control.\n",
    "\n",
    "The future of AI holds both promise and challenges. Ethical considerations \n",
    "around bias and fairness require careful attention. Explainability and \n",
    "transparency remain important research goals.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Sample text length: {len(sample_text)} characters\")\n",
    "print(f\"Estimated chapters: {sample_text.count('Chapter')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segmenter with same tokenizer as learner\n",
    "segmenter = CorpusSegmenter(tokenizer_name=\"gpt2\")\n",
    "\n",
    "# Segment the text\n",
    "book = segmenter.segment_book(\n",
    "    sample_text,\n",
    "    book_metadata={'title': 'AI Fundamentals', 'author': 'Demo'}\n",
    ")\n",
    "\n",
    "# Display structure\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CORPUS STRUCTURE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Title: {book['title']}\")\n",
    "print(f\"Author: {book['author']}\")\n",
    "\n",
    "# Wrap in corpus structure for training\n",
    "corpus = {'books': [book]}\n",
    "print(f\"\\nNumber of books: {len(corpus['books'])}\")\n",
    "print(f\"Chapters: {len(book['chapters'])}\")\n",
    "\n",
    "for i, chapter in enumerate(book['chapters'], 1):\n",
    "    num_paragraphs = len(chapter['paragraphs'])\n",
    "    num_sentences = sum(len(p['sentences']) for p in chapter['paragraphs'])\n",
    "    print(f\"  Chapter {i}: {num_paragraphs} paragraphs, {num_sentences} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical learner with 4 levels\n",
    "# node0: chunks, node1: clauses/sentences, node2: paragraphs, node3: articles\n",
    "\n",
    "# OPTION 1: Optimized Uniform Configuration (RECOMMENDED for WikiText)\n",
    "# Uses chunk_size=8 for exponential semantic scaling: 8 â†’ 64 â†’ 512 â†’ 4,096 tokens\n",
    "learner = HierarchicalConceptLearner(\n",
    "    num_nodes=4,\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    segmentation_mode='chunking',  # Fixed-length token chunks\n",
    "    chunk_size=8,                   # OPTIMIZED: provides balanced semantic scaling\n",
    "    node0_batch_size=1,            # Batching disabled (default)\n",
    "    base_url=\"http://kato:8000\"\n",
    ")\n",
    "\n",
    "# OPTION 1b: With batching enabled (4-7x faster for large datasets!)\n",
    "# learner = HierarchicalConceptLearner(\n",
    "#     num_nodes=4,\n",
    "#     tokenizer_name=\"gpt2\",\n",
    "#     segmentation_mode='chunking',\n",
    "#     chunk_size=8,                   # OPTIMIZED for WikiText (500-2000 token articles)\n",
    "#     node0_batch_size=50,           # Batch 50 chunks per API call (RECOMMENDED for speed)\n",
    "#     base_url=\"http://kato:8000\"\n",
    "# )\n",
    "\n",
    "# OPTION 2: Custom Per-Node Configuration (for advanced experimentation)\n",
    "# Note: Use this when you need different chunk sizes per level\n",
    "# nodes = [\n",
    "#     HierarchicalNode('node0', chunk_size=8, mode='chunking'),   # 8 tokens (phrases)\n",
    "#     HierarchicalNode('node1', chunk_size=8, mode='chunking'),   # 64 tokens (sentences)\n",
    "#     HierarchicalNode('node2', chunk_size=8, mode='chunking'),   # 512 tokens (paragraphs)\n",
    "#     HierarchicalNode('node3', chunk_size=8, mode='chunking')    # 4,096 tokens (articles)\n",
    "# ]\n",
    "# learner = HierarchicalConceptLearner(nodes=nodes, tokenizer_name='gpt2', node0_batch_size=50)\n",
    "\n",
    "# OPTION 3: Sentence mode (for semantic coherence experiments)\n",
    "# learner = HierarchicalConceptLearner(\n",
    "#     num_nodes=4,\n",
    "#     tokenizer_name=\"gpt2\",\n",
    "#     segmentation_mode='sentences',  # Sentence boundary detection\n",
    "#     min_sentence_tokens=3,          # Filter short sentences\n",
    "#     node0_batch_size=50,           # Batching works in sentence mode too!\n",
    "#     base_url=\"http://kato:8000\"\n",
    "# )\n",
    "\n",
    "print(f\"âœ“ Created hierarchical learner with {learner.num_nodes} nodes\")\n",
    "print(f\"  Tokenizer: {learner.tokenizer_name}\")\n",
    "print(f\"  Segmentation: {learner.segmentation_mode}\")\n",
    "print(f\"  Node0 batch size: {learner.node0_batch_size}\")\n",
    "print(f\"  Chunk size: {learner.node_configs[0].chunk_size}\")\n",
    "print(f\"\\n  Semantic coverage:\")\n",
    "coverage = learner.node_configs[0].chunk_size\n",
    "for i in range(learner.num_nodes):\n",
    "    print(f\"    node{i}: {coverage} tokens\")\n",
    "    coverage *= learner.node_configs[0].chunk_size\n",
    "print(f\"  Nodes: {list(learner.nodes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = train_hierarchical_single_pass(\n",
    "    corpus=corpus,\n",
    "    learner=learner,\n",
    "    delimiter='sentence',\n",
    "    num_levels=4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if key != 'total_time_seconds':\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Large-Scale Dataset Training\n",
    "\n",
    "**Alternative to Option A**: Stream from HuggingFace datasets for production-scale training.\n",
    "\n",
    "**If you completed Option A above, skip to Section 5 (Visualize Frequency Distribution).**\n",
    "\n",
    "**âš ï¸ IMPORTANT**: For large datasets (>10K samples), use the **streaming training** approach below to avoid memory issues. The streaming approach:\n",
    "- Uses constant memory (~1-10 MB per sample) instead of loading everything into RAM\n",
    "- Includes automatic checkpointing every 10K samples for crash recovery  \n",
    "- Processes samples one at a time: stream â†’ segment â†’ train â†’ discard\n",
    "- Can handle datasets of ANY size (millions/billions of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset streaming\n",
    "DATASET_KEY = 'wikitext'  # Options: 'c4', 'refinedweb', 'wikitext', 'openwebtext', etc.\n",
    "MAX_SAMPLES = 15000  # Adjust based on available time (see recommendations above)\n",
    "\n",
    "# Get dataset info\n",
    "dataset_info = StreamingDatasetLoader.get_dataset_info(DATASET_KEY)\n",
    "print(f\"\\nSelected Dataset: {dataset_info['description']}\")\n",
    "print(f\"Source: {dataset_info['name']}\")\n",
    "print(f\"Estimated samples in full dataset: {dataset_info['est_samples']:,}\")\n",
    "\n",
    "# Estimate training time\n",
    "estimate = StreamingDatasetLoader.estimate_time(DATASET_KEY, MAX_SAMPLES)\n",
    "print(f\"\\nYou selected: {MAX_SAMPLES:,} samples\")\n",
    "print(f\"Estimated processing time: {estimate['time_formatted']}\")\n",
    "print(f\"Processing rate: {estimate['rate']:.1f} samples/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOMMENDED: Streaming Training (Memory-Safe for Large Datasets)\n",
    "# This approach uses constant memory and includes checkpointing\n",
    "\n",
    "# OPTION 1: Optimized Uniform Configuration (RECOMMENDED for WikiText)\n",
    "# Uses chunk_size=8 for exponential semantic scaling: 8 â†’ 64 â†’ 512 â†’ 4,096 tokens\n",
    "learner = HierarchicalConceptLearner(\n",
    "    num_nodes=4,\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    segmentation_mode='chunking',  # Scalable, deduplication-friendly\n",
    "    chunk_size=8,                   # OPTIMIZED for WikiText (500-2000 token articles)\n",
    "    node0_batch_size=50,           # IMPORTANT: Enable batching for 4-7x speedup!\n",
    "    base_url=\"http://kato:8000\"\n",
    ")\n",
    "\n",
    "# OPTION 2: Custom Per-Node Configuration (for advanced experimentation)\n",
    "# Note: Only use variable chunk sizes if you have specific semantic goals\n",
    "# For most use cases, uniform chunk_size=8 is optimal\n",
    "# nodes = [\n",
    "#     HierarchicalNode('node0', chunk_size=8, mode='chunking'),   # 8 tokens (phrases)\n",
    "#     HierarchicalNode('node1', chunk_size=8, mode='chunking'),   # 64 tokens (sentences)\n",
    "#     HierarchicalNode('node2', chunk_size=8, mode='chunking'),   # 512 tokens (paragraphs)\n",
    "#     HierarchicalNode('node3', chunk_size=8, mode='chunking')    # 4,096 tokens (articles)\n",
    "# ]\n",
    "# learner = HierarchicalConceptLearner(\n",
    "#     nodes=nodes, \n",
    "#     tokenizer_name='gpt2',\n",
    "#     node0_batch_size=50  # Enable batching for 4-7x speedup!\n",
    "# )\n",
    "\n",
    "# OPTION 3: Sentence mode (uncomment to use)\n",
    "# learner = HierarchicalConceptLearner(\n",
    "#     num_nodes=4,\n",
    "#     tokenizer_name=\"gpt2\",\n",
    "#     segmentation_mode='sentences',  # Semantic boundaries\n",
    "#     min_sentence_tokens=3,\n",
    "#     node0_batch_size=50,           # Batching works in sentence mode too!\n",
    "#     base_url=\"http://kato:8000\"\n",
    "# )\n",
    "\n",
    "print(f\"âœ“ Created hierarchical learner with {learner.num_nodes} nodes\")\n",
    "print(f\"  Tokenizer: {learner.tokenizer_name}\")\n",
    "print(f\"  Segmentation: {learner.segmentation_mode}\")\n",
    "print(f\"  Chunk size: {learner.node_configs[0].chunk_size}\")\n",
    "print(f\"  Node0 batch size: {learner.node0_batch_size} ({'batching ENABLED - expect 4-7x speedup!' if learner.node0_batch_size > 1 else 'batching disabled'})\")\n",
    "print(f\"\\n  Semantic coverage:\")\n",
    "coverage = learner.node_configs[0].chunk_size\n",
    "for i in range(learner.num_nodes):\n",
    "    print(f\"    node{i}: {coverage} tokens\")\n",
    "    coverage *= learner.node_configs[0].chunk_size\n",
    "print(f\"  Nodes: {list(learner.nodes.keys())}\\n\")\n",
    "\n",
    "# Stream and train in one pass (no memory accumulation)\n",
    "# NOTE: Segmentation config now comes from learner's node0 configuration\n",
    "stats = train_from_streaming_dataset(\n",
    "    dataset_key=DATASET_KEY,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    learner=learner,\n",
    "    num_levels=4,\n",
    "    segment_method='simple',  # Options: 'simple', 'article', 'book'\n",
    "    checkpoint_interval=10000,  # Save checkpoint every 10K samples\n",
    "    checkpoint_dir='./checkpoints',\n",
    "    resume_from_checkpoint=False,  # Set to True to resume from last checkpoint\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if 'time' not in key.lower():\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: Parallel Training (Phase 3 - FASTEST!)\n",
    "\n",
    "**NEW! Phase 3 Optimization**: Train with parallel workers for additional 2-3x speedup on top of batching.\n",
    "\n",
    "**Combined Speedup (Phases 1+2+3)**: 15-28x faster than baseline!\n",
    "\n",
    "**How it works**:\n",
    "- Multiple workers process samples concurrently\n",
    "- Each worker gets its own isolated KATO session\n",
    "- No lock contention or race conditions\n",
    "- MongoDB handles concurrent writes safely\n",
    "\n",
    "**Recommended**: Use 4-8 workers depending on your CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING HIERARCHICAL CONCEPT LEARNER\n",
      "================================================================================\n",
      "Using custom node configurations (4 nodes)\n",
      "\n",
      "âœ“ 4 nodes initialized with:\n",
      "  - max_pattern_length = 0 (manual learning)\n",
      "  - stm_mode = CLEAR (STM clears after learn)\n",
      "  - process_predictions = False (predictions disabled)\n",
      "  - tokenizer = gpt2\n",
      "\n",
      "Per-node configuration:\n",
      "  node0: mode=chunking, chunk_size=8\n",
      "  node1: mode=chunking, chunk_size=8\n",
      "  node2: mode=chunking, chunk_size=8\n",
      "  node3: mode=chunking, chunk_size=8\n",
      "================================================================================\n",
      "âœ“ Created hierarchical learner with 4 nodes\n",
      "  Chunk size: 8 (optimized for WikiText)\n",
      "  Node0 batch size: 50 (batching ENABLED)\n",
      "\n",
      "  Semantic coverage:\n",
      "    node0: 8 tokens\n",
      "    node1: 64 tokens\n",
      "    node2: 512 tokens\n",
      "    node3: 4096 tokens\n",
      "  Training with parallel workers...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "PARALLEL STREAMING HIERARCHICAL TRAINING\n",
      "================================================================================\n",
      "Dataset: WikiText-103 - Wikipedia articles (script-free)\n",
      "Source: Salesforce/wikitext\n",
      "Samples: 10\n",
      "Workers: 4 (parallel processing)\n",
      "Segmentation: simple\n",
      "Node0 batch size: 50\n",
      "Est. Time (sequential): 1s\n",
      "Est. Time (parallel): 0s\n",
      "Expected speedup: 3.0x\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ“¡ Streaming: WikiText-103 - Wikipedia articles (script-free)\n",
      "   Dataset: Salesforce/wikitext\n",
      "   Samples: 10\n",
      "   Est. Time: 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.13sample/s, rate=2.1/s, errors=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PARALLEL TRAINING COMPLETE\n",
      "================================================================================\n",
      "âœ“ Samples processed: 10\n",
      "â±ï¸  Total time: 5s\n",
      "ðŸ“Š Rate: 2.1 samples/second\n",
      "ðŸš€ Workers: 4\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 68f2988dd7d4aea72b1e6945, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Training with parallel workers...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Phase 3: Train with parallel workers for additional 2-3x speedup\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_from_streaming_dataset_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Match number of nodes\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Recommended: 4-8 workers (adjust based on CPU cores)\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ“ Parallel training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPerformance Statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/tools/streaming_dataset_loader.py:1009\u001b[0m, in \u001b[0;36mtrain_from_streaming_dataset_parallel\u001b[0;34m(dataset_key, max_samples, learner, num_levels, segment_method, tokenizer_name, num_workers, verbose)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_from_streaming_dataset_parallel\u001b[39m(\n\u001b[1;32m    994\u001b[0m     dataset_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    995\u001b[0m     max_samples: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    Module-level wrapper for parallel streaming training.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m    Train hierarchical learner with parallel workers for 2-3x speedup.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    See StreamingDatasetLoader.train_from_streaming_dataset_parallel() for full documentation.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStreamingDatasetLoader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_from_streaming_dataset_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43msegment_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/tools/streaming_dataset_loader.py:944\u001b[0m, in \u001b[0;36mStreamingDatasetLoader.train_from_streaming_dataset_parallel\u001b[0;34m(dataset_key, max_samples, learner, num_levels, segment_method, tokenizer_name, num_workers, verbose)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhierarchical_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoDBAnalyzer\n\u001b[1;32m    943\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m MongoDBAnalyzer(learner\u001b[38;5;241m.\u001b[39mnodes[node_name])\n\u001b[0;32m--> 944\u001b[0m node_stats \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    945\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    946\u001b[0m stats[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_patterns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m node_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_patterns\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/work/tools/hierarchical_learning.py:209\u001b[0m, in \u001b[0;36mMongoDBAnalyzer.get_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03mGet comprehensive statistics about the knowledge base.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m              median_frequency, patterns_by_frequency_range\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Get all frequencies\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m frequencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatterns_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$project\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfrequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$group\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$sum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$avg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$max\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$min\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m frequencies:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_patterns\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    227\u001b[0m     }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/collection.py:3015\u001b[0m, in \u001b[0;36mCollection.aggregate\u001b[0;34m(self, pipeline, session, let, comment, **kwargs)\u001b[0m\n\u001b[1;32m   2929\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform an aggregation using the aggregation framework on this\u001b[39;00m\n\u001b[1;32m   2930\u001b[0m \u001b[38;5;124;03mcollection.\u001b[39;00m\n\u001b[1;32m   2931\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[38;5;124;03m    https://mongodb.com/docs/manual/reference/command/aggregate\u001b[39;00m\n\u001b[1;32m   3013\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_database\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_tmp_session(session, close\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m-> 3015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3016\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_CollectionAggregationCommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCommandCursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3019\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplicit_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/_csot.py:125\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/collection.py:2913\u001b[0m, in \u001b[0;36mCollection._aggregate\u001b[0;34m(self, aggregation_command, pipeline, cursor_class, session, explicit_session, let, comment, **kwargs)\u001b[0m\n\u001b[1;32m   2902\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m comment\n\u001b[1;32m   2903\u001b[0m cmd \u001b[38;5;241m=\u001b[39m aggregation_command(\n\u001b[1;32m   2904\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2905\u001b[0m     cursor_class,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2910\u001b[0m     user_fields\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcursor\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirstBatch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}},\n\u001b[1;32m   2911\u001b[0m )\n\u001b[0;32m-> 2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_read_preference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   2916\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_performs_write\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_Op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAGGREGATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2047\u001b[0m, in \u001b[0;36mMongoClient._retryable_read\u001b[0;34m(self, func, read_pref, session, operation, address, retryable, operation_id)\u001b[0m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;66;03m# Ensure that the client supports retrying on reads and there is no session in\u001b[39;00m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# transaction, otherwise, we will not support retry behavior for this call.\u001b[39;00m\n\u001b[1;32m   2044\u001b[0m retryable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\n\u001b[1;32m   2045\u001b[0m     retryable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mretry_reads \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (session \u001b[38;5;129;01mand\u001b[39;00m session\u001b[38;5;241m.\u001b[39min_transaction)\n\u001b[1;32m   2046\u001b[0m )\n\u001b[0;32m-> 2047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_read\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/_csot.py:125\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2014\u001b[0m, in \u001b[0;36mMongoClient._retry_internal\u001b[0;34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[0m\n\u001b[1;32m   1977\u001b[0m \u001b[38;5;129m@_csot\u001b[39m\u001b[38;5;241m.\u001b[39mapply\n\u001b[1;32m   1978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_retry_internal\u001b[39m(\n\u001b[1;32m   1979\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1988\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1989\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal retryable helper for all client transactions.\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m    :param func: Callback function we want to retry\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;124;03m    :return: Output of the calling func()\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ClientConnectionRetryable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmongo_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbulk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbulk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_read\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_pref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2765\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error(check_csot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2764\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_read \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write()\n\u001b[1;32m   2766\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[1;32m   2767\u001b[0m     \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[1;32m   2768\u001b[0m     \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m     \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_last_error()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2910\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable._read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2906\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper method for read-type retryable client executions\u001b[39;00m\n\u001b[1;32m   2907\u001b[0m \n\u001b[1;32m   2908\u001b[0m \u001b[38;5;124;03m    :return: Output for func()'s call\u001b[39;00m\n\u001b[1;32m   2909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2911\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_pref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead Preference required on read calls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2912\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_conn_from_server(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_pref, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[1;32m   2913\u001b[0m         conn,\n\u001b[1;32m   2914\u001b[0m         read_pref,\n\u001b[1;32m   2915\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:2858\u001b[0m, in \u001b[0;36m_ClientConnectionRetryable._get_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_server\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Server:\n\u001b[1;32m   2854\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves a server object based on provided object context\u001b[39;00m\n\u001b[1;32m   2855\u001b[0m \n\u001b[1;32m   2856\u001b[0m \u001b[38;5;124;03m    :return: Abstraction to connect to server\u001b[39;00m\n\u001b[1;32m   2857\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2864\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/mongo_client.py:1833\u001b[0m, in \u001b[0;36mMongoClient._select_server\u001b[0;34m(self, server_selector, session, operation, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m   1831\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m AutoReconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m no longer available\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m address)  \u001b[38;5;66;03m# noqa: UP031\u001b[39;00m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1833\u001b[0m         server \u001b[38;5;241m=\u001b[39m \u001b[43mtopology\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[43m            \u001b[49m\u001b[43mserver_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m            \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m            \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1838\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m server\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PyMongoError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;66;03m# Server selection errors in a transaction are transient.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/topology.py:409\u001b[0m, in \u001b[0;36mTopology.select_server\u001b[0;34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_server\u001b[39m(\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    401\u001b[0m     selector: Callable[[Selection], Selection],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Server:\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Like select_servers, but choose a random server if several match.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_server\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeprioritized_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _csot\u001b[38;5;241m.\u001b[39mget_timeout():\n\u001b[1;32m    418\u001b[0m         _csot\u001b[38;5;241m.\u001b[39mset_rtt(server\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;241m.\u001b[39mmin_round_trip_time)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/topology.py:387\u001b[0m, in \u001b[0;36mTopology._select_server\u001b[0;34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select_server\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     selector: Callable[[Selection], Selection],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     operation_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    386\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Server:\n\u001b[0;32m--> 387\u001b[0m     servers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_servers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_selection_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     servers \u001b[38;5;241m=\u001b[39m _filter_servers(servers, deprioritized_servers)\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(servers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/topology.py:294\u001b[0m, in \u001b[0;36mTopology.select_servers\u001b[0;34m(self, selector, operation, server_selection_timeout, address, operation_id)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcleanup_monitors()\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 294\u001b[0m     server_descriptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_servers_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    299\u001b[0m         cast(Server, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_server_by_address(sd\u001b[38;5;241m.\u001b[39maddress)) \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m server_descriptions\n\u001b[1;32m    300\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pymongo/synchronous/topology.py:344\u001b[0m, in \u001b[0;36mTopology._select_servers_loop\u001b[0;34m(self, selector, timeout, operation, operation_id, address)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _SERVER_SELECTION_LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):\n\u001b[1;32m    334\u001b[0m         _debug_log(\n\u001b[1;32m    335\u001b[0m             _SERVER_SELECTION_LOGGER,\n\u001b[1;32m    336\u001b[0m             message\u001b[38;5;241m=\u001b[39m_ServerSelectionStatusMessage\u001b[38;5;241m.\u001b[39mFAILED,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m             failure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message(selector),\n\u001b[1;32m    343\u001b[0m         )\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerSelectionTimeoutError(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message(selector)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Timeout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms, Topology Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logged_waiting:\n\u001b[1;32m    349\u001b[0m     _debug_log(\n\u001b[1;32m    350\u001b[0m         _SERVER_SELECTION_LOGGER,\n\u001b[1;32m    351\u001b[0m         message\u001b[38;5;241m=\u001b[39m_ServerSelectionStatusMessage\u001b[38;5;241m.\u001b[39mWAITING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m         remainingTimeMS\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())),\n\u001b[1;32m    358\u001b[0m     )\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 68f2988dd7d4aea72b1e6945, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>"
     ]
    }
   ],
   "source": [
    "# OPTION C: PARALLEL TRAINING (FASTEST - Phase 3!)\n",
    "# Combines Phase 1 batching + Phase 3 parallel workers for maximum speed\n",
    "\n",
    "# Configure dataset\n",
    "DATASET_KEY = 'wikitext'  # Options: 'c4', 'refinedweb', 'wikitext', 'openwebtext'\n",
    "MAX_SAMPLES = 10  # Start with small number to test\n",
    "\n",
    "# Create learner with optimized configuration (Phase 1 batching enabled)\n",
    "# RECOMMENDED: Use chunk_size=8 for exponential semantic scaling\n",
    "nodes = [\n",
    "    HierarchicalNode('node0', chunk_size=8, mode='chunking', base_url='http://kato:8000'),   # 8 tokens (phrases)\n",
    "    HierarchicalNode('node1', chunk_size=8, mode='chunking', base_url='http://kato:8000'),   # 64 tokens (sentences)\n",
    "    HierarchicalNode('node2', chunk_size=8, mode='chunking', base_url='http://kato:8000'),   # 512 tokens (paragraphs)\n",
    "    HierarchicalNode('node3', chunk_size=8, mode='chunking', base_url='http://kato:8000')    # 4,096 tokens (articles)\n",
    "]\n",
    "\n",
    "learner = HierarchicalConceptLearner(\n",
    "    nodes=nodes,\n",
    "    tokenizer_name='gpt2',\n",
    "    node0_batch_size=50  # Phase 1: Batching for 4-7x speedup\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created hierarchical learner with {learner.num_nodes} nodes\")\n",
    "print(f\"  Chunk size: {learner.node_configs[0].chunk_size} (optimized for WikiText)\")\n",
    "print(f\"  Node0 batch size: {learner.node0_batch_size} (batching ENABLED)\")\n",
    "print(f\"\\n  Semantic coverage:\")\n",
    "coverage = learner.node_configs[0].chunk_size\n",
    "for i in range(learner.num_nodes):\n",
    "    print(f\"    node{i}: {coverage} tokens\")\n",
    "    coverage *= learner.node_configs[0].chunk_size\n",
    "print(f\"  Training with parallel workers...\\n\")\n",
    "\n",
    "# Phase 3: Train with parallel workers for additional 2-3x speedup\n",
    "stats = train_from_streaming_dataset_parallel(\n",
    "    dataset_key=DATASET_KEY,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    learner=learner,\n",
    "    num_levels=4,  # Match number of nodes\n",
    "    num_workers=4,  # Recommended: 4-8 workers (adjust based on CPU cores)\n",
    "    segment_method='simple',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Parallel training complete!\")\n",
    "print(f\"\\nPerformance Statistics:\")\n",
    "print(f\"  Samples processed: {stats['samples_processed']:,}\")\n",
    "print(f\"  Total time: {stats['total_time_seconds']:.2f}s\")\n",
    "print(f\"  Rate: {stats['rate_samples_per_sec']:.2f} samples/sec\")\n",
    "print(f\"  Workers: {stats.get('num_workers', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nPattern Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    if 'patterns' in key.lower():\n",
    "        print(f\"  {key}: {value:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Frequency Distribution\n",
    "\n",
    "Create histograms to visualize pattern frequency distributions of the learned patterns in each node's knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize node0 (sentence patterns)\n",
    "print(\"Frequency distribution for node0 (sentence patterns):\")\n",
    "analyzer0 = MongoDBAnalyzer(learner.nodes['node0'])\n",
    "analyzer0.visualize_frequency_distribution(max_freq=10)\n",
    "analyzer0.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize node1 (paragraph patterns)\n",
    "print(\"Frequency distribution for node1 (paragraph patterns):\")\n",
    "analyzer1 = MongoDBAnalyzer(learner.nodes['node1'])\n",
    "analyzer1.visualize_frequency_distribution(max_freq=10)\n",
    "analyzer1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Detailed Frequency Histograms\n",
    "\n",
    "View exact frequency counts for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get histograms for all nodes\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FREQUENCY HISTOGRAMS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for node_name in ['node0', 'node1', 'node2', 'node3']:\n",
    "    analyzer = MongoDBAnalyzer(learner.nodes[node_name])\n",
    "    histogram = analyzer.get_frequency_histogram()\n",
    "    analyzer.close()\n",
    "    \n",
    "    if histogram:\n",
    "        print(f\"{node_name}:\")\n",
    "        for freq in sorted(histogram.keys())[:10]:  # Show first 10 frequencies\n",
    "            print(f\"  Frequency {freq}: {histogram[freq]} patterns\")\n",
    "        if len(histogram) > 10:\n",
    "            print(f\"  ... ({len(histogram) - 10} more frequency levels)\")\n",
    "    else:\n",
    "        print(f\"{node_name}: No patterns\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup Low-Frequency Patterns\n",
    "\n",
    "Remove patterns that appear less than a threshold (e.g., frequency < 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup patterns with frequency < 2\n",
    "print(\"Cleaning up patterns with frequency < 2...\\n\")\n",
    "\n",
    "deleted = cleanup_all_nodes(learner, threshold=2, verbose=True)\n",
    "\n",
    "print(f\"\\nTotal patterns deleted across all nodes: {sum(deleted.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical text generation: Unravel from node1 (paragraph) â†’ node0 (sentences) â†’ text\n",
    "print(f\"{'='*60}\")\n",
    "print(\"HIERARCHICAL GENERATION FROM NODE1 (PARAGRAPH LEVEL)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Sample a paragraph pattern from node1\n",
    "paragraph_patterns = sample_pattern_by_frequency(learner.nodes['node1'], num_samples=1)\n",
    "\n",
    "if paragraph_patterns:\n",
    "    para_pattern = paragraph_patterns[0]\n",
    "    print(f\"Sampled paragraph pattern: {para_pattern['name'][:50]}...\")\n",
    "    print(f\"Contains {len(para_pattern['pattern_data'])} events (sentence pattern names)\\n\")\n",
    "    \n",
    "    # Extract sentence pattern names from the paragraph\n",
    "    sentence_pattern_names = []\n",
    "    for event in para_pattern['pattern_data']:\n",
    "        sentence_pattern_names.extend(event)\n",
    "    \n",
    "    print(f\"Unraveling {len(sentence_pattern_names)} sentence patterns:\\n\")\n",
    "    \n",
    "    # For each sentence pattern name, retrieve it from node0 and decode\n",
    "    analyzer0 = MongoDBAnalyzer(learner.nodes['node0'])\n",
    "    \n",
    "    generated_paragraph = []\n",
    "    for sent_pattern_name in sentence_pattern_names:\n",
    "        # Retrieve the sentence pattern from node0's knowledge base\n",
    "        sent_pattern = analyzer0.patterns_collection.find_one(\n",
    "            {'name': sent_pattern_name},\n",
    "            {'pattern_data': 1, '_id': 0}\n",
    "        )\n",
    "        \n",
    "        if sent_pattern:\n",
    "            # Extract tokens\n",
    "            tokens = []\n",
    "            for event in sent_pattern['pattern_data']:\n",
    "                tokens.extend(event)\n",
    "            tokens = [t for t in tokens if t != '<EOS>']\n",
    "            \n",
    "            # Decode tokens to text\n",
    "            try:\n",
    "                token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                generated_paragraph.append(text)\n",
    "                print(f\"  â€¢ {text}\")\n",
    "            except:\n",
    "                generated_paragraph.append(' '.join(tokens))\n",
    "                print(f\"  â€¢ {' '.join(tokens)}\")\n",
    "    \n",
    "    analyzer0.close()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATED PARAGRAPH (combined):\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(' '.join(generated_paragraph))\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No paragraph patterns available. Train with more data to enable hierarchical generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get predictions from node0 (requires predictions to be enabled)\n",
    "# Uncomment to test if predictions are enabled:\n",
    "\n",
    "# # Observe some tokens to populate STM\n",
    "# test_tokens = learner.token_processor.tokenize_segment(\"Machine learning is powerful\")\n",
    "# for token in test_tokens:\n",
    "#     learner.nodes['node0'].observe(strings=[token])\n",
    "# \n",
    "# # Get predictions\n",
    "# predictions = learner.nodes['node0'].get_predictions()\n",
    "# \n",
    "# if predictions:\n",
    "#     print(f\"âœ“ Retrieved {len(predictions)} predictions\")\n",
    "#     print(\"\\nTop prediction:\")\n",
    "#     print(f\"  Pattern: {predictions[0]['name'][:40]}...\")\n",
    "#     print(f\"  Potential: {predictions[0].get('potential', 0):.3f}\")\n",
    "#     print(f\"  Confidence: {predictions[0].get('confidence', 0):.3f}\")\n",
    "# else:\n",
    "#     print(\"No predictions available. Predictions may not be enabled on this node.\")\n",
    "\n",
    "print(\"Prediction testing code ready (commented out by default)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example custom modeling function\n",
    "def my_custom_filter(predictions, field='name'):\n",
    "    \"\"\"\n",
    "    Custom modeling function: Filter predictions by potential and return top 3.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of prediction dicts\n",
    "        field: Field to extract (default: 'name')\n",
    "    \n",
    "    Returns:\n",
    "        List of filtered pattern names\n",
    "    \"\"\"\n",
    "    # Filter by minimum potential threshold\n",
    "    filtered = [p for p in predictions if p.get('potential', 0) >= 0.3]\n",
    "    \n",
    "    # Sort by confidence\n",
    "    filtered.sort(key=lambda p: p.get('confidence', 0), reverse=True)\n",
    "    \n",
    "    # Return top 3\n",
    "    return [p[field] for p in filtered[:3]]\n",
    "\n",
    "print(\"âœ“ Custom modeling function defined\")\n",
    "print(\"\\nExample usage with transfer_predictions:\")\n",
    "print(\"transfer_predictions(\")\n",
    "print(\"    node_source=learner.nodes['node0'],\")\n",
    "print(\"    node_target=learner.nodes['node1'],\")\n",
    "print(\"    field='name',\")\n",
    "print(\"    modeling_function=my_custom_filter\")\n",
    "print(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate built-in modeling functions\n",
    "print(\"Built-in Modeling Functions:\\n\")\n",
    "\n",
    "print(\"1. transfer_threshold:\")\n",
    "print(\"   - Filter predictions by metric threshold\")\n",
    "print(\"   - Example: transfer_threshold(predictions, metric='potential', threshold=0.4)\")\n",
    "\n",
    "print(\"\\n2. transfer_top_n:\")\n",
    "print(\"   - Return top N predictions sorted by metric\")\n",
    "print(\"   - Example: transfer_top_n(predictions, n=5, sort_by='confidence')\")\n",
    "\n",
    "print(\"\\n3. transfer_weighted:\")\n",
    "print(\"   - Weight patterns by metric with repetition\")\n",
    "print(\"   - Example: transfer_weighted(predictions, weight_by='confidence', max_repeats=5)\")\n",
    "\n",
    "print(\"\\n4. transfer_all_names:\")\n",
    "print(\"   - Pass through all predictions without filtering\")\n",
    "print(\"   - Example: transfer_all_names(predictions)\")\n",
    "\n",
    "print(\"\\nâœ“ All functions are available in the imported tools module\")\n",
    "print(\"\\nTo use with transfer_predictions:\")\n",
    "print(\"result = transfer_predictions(\")\n",
    "print(\"    node_source=learner.nodes['node0'],\")\n",
    "print(\"    node_target=learner.nodes['node1'],\")\n",
    "print(\"    field='name',\")\n",
    "print(\"    modeling_function=transfer_top_n,  # or any other function\")\n",
    "print(\"    num_predictions=10\")\n",
    "print(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we accomplished\n",
    "print(f\"{'='*80}\")\n",
    "print(\"HIERARCHICAL TRAINING SESSION SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"âœ“ What We Did:\")\n",
    "print(\"  1. Created 4-level hierarchical learner (node0 â†’ node1 â†’ node2 â†’ node3)\")\n",
    "print(\"  2. Segmented text into hierarchical structure (books â†’ chapters â†’ paragraphs â†’ sentences)\")\n",
    "print(\"  3. Trained single-pass with pattern names flowing up the hierarchy\")\n",
    "print(\"  4. Analyzed frequency distributions at each level\")\n",
    "print(\"  5. Cleaned up low-frequency patterns (noise removal)\")\n",
    "print(\"  6. Generated new text by sampling and unraveling learned patterns â­\")\n",
    "\n",
    "print(\"\\nâœ“ Key Insights:\")\n",
    "print(\"  â€¢ Pattern frequencies follow Zipfian distribution (natural language)\")\n",
    "print(\"  â€¢ Higher levels have fewer but more abstract patterns\")\n",
    "print(\"  â€¢ Cleanup improves pattern quality by removing rare/noisy patterns\")\n",
    "print(\"  â€¢ Text generation works by sampling + unraveling + decoding\")\n",
    "\n",
    "print(\"\\nâœ“ Pattern Statistics:\")\n",
    "all_stats_final = analyze_all_nodes(learner)\n",
    "for node_name in ['node0', 'node1', 'node2', 'node3']:\n",
    "    stats = all_stats_final[node_name]\n",
    "    print(f\"  {node_name}: {stats['total_patterns']:,} patterns \"\n",
    "          f\"(avg freq: {stats['average_frequency']:.2f})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Session Complete! You now have a trained hierarchical learning system.\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Prediction testing requires enabling predictions on nodes. This is optional and primarily for advanced research use cases involving prediction-based transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text by sampling from node0 (sentence patterns)\n",
    "print(f\"{'='*60}\")\n",
    "print(\"GENERATING SENTENCES FROM NODE0\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Sample 5 sentence patterns from node0\n",
    "sampled_sentences = sample_pattern_by_frequency(learner.nodes['node0'], num_samples=5)\n",
    "\n",
    "# Decode each pattern back to text\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(learner.token_processor.tokenizer_name)\n",
    "\n",
    "print(\"Generated Sentences:\\n\")\n",
    "for i, pattern in enumerate(sampled_sentences, 1):\n",
    "    # Extract tokens from pattern_data\n",
    "    # Pattern data is a list of events, each event is a list of symbols\n",
    "    tokens = []\n",
    "    for event in pattern['pattern_data']:\n",
    "        tokens.extend(event)\n",
    "    \n",
    "    # Remove special tokens like <EOS>\n",
    "    tokens = [t for t in tokens if t != '<EOS>']\n",
    "    \n",
    "    # Convert token strings to IDs (if needed) and decode\n",
    "    try:\n",
    "        # Try to decode directly if tokens are proper tokenizer tokens\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        text = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "        print(f\"{i}. {text}\")\n",
    "    except:\n",
    "        # Fallback: just print the tokens\n",
    "        print(f\"{i}. {' '.join(tokens)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(sampled_sentences)} sentences from learned patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample patterns from a node based on frequency\n",
    "def sample_pattern_by_frequency(node, num_samples=1):\n",
    "    \"\"\"\n",
    "    Sample pattern(s) from a node's knowledge base using frequency as probability.\n",
    "    Higher frequency patterns are more likely to be sampled.\n",
    "    \n",
    "    Args:\n",
    "        node: KATO node to sample from\n",
    "        num_samples: Number of patterns to sample\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled pattern dictionaries with 'name' and 'pattern_data'\n",
    "    \"\"\"\n",
    "    analyzer = MongoDBAnalyzer(node)\n",
    "    \n",
    "    # Get all patterns with their frequencies\n",
    "    patterns = list(analyzer.patterns_collection.find(\n",
    "        {},\n",
    "        {'name': 1, 'pattern_data': 1, 'frequency': 1, '_id': 0}\n",
    "    ))\n",
    "    analyzer.close()\n",
    "    \n",
    "    if not patterns:\n",
    "        print(f\"No patterns found in {node.node_id}\")\n",
    "        return []\n",
    "    \n",
    "    # Use frequency as sampling weight\n",
    "    import numpy as np\n",
    "    frequencies = np.array([p['frequency'] for p in patterns])\n",
    "    probabilities = frequencies / frequencies.sum()\n",
    "    \n",
    "    # Sample indices\n",
    "    sampled_indices = np.random.choice(len(patterns), size=num_samples, p=probabilities, replace=False)\n",
    "    \n",
    "    return [patterns[i] for i in sampled_indices]\n",
    "\n",
    "print(\"âœ“ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Text Generation from Learned Patterns â­\n",
    "\n",
    "**This is the PRIMARY use case of hierarchical learning!**\n",
    "\n",
    "Now that we've learned patterns at multiple levels, we can generate new text by:\n",
    "1. **Sampling** from learned patterns at any level using frequency statistics\n",
    "2. **Unraveling** patterns hierarchically (top-down: book â†’ chapter â†’ paragraph â†’ sentence)\n",
    "3. **Decoding** tokens back to human-readable text\n",
    "\n",
    "This section demonstrates:\n",
    "- Sampling from node0 (sentence level)\n",
    "- Unraveling patterns from higher nodes\n",
    "- Generating coherent text at multiple scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-analyze all nodes to see the impact of cleanup\n",
    "print(f\"{'='*60}\")\n",
    "print(\"POST-CLEANUP ANALYSIS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "all_stats_after = analyze_all_nodes(learner, verbose=True)\n",
    "\n",
    "# Compare statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLEANUP IMPACT SUMMARY\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(f\"{'Node':<8} {'Patterns':<12} {'Total Freq':<15} {'Avg Freq':<12} {'Max Freq':<10}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for node_name in ['node0', 'node1', 'node2', 'node3']:\n",
    "    stats = all_stats_after[node_name]\n",
    "    print(f\"{node_name:<8} {stats['total_patterns']:<12,} {stats['total_frequency']:<15,} \"\n",
    "          f\"{stats['average_frequency']:<12.2f} {stats['max_frequency']:<10}\")\n",
    "\n",
    "print(f\"\\nâœ“ Re-analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "**Experimentation Ideas:**\n",
    "1. Train on your own text data\n",
    "2. Try different tokenizers (BERT, RoBERTa, T5, LLaMA)\n",
    "3. Experiment with different cleanup thresholds\n",
    "4. Create more sophisticated modeling functions\n",
    "5. Use deeper hierarchies (5, 10+ nodes)\n",
    "6. Generate longer text sequences (paragraphs, chapters, books)\n",
    "7. Test prediction-based transfers with `transfer_predictions()`\n",
    "8. Analyze pattern content using `MongoDBAnalyzer.get_patterns_by_frequency()`\n",
    "\n",
    "**Large-Scale Training (NEW! Memory-Safe):**\n",
    "For datasets >10K samples, use the streaming training approach to avoid memory crashes:\n",
    "```python\n",
    "stats = train_from_streaming_dataset(\n",
    "    dataset_key='wikitext',\n",
    "    max_samples=1500000,\n",
    "    learner=learner,\n",
    "    num_levels=4,\n",
    "    checkpoint_interval=10000,\n",
    "    resume_from_checkpoint=False  # Set True to resume after crash\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits of Streaming Training:**\n",
    "- âœ… Constant memory usage (~1-10 MB per sample)\n",
    "- âœ… Automatic checkpointing every N samples\n",
    "- âœ… Resume from checkpoint after interruption\n",
    "- âœ… Can handle unlimited dataset sizes (millions/billions)\n",
    "- âœ… No OOM crashes!\n",
    "\n",
    "**Training on Different Text Types:**\n",
    "- Use `segment_method='simple'` for generic text (default)\n",
    "- Use `segment_method='article'` for article-like text with sections\n",
    "- Use `segment_method='book'` for book-like text with chapters\n",
    "\n",
    "**Supported Tokenizers:**\n",
    "- GPT-2: `\"gpt2\"` (default, BPE with Ä  space markers)\n",
    "- BERT: `\"bert-base-uncased\"`, `\"bert-base-cased\"` (WordPiece with ## continuation)\n",
    "- RoBERTa: `\"roberta-base\"` (byte-level BPE)\n",
    "- T5: `\"t5-small\"`, `\"t5-base\"`, `\"t5-large\"` (SentencePiece)\n",
    "- Others: ALBERT, DistilBERT, XLNet, ELECTRA, DeBERTa, BART, Phi-2, LLaMA-2\n",
    "\n",
    "**Advanced Usage:**\n",
    "```python\n",
    "# Use transfer_predictions for prediction-based transfers\n",
    "# (After training and enabling predictions on source node)\n",
    "result = transfer_predictions(\n",
    "    node_source=learner.nodes['node0'],\n",
    "    node_target=learner.nodes['node1'],\n",
    "    field='name',\n",
    "    modeling_function=my_threshold_model,\n",
    "    num_predictions=10\n",
    ")\n",
    "```\n",
    "\n",
    "**Text Generation at Different Scales:**\n",
    "- Generate from node0 â†’ sentences (token sequences)\n",
    "- Generate from node1 â†’ paragraphs (unravel to sentences)\n",
    "- Generate from node2 â†’ chapters (unravel to paragraphs â†’ sentences)\n",
    "- Generate from node3 â†’ books (full hierarchical unraveling)\n",
    "\n",
    "**Granularity Emerges from Hierarchy:**\n",
    "- node0: TOKEN-level patterns (from tokenized sentences)\n",
    "- node1: SENTENCE-level patterns (from token pattern sequences)\n",
    "- node2: PARAGRAPH-level patterns (from sentence pattern sequences)\n",
    "- node3: CHAPTER-level patterns (from paragraph pattern sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Generation from Learned Patterns â­\n",
    "\n",
    "**This is the PRIMARY use case of hierarchical learning!**\n",
    "\n",
    "Now that we've learned patterns at multiple levels, we can generate new text by:\n",
    "1. **Sampling** from learned patterns at any level using frequency statistics\n",
    "2. **Unraveling** patterns hierarchically (top-down: book â†’ chapter â†’ paragraph â†’ sentence)\n",
    "3. **Decoding** tokens back to human-readable text\n",
    "\n",
    "This section demonstrates:\n",
    "- Sampling from node0 (sentence level)\n",
    "- Unraveling patterns from higher nodes\n",
    "- Generating coherent text at multiple scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prediction Testing (Optional/Advanced)\n",
    "\n",
    "**Note:** Prediction testing requires enabling predictions on nodes. This is optional and primarily for advanced research use cases involving prediction-based transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Custom Modeling Functions (Optional/Advanced)\n",
    "\n",
    "Create custom functions to model prediction ensembles for transfer between nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "Review what we've accomplished in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "**Experimentation Ideas:**\n",
    "1. Train on your own text data\n",
    "2. Try different tokenizers (BERT, RoBERTa, LLaMA)\n",
    "3. Experiment with different cleanup thresholds\n",
    "4. Create more sophisticated modeling functions\n",
    "5. Use deeper hierarchies (5, 10+ nodes)\n",
    "6. Generate longer text sequences (paragraphs, chapters, books)\n",
    "7. Test prediction-based transfers with `transfer_predictions()`\n",
    "8. Analyze pattern content using `MongoDBAnalyzer.get_patterns_by_frequency()`\n",
    "\n",
    "**Advanced Usage:**\n",
    "```python\n",
    "# Use transfer_predictions for prediction-based transfers\n",
    "# (After training and enabling predictions on source node)\n",
    "result = transfer_predictions(\n",
    "    node_source=learner.nodes['node0'],\n",
    "    node_target=learner.nodes['node1'],\n",
    "    field='name',\n",
    "    modeling_function=my_threshold_model,\n",
    "    num_predictions=10\n",
    ")\n",
    "```\n",
    "\n",
    "**Text Generation at Different Scales:**\n",
    "- Generate from node0 â†’ sentences\n",
    "- Generate from node1 â†’ paragraphs (unravel to sentences)\n",
    "- Generate from node2 â†’ chapters (unravel to paragraphs â†’ sentences)\n",
    "- Generate from node3 â†’ books (full hierarchical unraveling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
