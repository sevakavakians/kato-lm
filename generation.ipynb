{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Hierarchical Text Generation\n",
    "\n",
    "**Educational Notebook**: Learn how to use KATO's hierarchical architecture for text generation.\n",
    "\n",
    "**Prerequisites**: Train patterns first using `training.ipynb`.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "This notebook demonstrates **hierarchical text generation** through two complementary processes:\n",
    "\n",
    "### 1. Bottom-Up Activation (Input → Predictions)\n",
    "\n",
    "```\n",
    "User Input: \"The cat sat\"\n",
    "     ↓\n",
    "Tokenize: [\"The\", \" cat\", \" sat\"]\n",
    "     ↓\n",
    "node0: Observe tokens → Get predictions (chunk patterns)\n",
    "     ↓\n",
    "node1: Observe node0 pattern names → Get predictions (paragraph patterns)\n",
    "     ↓\n",
    "node2: Observe node1 pattern names → Get predictions (chapter patterns)\n",
    "     ↓\n",
    "node3: Observe node2 pattern names → Get predictions (book patterns)\n",
    "```\n",
    "\n",
    "**Result**: All hierarchy levels are activated by the input.\n",
    "\n",
    "### 2. Top-Down Unraveling (Pattern → Tokens)\n",
    "\n",
    "```\n",
    "node3 prediction: PTRN|book_xyz\n",
    "     ↓\n",
    "Query MongoDB: Get pattern_data → [node2_pattern_A, node2_pattern_B, ...]\n",
    "     ↓\n",
    "For each node2 pattern:\n",
    "    Query MongoDB → [node1_pattern_X, node1_pattern_Y, ...]\n",
    "        ↓\n",
    "    For each node1 pattern:\n",
    "        Query MongoDB → [node0_pattern_1, node0_pattern_2, ...]\n",
    "            ↓\n",
    "        For each node0 pattern:\n",
    "            Query MongoDB → [\"token1\", \"token2\", \"token3\"]\n",
    "                ↓\n",
    "            Decode tokens → \"The cat sat on the mat\"\n",
    "```\n",
    "\n",
    "**Result**: High-level pattern names are recursively unraveled down to actual tokens.\n",
    "\n",
    "### Cascading Constraint Satisfaction\n",
    "\n",
    "The combination creates **exponential search space reduction**:\n",
    "\n",
    "- **node3** says: \"We're generating text from 'Moby Dick' context\"\n",
    "- **node2** says: \"We're in a chapter about whales\"\n",
    "- **node1** says: \"We're in a descriptive paragraph\"\n",
    "- **node0** says: \"Next tokens should describe whale behavior\"\n",
    "\n",
    "Each level constrains the levels below, producing coherent, context-aware text.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook **exposes all KATO API calls** directly (no library abstractions) so you can see:\n",
    "\n",
    "1. How to use `observe()`, `observe_sequence()`, `get_predictions()`, `clear_stm()`\n",
    "2. How to query MongoDB to retrieve pattern structures\n",
    "3. How to recursively unravel patterns from high-level → tokens\n",
    "4. How the hierarchy creates coherent, faithful text generation\n",
    "\n",
    "**Educational Focus**: Understanding KATO's API and hierarchical generation mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "!pip install -q requests datasets transformers # numpy matplotlib tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from tools.kato_client import KATOClient\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json\n",
    "\n",
    "# Note: We're using KATO's get_pattern() API for pattern retrieval\n",
    "# No direct database access needed - KATO combines ClickHouse + Redis data\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "\n",
    "**IMPORTANT**: These must match your training configuration!\n",
    "\n",
    "- `CHUNK_SIZE`: Number of tokens per chunk at node0 (must match training)\n",
    "- `TOKENIZER_NAME`: HuggingFace tokenizer used during training\n",
    "- `NODE_IDS`: KATO node identifiers (check your training manifest or MongoDB databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Chunk sizes: [8, 8, 8, 8]\n",
      "  Max predictions: [10, 10, 10, 10]\n",
      "  Tokenizer: gpt2\n",
      "  Node IDs: ['node0', 'node1', 'node2', 'node3']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - MODIFY THESE TO MATCH YOUR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "# Hierarchical configuration (MUST match training!)\n",
    "# chunk_sizes: How many tokens/patterns to process at each level\n",
    "# max_predictions: How many predictions in each ensemble sent to next level\n",
    "CHUNK_SIZES = [8, 8, 8, 8]      # [node0, node1, node2, node3]\n",
    "MAX_PREDICTIONS = [10, 10, 10, 10]  # [node0, node1, node2, node3]\n",
    "\n",
    "# Tokenizer (must match training)\n",
    "TOKENIZER_NAME = \"gpt2\"  # Options: \"gpt2\", \"bert-base-uncased\", \"roberta-base\", etc.\n",
    "\n",
    "# Recall threshold (pattern matching strictness)\n",
    "# Range: 0.0-1.0\n",
    "#   0.1 = permissive (many matches, lower quality)\n",
    "#   0.9 = strict (few matches, higher quality)\n",
    "# Default: 0.6 (balanced)\n",
    "RECALL_THRESHOLD_DEFAULT = 0.6\n",
    "\n",
    "# KATO server URL\n",
    "BASE_URL = \"http://kato:8000\"\n",
    "\n",
    "# Node identifiers (MUST match your training configuration)\n",
    "# Check your ClickHouse/Redis databases to find the correct node_ids\n",
    "# Format: {node_id}_kato databases in ClickHouse/Redis\n",
    "NODE_IDS = [\n",
    "    \"node0\",  # node0: chunk-level patterns (15 tokens)\n",
    "    \"node1\",  # node1: paragraph-level patterns (~225 tokens)\n",
    "    \"node2\",  # node2: chapter-level patterns (~3,375 tokens)\n",
    "    \"node3\"   # node3: book-level patterns (~50,625 tokens)\n",
    "]\n",
    "\n",
    "# Prediction fallback chain (try node3 first, fallback to node2, then node1, then node0)\n",
    "FALLBACK_LEVELS = [3, 2, 1, 0]\n",
    "\n",
    "# Validate configuration\n",
    "if len(CHUNK_SIZES) != len(NODE_IDS):\n",
    "    raise ValueError(f\"CHUNK_SIZES length ({len(CHUNK_SIZES)}) must match NODE_IDS length ({len(NODE_IDS)})\")\n",
    "if len(MAX_PREDICTIONS) != len(NODE_IDS):\n",
    "    raise ValueError(f\"MAX_PREDICTIONS length ({len(MAX_PREDICTIONS)}) must match NODE_IDS length ({len(NODE_IDS)})\")\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Chunk sizes: {CHUNK_SIZES}\")\n",
    "print(f\"  Max predictions: {MAX_PREDICTIONS}\")\n",
    "print(f\"  Tokenizer: {TOKENIZER_NAME}\")\n",
    "print(f\"  Node IDs: {NODE_IDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Class: TextTokenizer\n",
    "\n",
    "Simple wrapper around HuggingFace tokenizer for:\n",
    "1. Tokenizing input text\n",
    "2. Chunking tokens into fixed-length sequences\n",
    "3. Decoding tokens back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcadf520ead141bb9013a27ce59943b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03d8a7d82664a619a3ebfa7287f8285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022d276350e24a0990024303168cc529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435874ac40e2442f872981bcb7429a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3b0f83786e4ca8b5803453b9fa21fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer initialized: gpt2\n",
      "\n",
      "Test tokenization:\n",
      "  Input: The cat sat on the mat.\n",
      "  Tokens: ['The', 'Ġcat', 'Ġsat', 'Ġon', 'Ġthe', 'Ġmat', '.']\n",
      "  Decoded: The cat sat on the mat.\n"
     ]
    }
   ],
   "source": [
    "class TextTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer wrapper for KATO hierarchical generation.\n",
    "    \n",
    "    Uses HuggingFace AutoTokenizer for tokenization and decoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer_name: str):\n",
    "        \"\"\"Initialize with HuggingFace tokenizer.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        \n",
    "        # Required for some tokenizers (GPT-2, etc.)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text into list of token strings.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            List of token strings (e.g., [\"The\", \" cat\", \" sat\"])\n",
    "        \"\"\"\n",
    "        # Tokenize using HuggingFace tokenizer\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    def chunk_tokens(self, tokens: List[str], chunk_size: int) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Split tokens into fixed-length chunks.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of token strings\n",
    "            chunk_size: Number of tokens per chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of token chunks (last chunk may be shorter)\n",
    "            \n",
    "        Example:\n",
    "            >>> tokens = [\"The\", \" cat\", \" sat\", \" on\", \" the\", \" mat\"]\n",
    "            >>> chunk_tokens(tokens, chunk_size=3)\n",
    "            [[\"The\", \" cat\", \" sat\"], [\" on\", \" the\", \" mat\"]]\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            chunk = tokens[i:i + chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    def decode_tokens(self, tokens: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token strings back to text.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of token strings\n",
    "            \n",
    "        Returns:\n",
    "            Decoded text string\n",
    "        \"\"\"\n",
    "        # Convert token strings to IDs, then decode\n",
    "        # This handles subword tokenization correctly\n",
    "        text = self.tokenizer.convert_tokens_to_string(tokens)\n",
    "        return text\n",
    "\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = TextTokenizer(TOKENIZER_NAME)\n",
    "print(f\"✓ Tokenizer initialized: {TOKENIZER_NAME}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"The cat sat on the mat.\"\n",
    "test_tokens = tokenizer.tokenize(test_text)\n",
    "print(f\"\\nTest tokenization:\")\n",
    "print(f\"  Input: {test_text}\")\n",
    "print(f\"  Tokens: {test_tokens}\")\n",
    "print(f\"  Decoded: {tokenizer.decode_tokens(test_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KATO Client Session Cleanup\n",
    "\n",
    "Why This Is Needed:\n",
    "\n",
    "* Prevents multiple sessions for the same node\n",
    "* Clears stale state from previous runs\n",
    "* Ensures fresh initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up old sessions...\n",
      "  ℹ No existing nodes to clean up (first run)\n",
      "✓ Session cleanup complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SESSION CLEANUP - Ensure Clean State\n",
    "# ============================================================================\n",
    "# Close any existing KATO sessions to prevent stale state issues.\n",
    "# This is especially important when re-running cells after errors.\n",
    "\n",
    "print(\"Cleaning up old sessions...\")\n",
    "\n",
    "try:\n",
    "    # Try to close existing node clients\n",
    "    for node in [node0, node1, node2, node3]:\n",
    "        try:\n",
    "            node.close()\n",
    "            print(f\"  ✓ Closed {node.node_id} session\")\n",
    "        except Exception as e:\n",
    "            # Ignore errors - node may not exist or session already closed\n",
    "            pass\n",
    "except NameError:\n",
    "    # Nodes don't exist yet (first run)\n",
    "    print(\"  ℹ No existing nodes to clean up (first run)\")\n",
    "\n",
    "print(\"✓ Session cleanup complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KATO Client Initialization\n",
    "\n",
    "**Educational**: Create one `KATOClient` instance per hierarchical level.\n",
    "\n",
    "Each client:\n",
    "- Connects to KATO server\n",
    "- Has its own isolated session\n",
    "- Uses `max_pattern_length=0` (prediction mode, not learning)\n",
    "- Maintains its own Short-Term Memory (STM)\n",
    "\n",
    "**Note**: We're using KATO for **generation**, not training, so we won't call `learn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing KATO clients...\n",
      "  ✓ node0: recall=0.3, max_pred=10\n",
      "  ✓ node1: recall=0.1, max_pred=10\n",
      "  ✓ node2: recall=0.1, max_pred=10\n",
      "  ✓ node3: recall=0.1, max_pred=10\n",
      "\n",
      "✓ All KATO clients ready for generation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KATO CLIENT INITIALIZATION - ONE PER NODE\n",
    "# ============================================================================\n",
    "\n",
    "# Create separate KATO clients for each hierarchical level\n",
    "# max_pattern_length=0: Prediction mode (no auto-learning)\n",
    "# recall_threshold: Pattern matching strictness\n",
    "# max_predictions: Number of predictions per ensemble (KATO config)\n",
    "# process_predictions=True: MUST enable predictions (may be disabled from training)\n",
    "\n",
    "print(\"Initializing KATO clients...\")\n",
    "\n",
    "# recall_threshold controls pattern matching strictness:\n",
    "#   High (0.7-0.9): Strict matching, fewer but higher-quality predictions\n",
    "#   Medium (0.4-0.6): Balanced (default: 0.6)\n",
    "#   Low (0.1-0.3): Permissive matching, more predictions (useful for novel inputs)\n",
    "#\n",
    "# max_predictions controls ensemble size:\n",
    "#   - KATO returns top N predictions per call\n",
    "#   - Entire ensemble sent as ONE event to next level\n",
    "#   - KATO's pattern matching handles missing/extra symbols gracefully\n",
    "#   - Higher values: more context but slower, potentially noisy\n",
    "#   - Lower values: faster but may miss important patterns\n",
    "#\n",
    "# process_predictions=True:\n",
    "#   - CRITICAL: Must be True for generation\n",
    "#   - During training, this is often set to False to save computation\n",
    "#   - Explicitly setting True here ensures predictions work regardless of training config\n",
    "\n",
    "node0 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    node_id=NODE_IDS[0],\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=RECALL_THRESHOLD_DEFAULT,\n",
    "    max_predictions=MAX_PREDICTIONS[0],\n",
    "    process_predictions=False,  # Enable prediction processing\n",
    "    timeout=1200\n",
    ")\n",
    "\n",
    "node0.update_session_config({\n",
    "      'use_token_matching': True,  # False = Character-level mode, True = Token-level mode\n",
    "      'filter_pipeline': ['jaccard'],\n",
    "      'jaccard_threshold': 0.3,      # Token set overlap\n",
    "      'jaccard_min_overlap': 2,      # At least 2 shared tokens\n",
    "      # 'minhash_threshold': 0.1,      # Lower = more recall (try 0.4-0.6)\n",
    "      'recall_threshold': 0.3,       # Sequence similarity\n",
    "      'max_predictions': 10,\n",
    "      'max_candidates_per_stage': 1000,\n",
    "})\n",
    "\n",
    "config = node0.get_session_config()\n",
    "print(f\"  ✓ node0: recall={config['recall_threshold']}, max_pred={config['max_predictions']}\")\n",
    "\n",
    "node1 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    node_id=NODE_IDS[1],\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=0.1,\n",
    "    max_predictions=MAX_PREDICTIONS[1],\n",
    "    process_predictions=False,  # Enable prediction processing\n",
    "    timeout=1200\n",
    ")\n",
    "config = node1.get_session_config()\n",
    "print(f\"  ✓ node1: recall={config['recall_threshold']}, max_pred={config['max_predictions']}\")\n",
    "\n",
    "\n",
    "node2 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    node_id=NODE_IDS[2],\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=0.1,\n",
    "    max_predictions=MAX_PREDICTIONS[2],\n",
    "    process_predictions=False,  # Enable prediction processing\n",
    "    timeout=1200\n",
    ")\n",
    "config = node2.get_session_config()\n",
    "print(f\"  ✓ node2: recall={config['recall_threshold']}, max_pred={config['max_predictions']}\")\n",
    "\n",
    "\n",
    "node3 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    node_id=NODE_IDS[3],\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=0.1,\n",
    "    max_predictions=MAX_PREDICTIONS[3],\n",
    "    process_predictions=False,  # Enable prediction processing\n",
    "    timeout=1200\n",
    ")\n",
    "config = node3.get_session_config()\n",
    "print(f\"  ✓ node3: recall={config['recall_threshold']}, max_pred={config['max_predictions']}\")\n",
    "\n",
    "\n",
    "# Store in list for easy iteration\n",
    "nodes = [node0, node1, node2, node3]\n",
    "\n",
    "print(\"\\n✓ All KATO clients ready for generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure nodes for better generation based on validated settings\n",
    "# node0 uses jaccard filter for token matching\n",
    "# # Higher nodes use empty filter_pipeline for default pattern matching\n",
    "# result = node0.update_session_config({      \n",
    "#     'filter_pipeline': ['jaccard'],      \n",
    "#     'jaccard_threshold': 0.3,      \n",
    "#     # Token set overlap      \n",
    "#     'jaccard_min_overlap': 2,      \n",
    "#     # At least 2 shared tokens      \n",
    "#     'recall_threshold': 0.3,       \n",
    "#     # Sequence similarity (lower for more matches)      \n",
    "#     'max_predictions': 10,      \n",
    "#     'use_token_matching': True     \n",
    "#     # Enable token-level matching for node0\n",
    "# })\n",
    "\n",
    "# print(\"node0:\", result)\n",
    "# # Verify token matching is enabled\n",
    "# config = node0.get_session_config()\n",
    "# print(f\"  ✓ use_token_matching: {config.get('use_token_matching')}\")\n",
    "# # For node1, node2, node3: Use empty filter_pipeline if there's not a lot of data\n",
    "# # Jaccard at higher levels can return nothing if patterns are sparse\n",
    "# result = node1.update_session_config({      'filter_pipeline': [],  \n",
    "#                                       # Empty = use default pattern matcher      \n",
    "#                                       'recall_threshold': 0.1,       \n",
    "#                                       # Very permissive for higher levels      \n",
    "#                                       'max_predictions': 10,})\n",
    "# print(\"node1:\", result)result = node2.update_session_config({      \n",
    "#     'filter_pipeline': [],  # Empty = use default pattern matcher      \n",
    "#     'recall_threshold': 0.1,       # Very permissive for higher levels      \n",
    "#     'max_predictions': 10,})print(\"node2:\", result)\n",
    "# result = node3.update_session_config({      \n",
    "#     'filter_pipeline': [],  # Empty = use default pattern matcher      \n",
    "#     'recall_threshold': 0.1,       \n",
    "#     # Very permissive for higher levels      \n",
    "#     'max_predictions': 10,})\n",
    "# print(\"node3:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Retrieval via KATO API\n",
    "\n",
    "**Educational**: KATO provides a `get_pattern()` API that retrieves learned patterns from its hybrid storage architecture.\n",
    "\n",
    "### How KATO Stores Patterns\n",
    "\n",
    "KATO internally uses two databases:\n",
    "- **ClickHouse**: Stores pattern structure (`pattern_data`, `length`, `tokens`, etc.)\n",
    "- **Redis**: Stores metadata (`frequency`, `emotives`, additional context)\n",
    "\n",
    "### Using KATO's API\n",
    "\n",
    "Instead of directly accessing databases, we use KATO's API:\n",
    "\n",
    "```python\n",
    "# Get a pattern via KATO API\n",
    "result = node.get_pattern(pattern_name)\n",
    "\n",
    "# Response structure:\n",
    "{\n",
    "    \"pattern\": {\n",
    "        \"status\": \"okay\",\n",
    "        \"pattern\": {\n",
    "            \"name\": \"abc123...\",\n",
    "            \"pattern_data\": [[\"child1\"], [\"child2\"], ...],\n",
    "            \"length\": 8,\n",
    "            \"frequency\": 42,\n",
    "            \"metadata\": {...},\n",
    "            // Combined ClickHouse + Redis data\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Pattern Data Structure\n",
    "\n",
    "For **node0**, `pattern_data` contains actual tokens: `[[\"The\"], [\" cat\"], [\" sat\"]]`\n",
    "\n",
    "For **higher levels**, `pattern_data` contains child pattern names from the level below.\n",
    "\n",
    "**Benefits of using KATO's API**:\n",
    "- Combines data from both storage systems\n",
    "- Handles errors and missing patterns gracefully\n",
    "- Maintains abstraction - storage can change without breaking code\n",
    "- Provides consistent interface across all levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTION - Pattern Name Prefix Handling\n",
    "# ============================================================================\n",
    "\n",
    "def strip_pattern_prefix(pattern_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip 'PTRN|' prefix from pattern name if present.\n",
    "    \n",
    "    MongoDB stores pattern names WITHOUT the 'PTRN|' prefix in the 'name' field,\n",
    "    but pattern references in pattern_data and KATO predictions may include it.\n",
    "    This function ensures consistent lookups.\n",
    "    \n",
    "    Args:\n",
    "        pattern_name: Pattern name, possibly with 'PTRN|' prefix\n",
    "        \n",
    "    Returns:\n",
    "        Pattern name without 'PTRN|' prefix\n",
    "        \n",
    "    Examples:\n",
    "        >>> strip_pattern_prefix('PTRN|abc123')\n",
    "        'abc123'\n",
    "        >>> strip_pattern_prefix('abc123')\n",
    "        'abc123'\n",
    "    \"\"\"\n",
    "    if pattern_name.startswith('PTRN|'):\n",
    "        return pattern_name[5:]  # Remove 'PTRN|' (5 characters)\n",
    "    return pattern_name\n",
    "\n",
    "print(\"✓ Helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ STM inspection helper defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTION - STM Inspection for Debugging\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_stm(node, node_name: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Get and display current STM (Short-Term Memory) state.\n",
    "    \n",
    "    This helper is useful for debugging cascade issues - shows how many\n",
    "    events are in STM and previews the content.\n",
    "    \n",
    "    Args:\n",
    "        node: KATOClient instance\n",
    "        node_name: Display name (e.g., \"node0\", \"node1\")\n",
    "        verbose: Whether to print details\n",
    "        \n",
    "    Returns:\n",
    "        List of STM events\n",
    "        \n",
    "    Example:\n",
    "        >>> inspect_stm(node1, \"node1\")\n",
    "        node1 STM: 3 events\n",
    "          STM: ['PTRN|abc', 'PTRN|def', 'PTRN|xyz']\n",
    "    \"\"\"\n",
    "    stm_data = node.get_stm()\n",
    "    stm = stm_data.get('stm', [])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  {node_name} STM: {len(stm)} events\")\n",
    "        if len(stm) > 0:\n",
    "            print(f\"     STM: {stm}\")    \n",
    "    return stm\n",
    "\n",
    "print(\"✓ STM inspection helper defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ activate_hierarchy() function defined\n"
     ]
    }
   ],
   "source": [
    "def activate_hierarchy(\n",
    "    input_text: str,\n",
    "    verbose: bool = True,\n",
    "    recall_threshold_overrides: Dict[str, Dict[str, Any]] = None,\n",
    "    chunk_sizes: List[int] = None,\n",
    "    max_predictions: List[int] = None\n",
    ") -> Dict[int, Dict]:\n",
    "    \"\"\"\n",
    "    Activate hierarchical KATO system with input text (bottom-up).\n",
    "    \n",
    "    CORRECTED FLOW: For each chunk, cascade predictions through ALL levels.\n",
    "    \n",
    "    For each chunk:\n",
    "        1. Clear node0 STM\n",
    "        2. Observe chunk at node0 → get predictions\n",
    "        3. If predictions exist → observe at node1 → get predictions\n",
    "        4. If predictions exist → observe at node2 → get predictions\n",
    "        5. If predictions exist → observe at node3 → get predictions\n",
    "    \n",
    "    Higher levels (node1, node2, node3) accumulate events across chunks.\n",
    "    \n",
    "    CRITICAL: Pattern names must include \"PTRN|\" prefix when sent to higher levels\n",
    "    to match training format.\n",
    "    \n",
    "    Args:\n",
    "        input_text: User input text\n",
    "        verbose: Print intermediate results (including STM inspection)\n",
    "        recall_threshold_overrides: Per-node recall thresholds\n",
    "        chunk_sizes: Override CHUNK_SIZES for testing\n",
    "        max_predictions: Override MAX_PREDICTIONS for testing\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping level → predictions (from last chunk's cascade)\n",
    "    \"\"\"\n",
    "    # Use global configs if not overridden\n",
    "    if chunk_sizes is None:\n",
    "        chunk_sizes = CHUNK_SIZES\n",
    "    if max_predictions is None:\n",
    "        max_predictions = MAX_PREDICTIONS\n",
    "    \n",
    "    # Validate\n",
    "    if len(chunk_sizes) != len(nodes):\n",
    "        raise ValueError(f\"chunk_sizes must have {len(nodes)} elements, got {len(chunk_sizes)}\")\n",
    "    if len(max_predictions) != len(nodes):\n",
    "        raise ValueError(f\"max_predictions must have {len(nodes)} elements, got {len(max_predictions)}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Input: {input_text}\")\n",
    "        print(f\"Config: chunk_sizes={chunk_sizes}, max_pred={max_predictions}\")\n",
    "    \n",
    "    # Apply recall_threshold overrides if specified\n",
    "    if recall_threshold_overrides:\n",
    "        if verbose:\n",
    "            print(f\"\\n--- APPLYING RECALL_THRESHOLD OVERRIDES ---\")\n",
    "        \n",
    "        for node_name, genes in recall_threshold_overrides.items():\n",
    "            node_idx = int(node_name.replace('node', ''))\n",
    "            nodes[node_idx].update_genes(genes)\n",
    "            \n",
    "            if verbose and 'recall_threshold' in genes:\n",
    "                print(f\"✓ Updated {node_name} recall_threshold: {genes['recall_threshold']}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "    \n",
    "    # Step 1: Tokenize input text\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    if verbose:\n",
    "        print(f\"\\nTokens ({len(tokens)}): {tokens}\")\n",
    "    \n",
    "    # Step 2: Chunk tokens using chunk_sizes[0] (node0's chunk size)\n",
    "    chunks = tokenizer.chunk_tokens(tokens, chunk_sizes[0])\n",
    "    if verbose:\n",
    "        print(f\"\\nChunks ({len(chunks)}) with chunk_size={chunk_sizes[0]}:\")\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            print(f\"  Chunk {idx}: {chunk}\")\n",
    "    \n",
    "    # Clear all nodes' STM before starting\n",
    "    if verbose:\n",
    "        print(f\"\\n--- INITIALIZING: Clearing all nodes' STM ---\")\n",
    "    for i, node in enumerate(nodes):\n",
    "        node.clear_stm()\n",
    "        if verbose:\n",
    "            print(f\"✓ Cleared node{i} STM\")\n",
    "    \n",
    "    # Track predictions from last cascade (for return value)\n",
    "    predictions_0 = {'predictions': []}\n",
    "    predictions_1 = {'predictions': []}\n",
    "    predictions_2 = {'predictions': []}\n",
    "    predictions_3 = {'predictions': []}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PROCESS EACH CHUNK: CASCADE THROUGH ALL LEVELS\n",
    "    # ========================================================================\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"CHUNK {chunk_idx + 1}/{len(chunks)}: {chunk}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        # Clear node0 STM for new chunk\n",
    "        node0.clear_stm()\n",
    "        if verbose:\n",
    "            print(f\"✓ Cleared node0 STM for new chunk\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NODE0: Observe chunk → Get predictions\n",
    "        # ====================================================================\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- NODE0 ---\")\n",
    "        \n",
    "        # Build observations: ONE EVENT PER TOKEN\n",
    "        observations = [{'strings': [token]} for token in chunk]\n",
    "        \n",
    "        # Send chunk to node0\n",
    "        node0.observe_sequence(observations=observations, learn_at_end=False)\n",
    "        if verbose:\n",
    "            print(f\"✓ Observed {len(chunk)} tokens\")\n",
    "            inspect_stm(node0, \"node0\")\n",
    "        \n",
    "        # Get predictions from node0\n",
    "        predictions_0 = node0.get_predictions()\n",
    "        num_preds_0 = len(predictions_0.get('predictions', []))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Got {num_preds_0} predictions\")\n",
    "            if num_preds_0 > 0:\n",
    "                print(f\"  Sample predictions:\")\n",
    "                for i, pred in enumerate(predictions_0['predictions'][:3]):\n",
    "                    print(f\"    {i+1}. {pred['name'][:50]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "            else:\n",
    "                print(f\"  ⚠ No predictions from node0\")\n",
    "                print(f\"     Possible reasons:\")\n",
    "                print(f\"       - No patterns in KB match this token sequence\")\n",
    "                print(f\"       - recall_threshold too high (current: {RECALL_THRESHOLD_DEFAULT})\")\n",
    "        \n",
    "        if num_preds_0 == 0:\n",
    "            if verbose:\n",
    "                print(f\"⚠ No predictions from node0 - stopping cascade for this chunk\")\n",
    "            continue  # No predictions, can't cascade further\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NODE1: Observe node0 predictions → Get predictions\n",
    "        # ====================================================================\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- NODE1 ---\")\n",
    "        \n",
    "        # Send node0 predictions as ONE event to node1\n",
    "        # CRITICAL: Prepend \"PTRN|\" to match training format!\n",
    "        pattern_names_0 = [f\"PTRN|{pred['name']}\" for pred in predictions_0['predictions']]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Sending {len(pattern_names_0)} pattern names as 1 event:\")\n",
    "            for i, name in enumerate(pattern_names_0[:3]):\n",
    "                print(f\"    {i+1}. {name[:50]}...\")\n",
    "        \n",
    "        node1.observe(strings=pattern_names_0)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Observed (1 event)\")\n",
    "            inspect_stm(node1, \"node1\")\n",
    "        \n",
    "        # Get predictions from node1\n",
    "        predictions_1 = node1.get_predictions()\n",
    "        num_preds_1 = len(predictions_1.get('predictions', []))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Got {num_preds_1} predictions\")\n",
    "            if num_preds_1 > 0:\n",
    "                print(f\"  Sample predictions:\")\n",
    "                for i, pred in enumerate(predictions_1['predictions'][:3]):\n",
    "                    print(f\"    {i+1}. {pred['name'][:50]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "            else:\n",
    "                stm = node1.get_stm().get('stm', [])\n",
    "                print(f\"  ⚠ No predictions from node1\")\n",
    "                print(f\"     STM has {len(stm)} event(s)\")\n",
    "                if len(stm) > 0:\n",
    "                    print(f\"     First event: {stm[0][:3]}... ({len(stm[0])} symbols)\")\n",
    "                print(f\"     Possible reasons:\")\n",
    "                print(f\"       - No node1 patterns in KB match this sequence of node0 patterns\")\n",
    "                print(f\"       - recall_threshold too high (current: {RECALL_THRESHOLD_DEFAULT})\")\n",
    "                print(f\"       - Pattern names don't match KB format\")\n",
    "        \n",
    "        if num_preds_1 == 0:\n",
    "            if verbose:\n",
    "                print(f\"⚠ No predictions from node1 - stopping cascade for this chunk\")\n",
    "            continue  # No predictions, can't cascade further\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NODE2: Observe node1 predictions → Get predictions\n",
    "        # ====================================================================\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- NODE2 ---\")\n",
    "        \n",
    "        # Send node1 predictions as ONE event to node2\n",
    "        # CRITICAL: Prepend \"PTRN|\" to match training format!\n",
    "        pattern_names_1 = [f\"PTRN|{pred['name']}\" for pred in predictions_1['predictions']]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Sending {len(pattern_names_1)} pattern names as 1 event:\")\n",
    "            for i, name in enumerate(pattern_names_1[:3]):\n",
    "                print(f\"    {i+1}. {name[:50]}...\")\n",
    "        \n",
    "        node2.observe(strings=pattern_names_1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Observed (1 event)\")\n",
    "            inspect_stm(node2, \"node2\")\n",
    "        \n",
    "        # Get predictions from node2\n",
    "        predictions_2 = node2.get_predictions()\n",
    "        num_preds_2 = len(predictions_2.get('predictions', []))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Got {num_preds_2} predictions\")\n",
    "            if num_preds_2 > 0:\n",
    "                print(f\"  Sample predictions:\")\n",
    "                for i, pred in enumerate(predictions_2['predictions'][:3]):\n",
    "                    print(f\"    {i+1}. {pred['name'][:50]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "            else:\n",
    "                stm = node2.get_stm().get('stm', [])\n",
    "                print(f\"  ⚠ No predictions from node2\")\n",
    "                print(f\"     STM has {len(stm)} event(s)\")\n",
    "                if len(stm) > 0:\n",
    "                    print(f\"     First event: {stm[0][:3]}... ({len(stm[0])} symbols)\")\n",
    "                print(f\"     Possible reasons:\")\n",
    "                print(f\"       - No node2 patterns in KB match this sequence\")\n",
    "                print(f\"       - recall_threshold too high\")\n",
    "        \n",
    "        if num_preds_2 == 0:\n",
    "            if verbose:\n",
    "                print(f\"⚠ No predictions from node2 - stopping cascade for this chunk\")\n",
    "            continue  # No predictions, can't cascade further\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NODE3: Observe node2 predictions → Get predictions\n",
    "        # ====================================================================\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- NODE3 ---\")\n",
    "        \n",
    "        # Send node2 predictions as ONE event to node3\n",
    "        # CRITICAL: Prepend \"PTRN|\" to match training format!\n",
    "        pattern_names_2 = [f\"PTRN|{pred['name']}\" for pred in predictions_2['predictions']]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Sending {len(pattern_names_2)} pattern names as 1 event:\")\n",
    "            for i, name in enumerate(pattern_names_2[:3]):\n",
    "                print(f\"    {i+1}. {name[:50]}...\")\n",
    "        \n",
    "        node3.observe(strings=pattern_names_2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Observed (1 event)\")\n",
    "            inspect_stm(node3, \"node3\")\n",
    "        \n",
    "        # Get predictions from node3\n",
    "        predictions_3 = node3.get_predictions()\n",
    "        num_preds_3 = len(predictions_3.get('predictions', []))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Got {num_preds_3} predictions\")\n",
    "            if num_preds_3 > 0:\n",
    "                print(f\"  Sample predictions:\")\n",
    "                for i, pred in enumerate(predictions_3['predictions'][:3]):\n",
    "                    print(f\"    {i+1}. {pred['name'][:50]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "            else:\n",
    "                stm = node3.get_stm().get('stm', [])\n",
    "                print(f\"  ⚠ No predictions from node3\")\n",
    "                print(f\"     STM has {len(stm)} event(s)\")\n",
    "                if len(stm) > 0:\n",
    "                    print(f\"     First event: {stm[0][:3]}... ({len(stm[0])} symbols)\")\n",
    "    \n",
    "    # Return predictions from last chunk's cascade\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ACTIVATION COMPLETE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nFinal prediction counts (from last chunk cascade):\")\n",
    "        print(f\"  node0: {len(predictions_0.get('predictions', []))} predictions\")\n",
    "        print(f\"  node1: {len(predictions_1.get('predictions', []))} predictions\")\n",
    "        print(f\"  node2: {len(predictions_2.get('predictions', []))} predictions\")\n",
    "        print(f\"  node3: {len(predictions_3.get('predictions', []))} predictions\")\n",
    "    \n",
    "    return {\n",
    "        0: predictions_0,\n",
    "        1: predictions_1,\n",
    "        2: predictions_2,\n",
    "        3: predictions_3\n",
    "    }\n",
    "\n",
    "print(\"✓ activate_hierarchy() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Hierarchical Activation Flow (CORRECTED)\n",
    "\n",
    "#### Key Principle: Per-Chunk Cascading\n",
    "\n",
    "**CRITICAL**: For EACH chunk, predictions cascade through ALL hierarchical levels immediately.\n",
    "\n",
    "```\n",
    "Chunk 1 Processing:\n",
    "  Tokens → node0 → predictions → node1 → predictions → node2 → predictions → node3\n",
    "\n",
    "Chunk 2 Processing:\n",
    "  Tokens → node0 → predictions → node1 → predictions → node2 → predictions → node3\n",
    "\n",
    "Chunk 3 Processing:\n",
    "  Tokens → node0 → predictions → node1 → predictions → node2 → predictions → node3\n",
    "```\n",
    "\n",
    "**NOT** (this would be wrong):\n",
    "```\n",
    "❌ All chunks → node0 → THEN all to node1 → THEN all to node2 → THEN all to node3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Detailed Flow for Each Chunk\n",
    "\n",
    "```python\n",
    "# For each chunk in input:\n",
    "\n",
    "1. Clear node0 STM (fresh processing for new chunk)\n",
    "2. node0.observe(chunk_tokens) → get_predictions()\n",
    "   └─ If predictions exist:\n",
    "3.     node1.observe(node0_pattern_names) → get_predictions()\n",
    "       └─ If predictions exist:\n",
    "4.         node2.observe(node1_pattern_names) → get_predictions()\n",
    "           └─ If predictions exist:\n",
    "5.             node3.observe(node2_pattern_names) → get_predictions()\n",
    "\n",
    "# Higher levels (node1, node2, node3) accumulate events across chunks\n",
    "# node0 is cleared between chunks to match training\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters\n",
    "\n",
    "**During Training:**\n",
    "```\n",
    "Chunk 1: Tokens → node0 learns → Sends 1 pattern name to node1\n",
    "Chunk 2: Tokens → node0 learns → Sends 1 pattern name to node1\n",
    "Chunk 3: Tokens → node0 learns → Sends 1 pattern name to node1\n",
    "...\n",
    "node1 sees sequence: [name1, name2, name3, ...] → learns patterns\n",
    "```\n",
    "\n",
    "**During Generation (must match):**\n",
    "```\n",
    "Chunk 1: Tokens → node0 predicts → Sends N pattern names to node1 (ensemble)\n",
    "Chunk 2: Tokens → node0 predicts → Sends N pattern names to node1 (ensemble)\n",
    "Chunk 3: Tokens → node0 predicts → Sends N pattern names to node1 (ensemble)\n",
    "...\n",
    "node1 sees sequence: [ensemble1, ensemble2, ensemble3, ...] → predicts\n",
    "```\n",
    "\n",
    "**Key Insight**: \n",
    "- Each chunk cascades immediately through all levels\n",
    "- Higher levels accumulate events from multiple chunks\n",
    "- node0 is cleared per-chunk (matches training)\n",
    "- node1/node2/node3 maintain their STM across chunks (accumulate context)\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: 3 Chunks\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the mat\" \n",
    "Chunks: [\"The cat sat\", \"on the mat\"]\n",
    "\n",
    "Chunk 1: [\"The\", \"cat\", \"sat\"]\n",
    "  node0: observe → predict [A, B, C]\n",
    "  node1: observe [A, B, C] → predict [X, Y]\n",
    "  node2: observe [X, Y] → predict [P]\n",
    "  node3: observe [P] → predict [M]\n",
    "  \n",
    "Chunk 2: [\"on\", \"the\", \"mat\"]\n",
    "  node0: clear, observe → predict [D, E, F]\n",
    "  node1: observe [D, E, F] → predict [W, Z]  # STM now has 2 events\n",
    "  node2: observe [W, Z] → predict [Q]        # STM now has 2 events\n",
    "  node3: observe [Q] → predict [N]            # STM now has 2 events\n",
    "\n",
    "Final predictions returned: node0=[D,E,F], node1=[W,Z], node2=[Q], node3=[N]\n",
    "```\n",
    "\n",
    "**Why clear node0 but not others?**\n",
    "- node0 processes fixed-length chunks (must match training chunk size)\n",
    "- node1+ accumulate sequences over time (match training's paragraph/chapter/book accumulation)\n",
    "\n",
    "---\n",
    "\n",
    "#### Stopping Cascade Early\n",
    "\n",
    "If any level returns 0 predictions, the cascade stops for that chunk:\n",
    "\n",
    "```\n",
    "Chunk 1:\n",
    "  node0 → 0 predictions ⚠ STOP (no cascade to higher levels)\n",
    "\n",
    "Chunk 2:\n",
    "  node0 → [A, B]\n",
    "  node1 → 0 predictions ⚠ STOP (no cascade to node2/node3)\n",
    "```\n",
    "\n",
    "This prevents propagating \"no match\" signals up the hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2: Prediction Ensemble with Fallback\n",
    "\n",
    "**Purpose**: Select predictions from the highest level that has results **with usable 'future' data**.\n",
    "\n",
    "**Fallback Logic**:\n",
    "1. Try node3 first (book-level context)\n",
    "2. If empty or no 'future' data, try node2 (chapter-level context)\n",
    "3. If empty or no 'future' data, try node1 (paragraph-level context)\n",
    "4. If empty or no 'future' data, try node0 (chunk-level context)\n",
    "\n",
    "**IMPORTANT**: Only predictions with **non-empty 'future' field** can be used for generation!\n",
    "\n",
    "**Why 'future' validation?**: \n",
    "\n",
    "KATO predictions contain:\n",
    "- `'name'`: The matched pattern (what was recognized)\n",
    "- `'future'`: Predicted next symbols/tokens (what comes next)\n",
    "\n",
    "For text generation, we need the `'future'` field. Without it, we can't generate predictions - we'd just be unraveling the matched pattern (regenerating training data).\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Prediction WITH 'future' (usable ✓)\n",
    "{\n",
    "    'name': 'PTRN|abc123',\n",
    "    'future': [['PTRN|def456'], ['PTRN|ghi789']],  # ✓ Can generate from this\n",
    "    'confidence': 0.85\n",
    "}\n",
    "\n",
    "# Prediction WITHOUT 'future' (unusable ❌)\n",
    "{\n",
    "    'name': 'PTRN|abc123',\n",
    "    'future': [],  # ❌ Empty - nothing to generate\n",
    "    'confidence': 0.85\n",
    "}\n",
    "```\n",
    "\n",
    "**Why fallback?**: Novel input may not activate high-level patterns, or high-level patterns may not have 'future' data. Fallback ensures we find predictions with usable 'future' field at some level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ get_prediction_ensemble() function defined\n"
     ]
    }
   ],
   "source": [
    "def get_prediction_ensemble(\n",
    "    all_predictions: Dict[int, Dict],\n",
    "    fallback_levels: List[int] = [3, 2, 1, 0],\n",
    "    verbose: bool = True\n",
    ") -> Tuple[List[Dict], int]:\n",
    "    \"\"\"\n",
    "    Get prediction ensemble from highest available level with usable future data.\n",
    "    \n",
    "    Tries each level in fallback_levels order, returning predictions\n",
    "    from the first level that has non-empty 'future' fields.\n",
    "    \n",
    "    IMPORTANT: Only predictions with non-empty 'future' field can be used\n",
    "    for text generation. The 'future' field contains the predicted next\n",
    "    pattern names or tokens.\n",
    "    \n",
    "    Args:\n",
    "        all_predictions: Dictionary from activate_hierarchy()\n",
    "        fallback_levels: Levels to try in order (default: [3, 2, 1, 0])\n",
    "        verbose: Print which level was used\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (predictions list, level used)\n",
    "    \"\"\"\n",
    "    for level in fallback_levels:\n",
    "        predictions = all_predictions.get(level, {}).get('predictions', [])\n",
    "        \n",
    "        if len(predictions) > 0:\n",
    "            # Filter predictions with non-empty 'future' field\n",
    "            valid_predictions = [\n",
    "                pred for pred in predictions\n",
    "                if pred.get('future') and len(pred.get('future', [])) > 0\n",
    "            ]\n",
    "            \n",
    "            if len(valid_predictions) > 0:\n",
    "                if verbose:\n",
    "                    print(f\"\\n✓ Using node{level} predictions ({len(valid_predictions)} patterns with usable future data)\")\n",
    "                    if len(valid_predictions) < len(predictions):\n",
    "                        filtered = len(predictions) - len(valid_predictions)\n",
    "                        print(f\"  (Filtered out {filtered} predictions with empty 'future' field)\")\n",
    "                return valid_predictions, level\n",
    "            elif verbose:\n",
    "                print(f\"\\n⚠ node{level} has {len(predictions)} predictions but all have empty 'future' fields\")\n",
    "    \n",
    "    # No predictions at any level with usable future data\n",
    "    if verbose:\n",
    "        print(\"\\n⚠ No predictions with non-empty 'future' field at any level\")\n",
    "        print(\"   (Input may be novel, or predictions don't contain future data)\")\n",
    "    \n",
    "    return [], -1\n",
    "\n",
    "print(\"✓ get_prediction_ensemble() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3: Recursive Pattern Unraveling\n",
    "\n",
    "**Purpose**: Recursively unravel a pattern from high-level → tokens (top-down).\n",
    "\n",
    "**How it works**:\n",
    "- **Base case** (level 0): Pattern contains tokens → return them\n",
    "- **Recursive case** (level > 0): Pattern contains child pattern names → unravel each child\n",
    "\n",
    "**MongoDB Query**: This function shows the **exact MongoDB query** used to retrieve pattern structure:\n",
    "```python\n",
    "pattern_doc = db.find_one({'name': pattern_name})\n",
    "pattern_data = pattern_doc['pattern_data']\n",
    "```\n",
    "\n",
    "**Pattern Data Structure**:\n",
    "- `pattern_data = [[\"child1\"], [\"child2\"], [\"child3\"]]`\n",
    "- Each element is an event (list)\n",
    "- For node0: events contain tokens\n",
    "- For higher levels: events contain child pattern names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_pattern(\n",
    "    pattern_name: str,\n",
    "    level: int,\n",
    "    nodes: List[KATOClient],\n",
    "    verbose: bool = False,\n",
    "    indent: int = 0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively unravel a pattern to tokens (top-down traversal).\n",
    "    \n",
    "    This function takes a pattern from any level (1-3) and recursively unravels it\n",
    "    down to the base tokens. Each pattern represents 15 child patterns (fixed chunking),\n",
    "    and at the bottom (node0), each pattern contains 15 tokens.\n",
    "    \n",
    "    Args:\n",
    "        pattern_name: Pattern identifier (e.g., \"PTRN|abc123...\" or \"abc123...\")\n",
    "        level: Which hierarchical level this pattern is from (0-3)\n",
    "        nodes: List of KATOClient instances for each level\n",
    "        verbose: Print unraveling steps\n",
    "        indent: Indentation level for verbose output\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens that the pattern expands to\n",
    "    \n",
    "    Example:\n",
    "        >>> tokens = unravel_pattern(\"PTRN|book_xyz\", level=3, \n",
    "        ...                          nodes=nodes)\n",
    "    \"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    \n",
    "    # Clean the pattern name (remove PTRN| prefix if present)\n",
    "    clean_name = strip_pattern_prefix(pattern_name)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}[Level {level}] Unraveling: {clean_name[:16]}...\")\n",
    "    \n",
    "    # Query pattern from KATO API\n",
    "    result = nodes[level].get_pattern(clean_name)\n",
    "    inner = result.get('pattern', {})\n",
    "    pattern_ch = inner.get('pattern') if inner.get('status') == 'okay' else None\n",
    "    \n",
    "    if pattern_ch is None:\n",
    "        if verbose:\n",
    "            print(f\"{prefix}  ⚠️ Pattern not found, returning empty\")\n",
    "        return []\n",
    "    \n",
    "    # For node0 patterns, return tokens directly\n",
    "    if level == 0:\n",
    "        # Extract tokens from the observations\n",
    "        tokens = []\n",
    "        observations = pattern_ch.get('observations', [])\n",
    "        for obs in observations:\n",
    "            token_list = obs.get('strings', [])\n",
    "            tokens.extend(token_list)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{prefix}  → {len(tokens)} tokens\")\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    # For higher levels, recurse on each child pattern\n",
    "    all_tokens = []\n",
    "    observations = pattern_ch.get('observations', [])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}  Found {len(observations)} child patterns\")\n",
    "    \n",
    "    for obs in observations:\n",
    "        strings = obs.get('strings', [])\n",
    "        \n",
    "        # Child patterns may also have 'PTRN|' prefix - handle properly\n",
    "        for child_pattern in strings:\n",
    "            if verbose:\n",
    "                clean_child = strip_pattern_prefix(child_pattern)\n",
    "                print(f\"{prefix}  Child: {clean_child[:16]}...\")\n",
    "            \n",
    "            # Recursively unravel the child pattern\n",
    "            child_tokens = unravel_pattern(\n",
    "                child_pattern,\n",
    "                level - 1,\n",
    "                nodes=nodes,\n",
    "                verbose=verbose,\n",
    "                indent=indent + 1\n",
    "            )\n",
    "            all_tokens.extend(child_tokens)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}  Total: {len(all_tokens)} tokens\")\n",
    "    \n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_future_list(\n",
    "    future_list: List,\n",
    "    future_level: int,\n",
    "    nodes: List[KATOClient],\n",
    "    verbose: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Unravel a list of future patterns to tokens.\n",
    "    \n",
    "    Unlike the complex unravel_prediction_with_context that was causing topic mixing,\n",
    "    this function ONLY unravels the direct 'future' field from the selected prediction.\n",
    "    It does NOT traverse to other predictions or collect all futures recursively.\n",
    "    \n",
    "    Args:\n",
    "        future_list: List of future patterns from prediction['future']\n",
    "        future_level: Level at which these future patterns exist (0-3)\n",
    "        nodes: List of KATOClient instances for each level\n",
    "        verbose: Print detailed unraveling steps\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens from unraveling the future patterns\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"  Unraveling {len(future_list)} future events from level {future_level}\")\n",
    "    \n",
    "    all_tokens = []\n",
    "    \n",
    "    # Base case: If future_level is 0 or negative, items are already tokens\n",
    "    if future_level <= 0:\n",
    "        # These are already tokens, just extract them\n",
    "        for event in future_list:\n",
    "            if isinstance(event, list) and len(event) > 0:\n",
    "                all_tokens.append(event[0])\n",
    "            elif isinstance(event, str):\n",
    "                all_tokens.append(event)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    → Extracted {len(all_tokens)} tokens directly\")\n",
    "        \n",
    "        return all_tokens\n",
    "    \n",
    "    # Recursive case: future contains pattern names, unravel via KATO API\n",
    "    for event in future_list:\n",
    "        # Extract pattern name from event\n",
    "        # Future events can be: [\"pattern_name\"] or \"pattern_name\"\n",
    "        if isinstance(event, list) and len(event) > 0:\n",
    "            pattern_name = event[0]\n",
    "        elif isinstance(event, str):\n",
    "            pattern_name = event\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            clean_name = strip_pattern_prefix(pattern_name)\n",
    "            print(f\"  Unraveling future pattern: {clean_name[:16]}...\")\n",
    "        \n",
    "        # Unravel this pattern\n",
    "        tokens = unravel_pattern(\n",
    "            pattern_name,\n",
    "            level=future_level,\n",
    "            nodes=nodes,\n",
    "            verbose=verbose,\n",
    "            indent=2 if verbose else 0\n",
    "        )\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Total future tokens: {len(all_tokens)}\")\n",
    "    \n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ unravel_pattern() function defined (using KATO API)\n"
     ]
    }
   ],
   "source": [
    "def unravel_pattern(\n",
    "    pattern_name: str,\n",
    "    level: int,\n",
    "    nodes: List[KATOClient],\n",
    "    verbose: bool = False,\n",
    "    indent: int = 0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively unravel a pattern to tokens using KATO's API (top-down traversal).\n",
    "    \n",
    "    This function uses KATO's get_pattern() API to retrieve pattern structure, then:\n",
    "    - If level 0 (node0): Return the tokens directly\n",
    "    - If level > 0: Recursively unravel each child pattern\n",
    "    \n",
    "    Args:\n",
    "        pattern_name: Pattern identifier (e.g., \"PTRN|abc123...\" or \"abc123...\")\n",
    "        level: Which hierarchical level this pattern is from (0-3)\n",
    "        nodes: List of KATOClient instances\n",
    "        verbose: Print unraveling steps\n",
    "        indent: Indentation level for verbose output\n",
    "        \n",
    "    Returns:\n",
    "        List of token strings\n",
    "        \n",
    "    Example:\n",
    "        >>> tokens = unravel_pattern(\"PTRN|book_xyz\", level=3, nodes=nodes)\n",
    "    \"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}Unraveling node{level}: {pattern_name[:60]}...\")\n",
    "    \n",
    "    # Strip 'PTRN|' prefix if present\n",
    "    clean_name = strip_pattern_prefix(pattern_name)\n",
    "    \n",
    "    # Get pattern via KATO API (not direct database!)\n",
    "    result = nodes[level].get_pattern(clean_name)\n",
    "    inner = result.get('pattern', {})\n",
    "    pattern_ch = inner.get('pattern') if inner.get('status') == 'okay' else None\n",
    "    \n",
    "    if not pattern_ch:\n",
    "        if verbose:\n",
    "            print(f\"{prefix}  ⚠ Pattern not found via KATO API\")\n",
    "            print(f\"{prefix}    Searched for: {clean_name}\")\n",
    "        return []\n",
    "    \n",
    "    pattern_data = pattern_ch.get('pattern_data', [])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}  Pattern has {len(pattern_data)} events\")\n",
    "    \n",
    "    # Base case: node0 patterns contain actual tokens\n",
    "    if level == 0:\n",
    "        # Extract tokens from events\n",
    "        # pattern_data = [[\"The\"], [\" cat\"], [\" sat\"]]\n",
    "        tokens = [event[0] for event in pattern_data if len(event) > 0]\n",
    "        \n",
    "        if verbose:\n",
    "            decoded = tokenizer.decode_tokens(tokens)\n",
    "            print(f\"{prefix}  → Tokens: {tokens[:10]}... → '{decoded[:50]}...'\")\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    # Recursive case: higher-level patterns contain child pattern names\n",
    "    else:\n",
    "        # Extract child pattern names from events and strip prefix\n",
    "        child_patterns = [\n",
    "            strip_pattern_prefix(event[0]) \n",
    "            for event in pattern_data \n",
    "            if len(event) > 0\n",
    "        ]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{prefix}  → Has {len(child_patterns)} children at node{level-1}\")\n",
    "        \n",
    "        # Recursively unravel each child pattern\n",
    "        all_tokens = []\n",
    "        for child_pattern in child_patterns:\n",
    "            # Recursive call: unravel child at level-1\n",
    "            child_tokens = unravel_pattern(\n",
    "                child_pattern,\n",
    "                level - 1,\n",
    "                nodes=nodes,\n",
    "                verbose=verbose,\n",
    "                indent=indent + 1\n",
    "            )\n",
    "            all_tokens.extend(child_tokens)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{prefix}  ✓ Unraveled to {len(all_tokens)} total tokens\")\n",
    "        \n",
    "        return all_tokens\n",
    "\n",
    "print(\"✓ unravel_pattern() function defined (using KATO API)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ generate_text() function defined (with PRESENT + FUTURE + ALL 17 METRICS)\n"
     ]
    }
   ],
   "source": [
    "def extract_tokens_from_present(present_events, level=0, nodes=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Extract tokens from pred['present'] field (KATO event format).\n",
    "    \n",
    "    The 'present' field structure varies by hierarchical level:\n",
    "    - At node0: Contains actual tokens [['The'], ['cat'], ['sat']]\n",
    "    - At node1+: Contains pattern names [['PTRN|abc123'], ['PTRN|def456']]\n",
    "    \n",
    "    For higher levels, pattern names must be unraveled recursively to get tokens.\n",
    "    \n",
    "    This avoids using pred['name'] which returns the full stored pattern\n",
    "    (including future tokens from training time), causing repetition.\n",
    "    \n",
    "    Args:\n",
    "        present_events: List of KATO events from pred['present']\n",
    "        level: Hierarchical level (0 = node0, 1 = node1, etc.)\n",
    "        nodes: List of KATOClient instances (required for level > 0)\n",
    "        verbose: Print unraveling details\n",
    "    \n",
    "    Returns:\n",
    "        List of token strings\n",
    "    \"\"\"\n",
    "    if not present_events:\n",
    "        return []\n",
    "    \n",
    "    if level == 0:\n",
    "        # node0: Extract tokens directly (present contains actual tokens)\n",
    "        tokens = []\n",
    "        for event in present_events:\n",
    "            # Each event is a list of strings (could have anomalies)\n",
    "            # For token-level events, typically just one string per event\n",
    "            if event and len(event) > 0:\n",
    "                tokens.append(event[0])  # Take first string from event\n",
    "        return tokens\n",
    "    else:\n",
    "        # node1+: Unravel pattern names recursively (present contains pattern names)\n",
    "        # CRITICAL: pred['present'] at level N contains level N-1 patterns!\n",
    "        # E.g., node2 prediction's present contains node1 pattern names\n",
    "        # We unravel at level-1 because that's where the patterns actually exist\n",
    "        if nodes is None:\n",
    "            raise ValueError(\"nodes parameter required for hierarchical unraveling (level > 0)\")\n",
    "        \n",
    "        all_tokens = []\n",
    "        for event in present_events:\n",
    "            if event and len(event) > 0:\n",
    "                pattern_name = event[0]\n",
    "                \n",
    "                # Strip PTRN| prefix if present\n",
    "                if pattern_name.startswith('PTRN|'):\n",
    "                    pattern_name = pattern_name[5:]  # Remove 'PTRN|' prefix\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"    Unraveling present pattern: {pattern_name[:16]}... (from level {level-1})\")\n",
    "                \n",
    "                # Recursively unravel this pattern to tokens\n",
    "                # Pattern is from level-1 (the child level that was observed)\n",
    "                # level-1 is safe because we only reach this branch if level > 0\n",
    "                tokens = unravel_pattern(\n",
    "                    pattern_name,\n",
    "                    level=level-1,\n",
    "                    nodes=nodes,\n",
    "                    verbose=verbose,\n",
    "                    indent=2 if verbose else 0\n",
    "                )\n",
    "                \n",
    "                if tokens:\n",
    "                    all_tokens.extend(tokens)\n",
    "                elif verbose:\n",
    "                    print(f\"      Warning: Failed to unravel pattern {pattern_name[:16]}...\")\n",
    "        \n",
    "        return all_tokens\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    input_text: str,\n",
    "    max_predictions: int = 5,\n",
    "    verbose: bool = True,\n",
    "    verbose_unravel: bool = False,\n",
    "    recall_threshold_overrides: Dict[str, Dict[str, Any]] = None,\n",
    "    auto_adjust_recall: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate text continuations using hierarchical KATO system.\n",
    "    \n",
    "    Complete pipeline:\n",
    "    1. Activate hierarchy with input text (bottom-up)\n",
    "    2. Get prediction ensemble from highest available level with non-empty 'future'\n",
    "    3. Extract BOTH 'present' (matched pattern) and 'future' (predicted next)\n",
    "    4. Unravel both present and future predictions to tokens (top-down)\n",
    "    5. Combine: present tokens + future tokens\n",
    "    6. Decode combined tokens to text\n",
    "    \n",
    "    Args:\n",
    "        input_text: User input to condition generation\n",
    "        max_predictions: Maximum number of predictions to generate\n",
    "        verbose: Print pipeline steps\n",
    "        verbose_unravel: Print detailed unraveling steps\n",
    "        recall_threshold_overrides: Optional per-node recall_threshold overrides\n",
    "            Format: {'node0': {'recall_threshold': 0.3}, 'node1': {'recall_threshold': 0.6}, ...}\n",
    "        auto_adjust_recall: If True, automatically lower recall_threshold if no predictions found\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts with keys:\n",
    "            - 'text': Generated text string\n",
    "            - 'potential': Potential metric (used for ranking)\n",
    "            - 'confidence': Confidence metric\n",
    "            - 'similarity': Similarity metric\n",
    "            - 'evidence': Evidence metric\n",
    "            - 'bayesian_posterior': Bayesian posterior probability\n",
    "            - 'bayesian_prior': Bayesian prior probability\n",
    "            - 'bayesian_likelihood': Bayesian likelihood\n",
    "            - 'predictive_information': Predictive information metric\n",
    "            - 'snr': Signal-to-noise ratio\n",
    "            - 'entropy': Pattern entropy\n",
    "            - 'normalized_entropy': Normalized entropy\n",
    "            - 'global_normalized_entropy': Globally normalized entropy\n",
    "            - 'frequency': Pattern occurrence frequency\n",
    "            - 'pattern_probability': Pattern probability\n",
    "            - 'weighted_strength': Weighted strength metric\n",
    "            - 'fragmentation': Pattern fragmentation\n",
    "            - 'itfdf_similarity': ITFDF similarity score\n",
    "        \n",
    "    Example:\n",
    "        >>> # Generate with custom recall thresholds\n",
    "        >>> results = generate_text(\n",
    "        ...     \"The cat sat\",\n",
    "        ...     recall_threshold_overrides={\n",
    "        ...         'node0': {'recall_threshold': 0.3},\n",
    "        ...         'node1': {'recall_threshold': 0.6}\n",
    "        ...     }\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    print(f\"\\nInput: {input_text}\")\n",
    "    \n",
    "    # Step 1: Activate hierarchy (bottom-up)\n",
    "    all_predictions = activate_hierarchy(\n",
    "        input_text,\n",
    "        verbose=verbose,\n",
    "        recall_threshold_overrides=recall_threshold_overrides\n",
    "    )\n",
    "    \n",
    "    # Step 2: Get prediction ensemble with non-empty 'future' (with fallback)\n",
    "    predictions, used_level = get_prediction_ensemble(\n",
    "        all_predictions,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Auto-adjust recall_threshold if no predictions found\n",
    "    if not predictions and auto_adjust_recall:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"AUTO-ADJUSTING RECALL_THRESHOLD\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"⚠ No predictions with usable 'future' data.\")\n",
    "        print(\"Trying progressively lower thresholds...\\n\")\n",
    "        \n",
    "        # Try decreasing thresholds\n",
    "        for threshold in [0.5, 0.4, 0.3, 0.2, 0.1]:\n",
    "            print(f\"Trying recall_threshold={threshold}...\")\n",
    "            \n",
    "            # Create override dict for all nodes\n",
    "            override = {\n",
    "                f'node{i}': {'recall_threshold': threshold}\n",
    "                for i in range(4)\n",
    "            }\n",
    "            \n",
    "            # Clear all STMs before re-activating\n",
    "            # IMPORTANT: Must clear after changing recall_threshold!\n",
    "            for node in nodes:\n",
    "                node.clear_stm()\n",
    "            \n",
    "            # Re-activate hierarchy with new threshold\n",
    "            all_predictions = activate_hierarchy(\n",
    "                input_text,\n",
    "                verbose=False,  # Suppress verbose for retry attempts\n",
    "                recall_threshold_overrides=override\n",
    "            )\n",
    "            \n",
    "            # Check if we got predictions with usable future\n",
    "            predictions, used_level = get_prediction_ensemble(\n",
    "                all_predictions,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            if predictions:\n",
    "                print(f\"✓ Found {len(predictions)} predictions with usable 'future' at recall_threshold={threshold}\\n\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "                if verbose:\n",
    "                    print(f\"✓ Using node{used_level} predictions ({len(predictions)} patterns)\")\n",
    "                break\n",
    "        \n",
    "        if not predictions:\n",
    "            print(\"⚠ No predictions with usable 'future' found even with minimum recall_threshold (0.1)\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    if not predictions:\n",
    "        print(\"\\n⚠ No predictions with usable 'future' field available.\")\n",
    "        print(\"   (Input may be novel, or predictions don't contain future data)\")\n",
    "        return []\n",
    "    \n",
    "    # Limit to max_predictions\n",
    "    predictions = predictions[:max_predictions]\n",
    "    \n",
    "    # Step 3: Unravel BOTH 'present' (matched pattern) and 'future' fields\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TOP-DOWN UNRAVELING (Present + Future)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\")\n",
    "    print(f\"** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        present_events = pred.get('present', [])  # Matched sequence (KATO events)\n",
    "        future_list = pred.get('future', [])  # Predicted next patterns/tokens\n",
    "        \n",
    "        # Extract ALL available metrics from the prediction (17 total)\n",
    "        metrics = {\n",
    "            # Core Quality Metrics\n",
    "            'potential': pred.get('potential', 0.0),\n",
    "            'confidence': pred.get('confidence', 0.0),\n",
    "            'similarity': pred.get('similarity', 0.0),\n",
    "            'evidence': pred.get('evidence', 0.0),\n",
    "            # Bayesian Metrics\n",
    "            'bayesian_posterior': pred.get('bayesian_posterior', 0.0),\n",
    "            'bayesian_prior': pred.get('bayesian_prior', 0.0),\n",
    "            'bayesian_likelihood': pred.get('bayesian_likelihood', 0.0),\n",
    "            # Information Theory Metrics\n",
    "            'predictive_information': pred.get('predictive_information', 0.0),\n",
    "            'snr': pred.get('snr', 0.0),\n",
    "            'entropy': pred.get('entropy', 0.0),\n",
    "            'normalized_entropy': pred.get('normalized_entropy', 0.0),\n",
    "            'global_normalized_entropy': pred.get('global_normalized_entropy', 0.0),\n",
    "            # Pattern Strength Metrics\n",
    "            'frequency': pred.get('frequency', 0),\n",
    "            'pattern_probability': pred.get('pattern_probability', 0.0),\n",
    "            'weighted_strength': pred.get('weighted_strength', 0.0),\n",
    "            'fragmentation': pred.get('fragmentation', 0.0),\n",
    "            'itfdf_similarity': pred.get('itfdf_similarity', 0.0),\n",
    "            'tfidf_score': pred.get('tfidf_score', 0.0),\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nPrediction {i+1}/{len(predictions)}:\")\n",
    "        print(f\"  Matched Pattern: {pred['name'][:60]}...\")\n",
    "        print(f\"  Level: node{used_level}\")\n",
    "        print(f\"  Present Events: {len(present_events)} tokens\")\n",
    "        print(f\"  Future Events: {len(future_list)} events\")\n",
    "        \n",
    "        # Display all metrics in grouped format\n",
    "        print(f\"\\n  Prediction Metrics:\")\n",
    "        print(f\"    --- Core Quality ---\")\n",
    "        print(f\"    Potential:                  {metrics['potential']:.4f}\")\n",
    "        print(f\"    Confidence:                 {metrics['confidence']:.4f}\")\n",
    "        print(f\"    Similarity:                 {metrics['similarity']:.4f}\")\n",
    "        print(f\"    Evidence:                   {metrics['evidence']:.4f}\")\n",
    "        print(f\"    --- Bayesian ---\")\n",
    "        print(f\"    Bayesian Posterior:         {metrics['bayesian_posterior']:.4f}\")\n",
    "        print(f\"    Bayesian Prior:             {metrics['bayesian_prior']:.4f}\")\n",
    "        print(f\"    Bayesian Likelihood:        {metrics['bayesian_likelihood']:.4f}\")\n",
    "        print(f\"    --- Information Theory ---\")\n",
    "        print(f\"    Predictive Information:     {metrics['predictive_information']:.4f}\")\n",
    "        print(f\"    SNR:                        {metrics['snr']:.4f}\")\n",
    "        print(f\"    Entropy:                    {metrics['entropy']:.4f}\")\n",
    "        print(f\"    Normalized Entropy:         {metrics['normalized_entropy']:.4f}\")\n",
    "        print(f\"    Global Normalized Entropy:  {metrics['global_normalized_entropy']:.4f}\")\n",
    "        print(f\"    --- Pattern Strength ---\")\n",
    "        print(f\"    Frequency:                  {metrics['frequency']}\")\n",
    "        print(f\"    Pattern Probability:        {metrics['pattern_probability']:.4f}\")\n",
    "        print(f\"    Weighted Strength:          {metrics['weighted_strength']:.4f}\")\n",
    "        print(f\"    Fragmentation:              {metrics['fragmentation']:.4f}\")\n",
    "        print(f\"    ITFDF Similarity:           {metrics['itfdf_similarity']:.4f}\")\n",
    "        print(f\"    TF-IDF Score:               {metrics['tfidf_score']:.4f}\")\n",
    "        \n",
    "        # First, extract the PRESENT tokens (unravel if from higher level)\n",
    "        if verbose_unravel:\n",
    "            print(f\"\\n  Extracting PRESENT tokens from pred['present'] field...\")\n",
    "        \n",
    "        present_tokens = extract_tokens_from_present(\n",
    "            present_events,\n",
    "            level=used_level,\n",
    "            nodes=nodes,\n",
    "            verbose=verbose_unravel\n",
    "        )\n",
    "        \n",
    "        if not present_tokens:\n",
    "            print(f\"  ⚠ Failed to unravel present pattern\")\n",
    "            # Even if present fails, try to get future\n",
    "        \n",
    "        # Then, unravel the FUTURE (predicted next patterns/tokens)\n",
    "        if not future_list:\n",
    "            print(f\"  ⚠ Empty 'future' field\")\n",
    "            if not present_tokens:\n",
    "                continue  # Skip if both present and future are empty\n",
    "            # If we have present but no future, just use present\n",
    "            tokens = present_tokens\n",
    "            future_tokens = []\n",
    "        else:\n",
    "            # Calculate future level\n",
    "            future_level = used_level - 1 if used_level > 0 else -1\n",
    "            \n",
    "            if verbose_unravel:\n",
    "                print(f\"\\n  Unraveling FUTURE (predicted next) from level {future_level}...\")\n",
    "            \n",
    "            future_tokens = unravel_future_list(\n",
    "                future_list,\n",
    "                future_level=future_level,\n",
    "                nodes=nodes,\n",
    "                verbose=verbose_unravel\n",
    "            )\n",
    "            \n",
    "            if not future_tokens:\n",
    "                print(f\"  ⚠ Failed to unravel future\")\n",
    "                # If future fails but we have present, use just present\n",
    "                if present_tokens:\n",
    "                    tokens = present_tokens\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                # Combine present + future tokens\n",
    "                tokens = present_tokens + future_tokens\n",
    "        \n",
    "        # Decode combined tokens to text\n",
    "        generated_text = tokenizer.decode_tokens(tokens)\n",
    "\n",
    "        _present = tokenizer.decode_tokens(present_tokens) if present_tokens else \"\"\n",
    "        _future = tokenizer.decode_tokens(future_tokens) if future_tokens else \"\"\n",
    "        \n",
    "        print(f\"\\n  ✓ Generated:\")\n",
    "        print(f\"     Present tokens: {len(present_tokens)}\")\n",
    "        print(f\"     Future tokens: {len(future_tokens)}\")\n",
    "        print(f\"     Total tokens: {len(tokens)}\")\n",
    "        print(f\"==\"*20)\n",
    "        print(f\"  Present: {_present}\")\n",
    "        print(f\"--\"*20)\n",
    "        print(f\"  Future: {_future}\")\n",
    "        print(f\"==\"*20)\n",
    "        print(f\"  Text preview: {generated_text}\")\n",
    "        \n",
    "        # Store result with all metrics\n",
    "        result_dict = {\n",
    "            'text': generated_text,\n",
    "            **metrics  # Unpack all metrics into the result dict\n",
    "        }\n",
    "        results.append(result_dict)\n",
    "    \n",
    "    # Sort by potential (primary ranking metric), fallback to confidence\n",
    "    results.sort(key=lambda x: x['potential'] if x['potential'] > 0 else x['confidence'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GENERATION COMPLETE: {len(results)} results\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ generate_text() function defined (with PRESENT + FUTURE + ALL 17 METRICS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input: The cat sat on the \n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input: The cat sat on the \n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (6): ['The', 'Ġcat', 'Ġsat', 'Ġon', 'Ġthe', 'Ġ']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['The', 'Ġcat', 'Ġsat', 'Ġon', 'Ġthe', 'Ġ']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['The', 'Ġcat', 'Ġsat', 'Ġon', 'Ġthe', 'Ġ']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 6 tokens\n",
      "  node0 STM: 6 events\n",
      "     STM: [['The'], ['Ġcat'], ['Ġsat'], ['Ġon'], ['Ġthe'], ['Ġ']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. e3ad92dd06caca4e0ef3156504e8400d64187c85... (conf: 1.000)\n",
      "    2. c0a6516eb75369b71dfd7b99e92bdac02a9d3068... (conf: 1.000)\n",
      "    3. a49a802d747f5bf5f89a69d96c7fb9661f2b7427... (conf: 0.750)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|e3ad92dd06caca4e0ef3156504e8400d64187c85...\n",
      "    2. PTRN|c0a6516eb75369b71dfd7b99e92bdac02a9d3068...\n",
      "    3. PTRN|a49a802d747f5bf5f89a69d96c7fb9661f2b7427...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|0777c81408410691ec117afe83dc6684807c811e', 'PTRN|0c4cf34a4563e3159c4b21a123114526c4c11b15', 'PTRN|1fab154113988c733cff48dbe20afa1158b2bf21', 'PTRN|68b45493038531306b2f0fd676a6cde7991d4520', 'PTRN|76d131abfb406b4f3f6b3bbec20a4c02b000dcd4', 'PTRN|8711197f25a034aaeb0e017f96a5654a77329dcf', 'PTRN|a49a802d747f5bf5f89a69d96c7fb9661f2b7427', 'PTRN|c0a6516eb75369b71dfd7b99e92bdac02a9d3068', 'PTRN|c80377b3fca20ddb86b9a2572d6fdf40f765f7ad', 'PTRN|e3ad92dd06caca4e0ef3156504e8400d64187c85']]\n",
      "✓ Got 9 predictions\n",
      "  Sample predictions:\n",
      "    1. 18afdfc92f87f6c3f277133dffe4e4dc71284bbc... (conf: 1.000)\n",
      "    2. 48ac3f14de33a4a2ddb3d438617c5c25a86ce64c... (conf: 1.000)\n",
      "    3. 609c26e28a5e9f56576ea8e121bc228c4ae8967f... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 9 pattern names as 1 event:\n",
      "    1. PTRN|18afdfc92f87f6c3f277133dffe4e4dc71284bbc...\n",
      "    2. PTRN|48ac3f14de33a4a2ddb3d438617c5c25a86ce64c...\n",
      "    3. PTRN|609c26e28a5e9f56576ea8e121bc228c4ae8967f...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|18afdfc92f87f6c3f277133dffe4e4dc71284bbc', 'PTRN|43f54faddfc8d2d0e833a6f012901a983224ff3a', 'PTRN|456e83051da80b631f7769001fa0c553074fb60c', 'PTRN|48ac3f14de33a4a2ddb3d438617c5c25a86ce64c', 'PTRN|609c26e28a5e9f56576ea8e121bc228c4ae8967f', 'PTRN|9a02b5231623fec88bdff1f504c01240514ce091', 'PTRN|a9bfed195b5f585ccb97411f881f78a3ee04bccd', 'PTRN|f07acbf202c15ab3811541b37db784084d1c76d8', 'PTRN|f9c925d3629238ae4ed762ebfd361c7895b16d4f']]\n",
      "✓ Got 6 predictions\n",
      "  Sample predictions:\n",
      "    1. 0edf85984303b5e7edf04a2719a7a9823d79f337... (conf: 1.000)\n",
      "    2. 45d47547e1910013a158fa210fd287f95f8a009b... (conf: 1.000)\n",
      "    3. fd46712761f6833c53015b2fe771bb56c2f9d58e... (conf: 1.000)\n",
      "\n",
      "--- NODE3 ---\n",
      "Sending 6 pattern names as 1 event:\n",
      "    1. PTRN|0edf85984303b5e7edf04a2719a7a9823d79f337...\n",
      "    2. PTRN|45d47547e1910013a158fa210fd287f95f8a009b...\n",
      "    3. PTRN|fd46712761f6833c53015b2fe771bb56c2f9d58e...\n",
      "✓ Observed (1 event)\n",
      "  node3 STM: 1 events\n",
      "     STM: [['PTRN|0edf85984303b5e7edf04a2719a7a9823d79f337', 'PTRN|45d47547e1910013a158fa210fd287f95f8a009b', 'PTRN|b92a754792be6b45dc670b113f4218d850d08e2d', 'PTRN|c89534e317a9dfea6c7609e75d9ec43fc3727d69', 'PTRN|f55829fb68dd8bbcc92ac4b9b65063cc86a7a3cf', 'PTRN|fd46712761f6833c53015b2fe771bb56c2f9d58e']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node3\n",
      "     STM has 1 event(s)\n",
      "     First event: ['PTRN|0edf85984303b5e7edf04a2719a7a9823d79f337', 'PTRN|45d47547e1910013a158fa210fd287f95f8a009b', 'PTRN|b92a754792be6b45dc670b113f4218d850d08e2d']... (6 symbols)\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 10 predictions\n",
      "  node1: 9 predictions\n",
      "  node2: 6 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "✓ Using node2 predictions (1 patterns with usable future data)\n",
      "  (Filtered out 5 predictions with empty 'future' field)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/1:\n",
      "  Matched Pattern: fd46712761f6833c53015b2fe771bb56c2f9d58e...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  1.1014\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.1667\n",
      "    Evidence:                   0.3333\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         0.1667\n",
      "    Bayesian Prior:             0.1667\n",
      "    Bayesian Likelihood:        0.1667\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.0278\n",
      "    SNR:                        -0.6000\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.1896\n",
      "    Global Normalized Entropy:  0.0001\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        0.1667\n",
      "    Weighted Strength:          0.0278\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           0.9014\n",
      "    TF-IDF Score:               16.1554\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 36\n",
      "     Total tokens: 100\n",
      "========================================\n",
      "  Present:  Bomar sat on the bench for the rest of the season 's games . Known as a devastating blocker and \" lightning fast , \" he was the first Commodore football player elected to the College Football Hall of Fame in 1956 . At his induction Bomar said , \" I just wish all the men who played with me at\n",
      "----------------------------------------\n",
      "  Future:  Vanderbilt between 1921 and 1924 could also receive this coveted award . They deserve it more than I do . After all , they made it possible for me to be chosen . \" \n",
      "\n",
      "========================================\n",
      "  Text preview:  Bomar sat on the bench for the rest of the season 's games . Known as a devastating blocker and \" lightning fast , \" he was the first Commodore football player elected to the College Football Hall of Fame in 1956 . At his induction Bomar said , \" I just wish all the men who played with me at Vanderbilt between 1921 and 1924 could also receive this coveted award . They deserve it more than I do . After all , they made it possible for me to be chosen . \" \n",
      "\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 1 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  1.1014\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.1667\n",
      "    Evidence:                   0.3333\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         0.1667\n",
      "    Bayesian Prior:             0.1667\n",
      "    Bayesian Likelihood:        0.1667\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.0278\n",
      "    SNR:                        -0.6000\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.1896\n",
      "    Global Normalized Entropy:  0.0001\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        0.1667\n",
      "    Weighted Strength:          0.0278\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           0.9014\n",
      "    TF-IDF Score:               16.1554\n",
      "  Text:  Bomar sat on the bench for the rest of the season 's games . Known as a devastating blocker and \" lightning fast , \" he was the first Commodore football player elected to the College Football Hall of Fame in 1956 . At his induction Bomar said , \" I just wish all the men who played with me at Vanderbilt between 1921 and 1924 could also receive this coveted award . They deserve it more than I do . After all , they made it possible for me to be chosen . \" \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple short input\n",
    "input_text = \"The cat sat on the \"\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    # max_predictions=100,\n",
    "    verbose=True,\n",
    "    verbose_unravel=False  # Set True to see detailed unraveling\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    --- Core Quality ---\")\n",
    "    print(f\"    Potential:                  {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:                 {result['confidence']:.4f}\")\n",
    "    print(f\"    Similarity:                 {result['similarity']:.4f}\")\n",
    "    print(f\"    Evidence:                   {result['evidence']:.4f}\")\n",
    "    print(f\"    --- Bayesian ---\")\n",
    "    print(f\"    Bayesian Posterior:         {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Prior:             {result['bayesian_prior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:        {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    --- Information Theory ---\")\n",
    "    print(f\"    Predictive Information:     {result['predictive_information']:.4f}\")\n",
    "    print(f\"    SNR:                        {result['snr']:.4f}\")\n",
    "    print(f\"    Entropy:                    {result['entropy']:.4f}\")\n",
    "    print(f\"    Normalized Entropy:         {result['normalized_entropy']:.4f}\")\n",
    "    print(f\"    Global Normalized Entropy:  {result['global_normalized_entropy']:.4f}\")\n",
    "    print(f\"    --- Pattern Strength ---\")\n",
    "    print(f\"    Frequency:                  {result['frequency']}\")\n",
    "    print(f\"    Pattern Probability:        {result['pattern_probability']:.4f}\")\n",
    "    print(f\"    Weighted Strength:          {result['weighted_strength']:.4f}\")\n",
    "    print(f\"    Fragmentation:              {result['fragmentation']:.4f}\")\n",
    "    print(f\"    ITFDF Similarity:           {result['itfdf_similarity']:.4f}\")\n",
    "    print(f\"    TF-IDF Score:               {result['tfidf_score']:.4f}\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Longer Input (More Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input: Machine learning is a field of artificial intelligence that\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input: Machine learning is a field of artificial intelligence that\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (9): ['Machine', 'Ġlearning', 'Ġis', 'Ġa', 'Ġfield', 'Ġof', 'Ġartificial', 'Ġintelligence', 'Ġthat']\n",
      "\n",
      "Chunks (2) with chunk_size=8:\n",
      "  Chunk 0: ['Machine', 'Ġlearning', 'Ġis', 'Ġa', 'Ġfield', 'Ġof', 'Ġartificial', 'Ġintelligence']\n",
      "  Chunk 1: ['Ġthat']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/2: ['Machine', 'Ġlearning', 'Ġis', 'Ġa', 'Ġfield', 'Ġof', 'Ġartificial', 'Ġintelligence']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 8 tokens\n",
      "  node0 STM: 8 events\n",
      "     STM: [['Machine'], ['Ġlearning'], ['Ġis'], ['Ġa'], ['Ġfield'], ['Ġof'], ['Ġartificial'], ['Ġintelligence']]\n",
      "✓ Got 2 predictions\n",
      "  Sample predictions:\n",
      "    1. a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f... (conf: 1.000)\n",
      "    2. f9b44c1ec1d7e8f5d9107720072a54c37f768aea... (conf: 1.000)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 2 pattern names as 1 event:\n",
      "    1. PTRN|a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f...\n",
      "    2. PTRN|f9b44c1ec1d7e8f5d9107720072a54c37f768aea...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f', 'PTRN|f9b44c1ec1d7e8f5d9107720072a54c37f768aea']]\n",
      "✓ Got 2 predictions\n",
      "  Sample predictions:\n",
      "    1. 7ada9600ca3d3abd5b4fda177bd7791cb7f17ce1... (conf: 1.000)\n",
      "    2. a134a2f225bb8d031deedeb8e88ef8d13c869f48... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 2 pattern names as 1 event:\n",
      "    1. PTRN|7ada9600ca3d3abd5b4fda177bd7791cb7f17ce1...\n",
      "    2. PTRN|a134a2f225bb8d031deedeb8e88ef8d13c869f48...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|7ada9600ca3d3abd5b4fda177bd7791cb7f17ce1', 'PTRN|a134a2f225bb8d031deedeb8e88ef8d13c869f48']]\n",
      "✓ Got 1 predictions\n",
      "  Sample predictions:\n",
      "    1. 1af413a8bf69575568106c530fd59dffb25b918a... (conf: 1.000)\n",
      "\n",
      "--- NODE3 ---\n",
      "Sending 1 pattern names as 1 event:\n",
      "    1. PTRN|1af413a8bf69575568106c530fd59dffb25b918a...\n",
      "✓ Observed (1 event)\n",
      "  node3 STM: 1 events\n",
      "     STM: [['PTRN|1af413a8bf69575568106c530fd59dffb25b918a']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node3\n",
      "     STM has 1 event(s)\n",
      "     First event: ['PTRN|1af413a8bf69575568106c530fd59dffb25b918a']... (1 symbols)\n",
      "\n",
      "================================================================================\n",
      "CHUNK 2/2: ['Ġthat']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 1 tokens\n",
      "  node0 STM: 1 events\n",
      "     STM: [['Ġthat']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node0\n",
      "     Possible reasons:\n",
      "       - No patterns in KB match this token sequence\n",
      "       - recall_threshold too high (current: 0.6)\n",
      "⚠ No predictions from node0 - stopping cascade for this chunk\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 0 predictions\n",
      "  node1: 2 predictions\n",
      "  node2: 1 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "✓ Using node2 predictions (1 patterns with usable future data)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/1:\n",
      "  Matched Pattern: 1af413a8bf69575568106c530fd59dffb25b918a...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  2.4444\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.4000\n",
      "    Evidence:                   0.3333\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         1.0000\n",
      "    Bayesian Prior:             1.0000\n",
      "    Bayesian Likelihood:        0.4000\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.4000\n",
      "    SNR:                        0.3333\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.0598\n",
      "    Global Normalized Entropy:  0.0000\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        1.0000\n",
      "    Weighted Strength:          0.4000\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           1.0000\n",
      "    TF-IDF Score:               16.1554\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 35\n",
      "     Total tokens: 99\n",
      "========================================\n",
      "  Present:  characteristic of the field . Suppose that F is a field of characteristic p , and consider the function <formula> that raises each element of F to the power p . This is called the Frobenius automorphism of F. It is an automorphism of the field because of the Freshman 's dream\n",
      "----------------------------------------\n",
      "  Future:  identity <formula> . The Frobenius automorphism is important in number theory because it generates the Galois group of F over its prime subfield . \n",
      "\n",
      "========================================\n",
      "  Text preview:  characteristic of the field . Suppose that F is a field of characteristic p , and consider the function <formula> that raises each element of F to the power p . This is called the Frobenius automorphism of F. It is an automorphism of the field because of the Freshman 's dream identity <formula> . The Frobenius automorphism is important in number theory because it generates the Galois group of F over its prime subfield . \n",
      "\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 1 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  2.4444\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.4000\n",
      "    Evidence:                   0.3333\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         1.0000\n",
      "    Bayesian Prior:             1.0000\n",
      "    Bayesian Likelihood:        0.4000\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.4000\n",
      "    SNR:                        0.3333\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.0598\n",
      "    Global Normalized Entropy:  0.0000\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        1.0000\n",
      "    Weighted Strength:          0.4000\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           1.0000\n",
      "    TF-IDF Score:               16.1554\n",
      "  Text:  characteristic of the field . Suppose that F is a field of characteristic p , and consider the function <formula> that raises each element of F to the power p . This is called the Frobenius automorphism of F. It is an automorphism of the field because of the Freshman 's dream identity <formula> . The Frobenius automorphism is important in number theory because it generates the Galois group of F over its prime subfield . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Longer input with more context\n",
    "input_text = \"Machine learning is a field of artificial intelligence that\"\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    # max_predictions=3,\n",
    "    verbose=True,\n",
    "    verbose_unravel=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    --- Core Quality ---\")\n",
    "    print(f\"    Potential:                  {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:                 {result['confidence']:.4f}\")\n",
    "    print(f\"    Similarity:                 {result['similarity']:.4f}\")\n",
    "    print(f\"    Evidence:                   {result['evidence']:.4f}\")\n",
    "    print(f\"    --- Bayesian ---\")\n",
    "    print(f\"    Bayesian Posterior:         {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Prior:             {result['bayesian_prior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:        {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    --- Information Theory ---\")\n",
    "    print(f\"    Predictive Information:     {result['predictive_information']:.4f}\")\n",
    "    print(f\"    SNR:                        {result['snr']:.4f}\")\n",
    "    print(f\"    Entropy:                    {result['entropy']:.4f}\")\n",
    "    print(f\"    Normalized Entropy:         {result['normalized_entropy']:.4f}\")\n",
    "    print(f\"    Global Normalized Entropy:  {result['global_normalized_entropy']:.4f}\")\n",
    "    print(f\"    --- Pattern Strength ---\")\n",
    "    print(f\"    Frequency:                  {result['frequency']}\")\n",
    "    print(f\"    Pattern Probability:        {result['pattern_probability']:.4f}\")\n",
    "    print(f\"    Weighted Strength:          {result['weighted_strength']:.4f}\")\n",
    "    print(f\"    Fragmentation:              {result['fragmentation']:.4f}\")\n",
    "    print(f\"    ITFDF Similarity:           {result['itfdf_similarity']:.4f}\")\n",
    "    print(f\"    TF-IDF Score:               {result['tfidf_score']:.4f}\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [],\n",
       " 'future_potentials': [{'future': [['Ġcharacteristic'],\n",
       "    ['Ġp'],\n",
       "    ['Ġ,'],\n",
       "    ['Ġand']],\n",
       "   'aggregate_potential': 0.5,\n",
       "   'supporting_patterns': 1,\n",
       "   'total_weighted_frequency': 0.5},\n",
       "  {'future': [['Ġresearch'], ['Ġwas'], ['Ġfounded']],\n",
       "   'aggregate_potential': 0.5,\n",
       "   'supporting_patterns': 1,\n",
       "   'total_weighted_frequency': 0.5}],\n",
       " 'session_id': 'session-6e6d2f9dcb794b05898de0f2d69c3b08-1770237360667',\n",
       " 'processor_id': None,\n",
       " 'count': 0,\n",
       " 'time': None,\n",
       " 'unique_id': None}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'type': 'prototypical',\n",
       "   'name': 'a134a2f225bb8d031deedeb8e88ef8d13c869f48',\n",
       "   'frequency': 1,\n",
       "   'emotives': {},\n",
       "   'matches': ['PTRN|a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f'],\n",
       "   'past': [['PTRN|feb15b241126e6dd3cfe6c11d36df820707f9320']],\n",
       "   'present': [['PTRN|a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f']],\n",
       "   'missing': [[]],\n",
       "   'extras': [['PTRN|f9b44c1ec1d7e8f5d9107720072a54c37f768aea']],\n",
       "   'anomalies': [],\n",
       "   'potential': 2.228553390593274,\n",
       "   'evidence': 0.125,\n",
       "   'similarity': 0.2,\n",
       "   'fragmentation': 0.0,\n",
       "   'snr': 0.3333333333333333,\n",
       "   'confluence': 7.901827692251944e-06,\n",
       "   'predictive_information': 0.2,\n",
       "   'sequence': [['PTRN|feb15b241126e6dd3cfe6c11d36df820707f9320'],\n",
       "    ['PTRN|a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f'],\n",
       "    ['PTRN|2e86b0e051a29ea19afb5ffac8c868eab98025af'],\n",
       "    ['PTRN|78f6657d7f60378ad76a2ebb2b504548da644d9d'],\n",
       "    ['PTRN|641e381be6d670a8b75c2e56859d2d9a27c3d08d'],\n",
       "    ['PTRN|c60364fef3ab841fa6344b885f74fe3ce019da02'],\n",
       "    ['PTRN|e85dce73889872ddbd5a3d6776c1629d83eff322'],\n",
       "    ['PTRN|a57fc6dac8af32e73c30baba135353ba0df81092']],\n",
       "   'future': [['PTRN|2e86b0e051a29ea19afb5ffac8c868eab98025af'],\n",
       "    ['PTRN|78f6657d7f60378ad76a2ebb2b504548da644d9d'],\n",
       "    ['PTRN|641e381be6d670a8b75c2e56859d2d9a27c3d08d'],\n",
       "    ['PTRN|c60364fef3ab841fa6344b885f74fe3ce019da02'],\n",
       "    ['PTRN|e85dce73889872ddbd5a3d6776c1629d83eff322'],\n",
       "    ['PTRN|a57fc6dac8af32e73c30baba135353ba0df81092']],\n",
       "   'confidence': 1.0,\n",
       "   'entropy': 0.0,\n",
       "   'normalized_entropy': 0.051003006920242,\n",
       "   'global_normalized_entropy': 1.366187615040844e-05,\n",
       "   'itfdf_similarity': 0.8535533905932737,\n",
       "   'tfidf_score': 17.94937078210099,\n",
       "   'pattern_probability': 0.5,\n",
       "   'weighted_strength': 0.1,\n",
       "   'bayesian_posterior': 0.5,\n",
       "   'bayesian_prior': 0.5,\n",
       "   'bayesian_likelihood': 0.2},\n",
       "  {'type': 'prototypical',\n",
       "   'name': '7ada9600ca3d3abd5b4fda177bd7791cb7f17ce1',\n",
       "   'frequency': 1,\n",
       "   'emotives': {},\n",
       "   'matches': ['PTRN|f9b44c1ec1d7e8f5d9107720072a54c37f768aea'],\n",
       "   'past': [['PTRN|6d50ebc55c191bedb316d5171585792893b19b77'],\n",
       "    ['PTRN|d927c28b81ec17fa42d93571811087979fba64c7'],\n",
       "    ['PTRN|74deafdf566f7551bf40b443af287dd321814614'],\n",
       "    ['PTRN|215e7f4b2a85d6538c7544c9714e82348b325f01'],\n",
       "    ['PTRN|949acf41afa2310f0ec3672b42c9735a1f63e980']],\n",
       "   'present': [['PTRN|f9b44c1ec1d7e8f5d9107720072a54c37f768aea']],\n",
       "   'missing': [[]],\n",
       "   'extras': [['PTRN|a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f']],\n",
       "   'anomalies': [],\n",
       "   'potential': 2.228553390593274,\n",
       "   'evidence': 0.125,\n",
       "   'similarity': 0.2,\n",
       "   'fragmentation': 0.0,\n",
       "   'snr': 0.3333333333333333,\n",
       "   'confluence': 7.901827692251944e-06,\n",
       "   'predictive_information': 0.2,\n",
       "   'sequence': [['PTRN|6d50ebc55c191bedb316d5171585792893b19b77'],\n",
       "    ['PTRN|d927c28b81ec17fa42d93571811087979fba64c7'],\n",
       "    ['PTRN|74deafdf566f7551bf40b443af287dd321814614'],\n",
       "    ['PTRN|215e7f4b2a85d6538c7544c9714e82348b325f01'],\n",
       "    ['PTRN|949acf41afa2310f0ec3672b42c9735a1f63e980'],\n",
       "    ['PTRN|f9b44c1ec1d7e8f5d9107720072a54c37f768aea'],\n",
       "    ['PTRN|16b00f0c428707c22b5a5c624125547d06081d8e'],\n",
       "    ['UNKNOWN']],\n",
       "   'future': [['PTRN|16b00f0c428707c22b5a5c624125547d06081d8e'], ['UNKNOWN']],\n",
       "   'confidence': 1.0,\n",
       "   'entropy': 0.0,\n",
       "   'normalized_entropy': 0.051003006920242,\n",
       "   'global_normalized_entropy': 1.366187615040844e-05,\n",
       "   'itfdf_similarity': 0.8535533905932737,\n",
       "   'tfidf_score': 17.94937078210099,\n",
       "   'pattern_probability': 0.5,\n",
       "   'weighted_strength': 0.1,\n",
       "   'bayesian_posterior': 0.5,\n",
       "   'bayesian_prior': 0.5,\n",
       "   'bayesian_likelihood': 0.2}],\n",
       " 'future_potentials': [{'future': [['PTRN|2e86b0e051a29ea19afb5ffac8c868eab98025af'],\n",
       "    ['PTRN|78f6657d7f60378ad76a2ebb2b504548da644d9d'],\n",
       "    ['PTRN|641e381be6d670a8b75c2e56859d2d9a27c3d08d'],\n",
       "    ['PTRN|c60364fef3ab841fa6344b885f74fe3ce019da02'],\n",
       "    ['PTRN|e85dce73889872ddbd5a3d6776c1629d83eff322'],\n",
       "    ['PTRN|a57fc6dac8af32e73c30baba135353ba0df81092']],\n",
       "   'aggregate_potential': 0.5,\n",
       "   'supporting_patterns': 1,\n",
       "   'total_weighted_frequency': 0.2},\n",
       "  {'future': [['PTRN|16b00f0c428707c22b5a5c624125547d06081d8e'], ['UNKNOWN']],\n",
       "   'aggregate_potential': 0.5,\n",
       "   'supporting_patterns': 1,\n",
       "   'total_weighted_frequency': 0.2}],\n",
       " 'session_id': 'session-3a642439c41f4a20950fc3cb2bc42ced-1770237360759',\n",
       " 'processor_id': None,\n",
       " 'count': 2,\n",
       " 'time': None,\n",
       " 'unique_id': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node1.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'type': 'prototypical',\n",
       "   'name': '1af413a8bf69575568106c530fd59dffb25b918a',\n",
       "   'frequency': 1,\n",
       "   'emotives': {},\n",
       "   'matches': ['PTRN|a134a2f225bb8d031deedeb8e88ef8d13c869f48'],\n",
       "   'past': [['PTRN|126885a0e82cb47df4fe488b35adc98c944d49ee']],\n",
       "   'present': [['PTRN|a134a2f225bb8d031deedeb8e88ef8d13c869f48']],\n",
       "   'missing': [[]],\n",
       "   'extras': [['PTRN|7ada9600ca3d3abd5b4fda177bd7791cb7f17ce1']],\n",
       "   'anomalies': [],\n",
       "   'potential': 2.4444444444444446,\n",
       "   'evidence': 0.3333333333333333,\n",
       "   'similarity': 0.4,\n",
       "   'fragmentation': 0.0,\n",
       "   'snr': 0.3333333333333333,\n",
       "   'confluence': 2.740101381693775e-05,\n",
       "   'predictive_information': 0.4,\n",
       "   'sequence': [['PTRN|126885a0e82cb47df4fe488b35adc98c944d49ee'],\n",
       "    ['PTRN|a134a2f225bb8d031deedeb8e88ef8d13c869f48'],\n",
       "    ['PTRN|f9817e5c05758fc1020577c074544c09aaebbb89']],\n",
       "   'future': [['PTRN|f9817e5c05758fc1020577c074544c09aaebbb89']],\n",
       "   'confidence': 1.0,\n",
       "   'entropy': 0.0,\n",
       "   'normalized_entropy': 0.059801911435595254,\n",
       "   'global_normalized_entropy': 2.4834772683318922e-05,\n",
       "   'itfdf_similarity': 1.0,\n",
       "   'tfidf_score': 16.15537166869856,\n",
       "   'pattern_probability': 1.0,\n",
       "   'weighted_strength': 0.4,\n",
       "   'bayesian_posterior': 1.0,\n",
       "   'bayesian_prior': 1.0,\n",
       "   'bayesian_likelihood': 0.4}],\n",
       " 'future_potentials': [{'future': [['PTRN|f9817e5c05758fc1020577c074544c09aaebbb89']],\n",
       "   'aggregate_potential': 1.0,\n",
       "   'supporting_patterns': 1,\n",
       "   'total_weighted_frequency': 0.4}],\n",
       " 'session_id': 'session-38a5af6703784edc95aa8882512ecd59-1770237360836',\n",
       " 'processor_id': None,\n",
       " 'count': 1,\n",
       " 'time': None,\n",
       " 'unique_id': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node2.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [],\n",
       " 'future_potentials': [],\n",
       " 'session_id': 'session-6b0e7eea03cf43548464ed6166d13a7c-1770237360864',\n",
       " 'processor_id': None,\n",
       " 'count': 0,\n",
       " 'time': None,\n",
       " 'unique_id': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node3.get_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Custom Input (Try Your Own!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input: The researchers found that\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input: The researchers found that\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (4): ['The', 'Ġresearchers', 'Ġfound', 'Ġthat']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['The', 'Ġresearchers', 'Ġfound', 'Ġthat']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['The', 'Ġresearchers', 'Ġfound', 'Ġthat']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 4 tokens\n",
      "  node0 STM: 4 events\n",
      "     STM: [['The'], ['Ġresearchers'], ['Ġfound'], ['Ġthat']]\n",
      "✓ Got 2 predictions\n",
      "  Sample predictions:\n",
      "    1. 1967e8489f2c5e298c4006662da3cbe8709bf715... (conf: 1.000)\n",
      "    2. 43d30465d6248c07718f645080cb15201d29d84b... (conf: 1.000)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 2 pattern names as 1 event:\n",
      "    1. PTRN|1967e8489f2c5e298c4006662da3cbe8709bf715...\n",
      "    2. PTRN|43d30465d6248c07718f645080cb15201d29d84b...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|1967e8489f2c5e298c4006662da3cbe8709bf715', 'PTRN|43d30465d6248c07718f645080cb15201d29d84b']]\n",
      "✓ Got 2 predictions\n",
      "  Sample predictions:\n",
      "    1. 092dabe0fb4aec9124b21e23514b7d3dab7e197c... (conf: 1.000)\n",
      "    2. 3977e9cd8b4c6609c403337dbf1ef2a57bdf89c3... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 2 pattern names as 1 event:\n",
      "    1. PTRN|092dabe0fb4aec9124b21e23514b7d3dab7e197c...\n",
      "    2. PTRN|3977e9cd8b4c6609c403337dbf1ef2a57bdf89c3...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|092dabe0fb4aec9124b21e23514b7d3dab7e197c', 'PTRN|3977e9cd8b4c6609c403337dbf1ef2a57bdf89c3']]\n",
      "✓ Got 2 predictions\n",
      "  Sample predictions:\n",
      "    1. 164d6ddf8f0f0ca792eef5c9e239b04d10eedfbb... (conf: 1.000)\n",
      "    2. bb5d51788cd45acaf774ad89bd7a636bc0a9d76b... (conf: 1.000)\n",
      "\n",
      "--- NODE3 ---\n",
      "Sending 2 pattern names as 1 event:\n",
      "    1. PTRN|164d6ddf8f0f0ca792eef5c9e239b04d10eedfbb...\n",
      "    2. PTRN|bb5d51788cd45acaf774ad89bd7a636bc0a9d76b...\n",
      "✓ Observed (1 event)\n",
      "  node3 STM: 1 events\n",
      "     STM: [['PTRN|164d6ddf8f0f0ca792eef5c9e239b04d10eedfbb', 'PTRN|bb5d51788cd45acaf774ad89bd7a636bc0a9d76b']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node3\n",
      "     STM has 1 event(s)\n",
      "     First event: ['PTRN|164d6ddf8f0f0ca792eef5c9e239b04d10eedfbb', 'PTRN|bb5d51788cd45acaf774ad89bd7a636bc0a9d76b']... (2 symbols)\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 2 predictions\n",
      "  node1: 2 predictions\n",
      "  node2: 2 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "✓ Using node2 predictions (2 patterns with usable future data)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/2:\n",
      "  Matched Pattern: 164d6ddf8f0f0ca792eef5c9e239b04d10eedfbb...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  2.2980\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.4000\n",
      "    Evidence:                   0.3333\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         0.5455\n",
      "    Bayesian Prior:             0.5000\n",
      "    Bayesian Likelihood:        0.4000\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.3667\n",
      "    SNR:                        0.3333\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.0598\n",
      "    Global Normalized Entropy:  0.0000\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        0.5000\n",
      "    Weighted Strength:          0.2000\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           0.8536\n",
      "    TF-IDF Score:               16.1554\n",
      "\n",
      "  Extracting PRESENT tokens from pred['present'] field...\n",
      "    Unraveling present pattern: 3977e9cd8b4c6609... (from level 1)\n",
      "    Unraveling node1: 3977e9cd8b4c6609c403337dbf1ef2a57bdf89c3...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 27b97179cbdb738c9d9b8598a1d6e9848402e4fe...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠJuly', 'Ġ1969', 'Ġ.', 'ĠThe', 'Ġresults', 'Ġwere', 'Ġ\"', 'Ġgenerally']... → ' July 1969 . The results were \" generally...'\n",
      "      Unraveling node0: 21dd626c5f36c4ec1982af60757431e82516b0c1...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġvery', 'Ġpositive', 'Ġ\"', 'Ġ;', 'Ġchildren', 'Ġlearned', 'Ġfrom', 'Ġthe']... → ' very positive \" ; children learned from the...'\n",
      "      Unraveling node0: abbd23eb9406d6b2e2805e1b5894ebc0fc2eeb3c...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġshows', 'Ġ,', 'Ġtheir', 'Ġappeal', 'Ġwas', 'Ġhigh', 'Ġ,', 'Ġand']... → ' shows , their appeal was high , and...'\n",
      "      Unraveling node0: 41dc7326a8d6c279e55cc22ccef0de1c8edc6598...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġtheir', 'Ġattention', 'Ġwas', 'Ġsustained', 'Ġover', 'Ġthe', 'Ġfull', 'Ġhour']... → ' their attention was sustained over the full hour...'\n",
      "      Unraveling node0: 1967e8489f2c5e298c4006662da3cbe8709bf715...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ.', 'ĠHowever', 'Ġ,', 'Ġthe', 'Ġresearchers', 'Ġfound', 'Ġthat', 'Ġalthough']... → ' . However , the researchers found that although...'\n",
      "      Unraveling node0: f269828abd48ac348635e46dd3582a0553e59ef7...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġchildren', \"Ġ'\", 's', 'Ġattention', 'Ġwas', 'Ġhigh', 'Ġduring', 'Ġthe']... → ' children 's attention was high during the...'\n",
      "      Unraveling node0: 6d2ecb24acc873629f7b1b43e8fae68e530d9eb3...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠM', 'uppet', 'Ġsegments', 'Ġ,', 'Ġtheir', 'Ġinterest', 'Ġwave', 'red']... → ' Muppet segments , their interest wavered...'\n",
      "      Unraveling node0: 43d2877ee8558720342e8cc4e95cd1ad55f6b141...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġduring', 'Ġthe', 'Ġ\"', 'ĠStreet', 'Ġ\"', 'Ġsegments', 'Ġ,', 'Ġwhen']... → ' during the \" Street \" segments , when...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "\n",
      "  Unraveling FUTURE (predicted next) from level 1...\n",
      "  Unraveling 1 future events from level 1\n",
      "  Unraveling future pattern: 434c5d891165aa8c...\n",
      "    Unraveling node1: PTRN|434c5d891165aa8c2070e7fa98dc75448f2894a7...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: d8fec54ec7eefa9258132de15edbf26be34cf856...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġno', 'ĠM', 'upp', 'ets', 'Ġwere', 'Ġon', 'Ġscreen', 'Ġ.']... → ' no Muppets were on screen ....'\n",
      "      Unraveling node0: b7e610b045ec6ab0836e606a59f7f0eb7e8b33e6...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠThis', 'Ġwas', 'Ġbecause', 'Ġthe', 'Ġproducers', 'Ġhad', 'Ġfollowed', 'Ġthe']... → ' This was because the producers had followed the...'\n",
      "      Unraveling node0: 35912ecd88d47a76383bcd2b88a104c3680b8ae8...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġadvice', 'Ġof', 'Ġchild', 'Ġpsychologists', 'Ġwho', 'Ġwere', 'Ġconcerned', 'Ġthat']... → ' advice of child psychologists who were concerned ...'\n",
      "      Unraveling node0: 2fd86eae4f45fa84d728c383096cc188c2634d82...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġchildren', 'Ġwould', 'Ġbe', 'Ġconfused', 'Ġif', 'Ġhuman', 'Ġactors', 'Ġand']... → ' children would be confused if human actors and...'\n",
      "      Unraveling node0: 5c53546f9915bcc841213c41dbc9e270a38b18b8...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠM', 'upp', 'ets', 'Ġwere', 'Ġshown', 'Ġtogether', 'Ġ.', 'ĠAs']... → ' Muppets were shown together . As...'\n",
      "      Unraveling node0: 5d0562b6d2210cf9b552aaf71880ed154ddd7a25...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġa', 'Ġresult', 'Ġof', 'Ġthis', 'Ġdecision', 'Ġ,', 'Ġthe', 'Ġappeal']... → ' a result of this decision , the appeal...'\n",
      "      Unraveling node0: 79a7a2b3623b231f18933c2697e19088250ba3d1...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġof', 'Ġthe', 'Ġtest', 'Ġepisodes', 'Ġwas', 'Ġlower', 'Ġthan', 'Ġthe']... → ' of the test episodes was lower than the...'\n",
      "      Unraveling node0: 0fa35ce29c30052596bb7c8e688916927d066e02...\n",
      "        Pattern has 4 events\n",
      "        → Tokens: ['Ġtarget', 'Ġ.', 'Ġ', 'Ċ']... → ' target . \n",
      "...'\n",
      "      ✓ Unraveled to 60 total tokens\n",
      "  Total future tokens: 60\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 60\n",
      "     Total tokens: 124\n",
      "========================================\n",
      "  Present:  July 1969 . The results were \" generally very positive \" ; children learned from the shows , their appeal was high , and their attention was sustained over the full hour . However , the researchers found that although children 's attention was high during the Muppet segments , their interest wavered during the \" Street \" segments , when\n",
      "----------------------------------------\n",
      "  Future:  no Muppets were on screen . This was because the producers had followed the advice of child psychologists who were concerned that children would be confused if human actors and Muppets were shown together . As a result of this decision , the appeal of the test episodes was lower than the target . \n",
      "\n",
      "========================================\n",
      "  Text preview:  July 1969 . The results were \" generally very positive \" ; children learned from the shows , their appeal was high , and their attention was sustained over the full hour . However , the researchers found that although children 's attention was high during the Muppet segments , their interest wavered during the \" Street \" segments , when no Muppets were on screen . This was because the producers had followed the advice of child psychologists who were concerned that children would be confused if human actors and Muppets were shown together . As a result of this decision , the appeal of the test episodes was lower than the target . \n",
      "\n",
      "\n",
      "Prediction 2/2:\n",
      "  Matched Pattern: bb5d51788cd45acaf774ad89bd7a636bc0a9d76b...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  2.2702\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.3333\n",
      "    Evidence:                   0.2500\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         0.4545\n",
      "    Bayesian Prior:             0.5000\n",
      "    Bayesian Likelihood:        0.3333\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.3667\n",
      "    SNR:                        0.3333\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.0598\n",
      "    Global Normalized Entropy:  0.0000\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        0.5000\n",
      "    Weighted Strength:          0.1667\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           0.8536\n",
      "    TF-IDF Score:               16.1554\n",
      "\n",
      "  Extracting PRESENT tokens from pred['present'] field...\n",
      "    Unraveling present pattern: 092dabe0fb4aec91... (from level 1)\n",
      "    Unraveling node1: 092dabe0fb4aec9124b21e23514b7d3dab7e197c...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 30c9d50a761da15f781df4ef02262eca6b7be62d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġformat', 'Ġwas', 'Ġnot', 'Ġthe', 'Ġbest', 'Ġway', 'Ġto', 'Ġattract']... → ' format was not the best way to attract...'\n",
      "      Unraveling node0: 9578a3b865d8b27a6b45733c1a13ef7e1c6db7d1...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġyoung', 'Ġchildren', \"Ġ'\", 's', 'Ġattention', 'Ġ.', 'ĠThe', 'Ġgrowth']... → ' young children 's attention . The growth...'\n",
      "      Unraveling node0: b350452057e078af6b57ec5590adb9ad5901192d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġof', 'Ġhome', 'Ġvideos', 'Ġduring', 'Ġthe', \"Ġ'\", 'Ġ80', 's']... → ' of home videos during the ' 80s...'\n",
      "      Unraveling node0: 99f4db378561cd7505e4b2d90543b4fd780bd361...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġand', 'Ġthe', 'Ġincrease', 'Ġof', 'Ġthirty', 'Ġ@', '-', '@']... → ' and the increase of thirty @-@...'\n",
      "      Unraveling node0: 9916b372670ab6a5e0fd8efc2323b778debc236b...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġminute', 'Ġchildren', \"Ġ'\", 's', 'Ġshows', 'Ġon', 'Ġcable', 'Ġhad']... → ' minute children 's shows on cable had...'\n",
      "      Unraveling node0: f12ecc66075dd5f7d61585e1db6cdcce372be3b1...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġdemonstrated', 'Ġthat', 'Ġchildren', \"Ġ'\", 's', 'Ġattention', 'Ġcould', 'Ġbe']... → ' demonstrated that children 's attention could be...'\n",
      "      Unraveling node0: 1ca716cc8290d17fabbc28a731485cfb55cc7c12...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġsustained', 'Ġfor', 'Ġlonger', 'Ġperiods', 'Ġof', 'Ġtime', 'Ġ,', 'Ġbut']... → ' sustained for longer periods of time , but...'\n",
      "      Unraveling node0: 43d30465d6248c07718f645080cb15201d29d84b...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthe', 'ĠCT', 'W', \"Ġ'\", 's', 'Ġresearchers', 'Ġfound', 'Ġthat']... → ' the CTW 's researchers found that...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "\n",
      "  Unraveling FUTURE (predicted next) from level 1...\n",
      "  Unraveling 1 future events from level 1\n",
      "  Unraveling future pattern: 3e5ab6e31560ffa1...\n",
      "    Unraveling node1: PTRN|3e5ab6e31560ffa17cc1671a370121853f0e33cb...\n",
      "      Pattern has 3 events\n",
      "      → Has 3 children at node0\n",
      "      Unraveling node0: c85da68d9acb10e6c201815bb36e5979a2510a85...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġtheir', 'Ġviewers', 'Ġ,', 'Ġespecially', 'Ġthe', 'Ġyounger', 'Ġones', 'Ġ,']... → ' their viewers , especially the younger ones ,...'\n",
      "      Unraveling node0: a6071da290774c10f30eab76a4cc4806e7681466...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġlost', 'Ġattention', 'Ġin', 'ĠS', 'esame', 'ĠStreet', 'Ġafter', 'Ġ40']... → ' lost attention in Sesame Street after 40...'\n",
      "      Unraveling node0: 19a6e03e45cf6ef566374e430905095bd72a6726...\n",
      "        Pattern has 6 events\n",
      "        → Tokens: ['Ġto', 'Ġ45', 'Ġminutes', 'Ġ.', 'Ġ', 'Ċ']... → ' to 45 minutes . \n",
      "...'\n",
      "      ✓ Unraveled to 22 total tokens\n",
      "  Total future tokens: 22\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 22\n",
      "     Total tokens: 86\n",
      "========================================\n",
      "  Present:  format was not the best way to attract young children 's attention . The growth of home videos during the ' 80s and the increase of thirty @-@ minute children 's shows on cable had demonstrated that children 's attention could be sustained for longer periods of time , but the CTW 's researchers found that\n",
      "----------------------------------------\n",
      "  Future:  their viewers , especially the younger ones , lost attention in Sesame Street after 40 to 45 minutes . \n",
      "\n",
      "========================================\n",
      "  Text preview:  format was not the best way to attract young children 's attention . The growth of home videos during the ' 80s and the increase of thirty @-@ minute children 's shows on cable had demonstrated that children 's attention could be sustained for longer periods of time , but the CTW 's researchers found that their viewers , especially the younger ones , lost attention in Sesame Street after 40 to 45 minutes . \n",
      "\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 2 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  2.2980\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.4000\n",
      "    Evidence:                   0.3333\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         0.5455\n",
      "    Bayesian Prior:             0.5000\n",
      "    Bayesian Likelihood:        0.4000\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.3667\n",
      "    SNR:                        0.3333\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.0598\n",
      "    Global Normalized Entropy:  0.0000\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        0.5000\n",
      "    Weighted Strength:          0.2000\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           0.8536\n",
      "    TF-IDF Score:               16.1554\n",
      "  Text:  July 1969 . The results were \" generally very positive \" ; children learned from the shows , their appeal was high , and their attention was sustained over the full hour . However , the researchers found that although children 's attention was high during the Muppet segments , their interest wavered during the \" Street \" segments , when no Muppets were on screen . This was because the producers had followed the advice of child psychologists who were concerned that children would be confused if human actors and Muppets were shown together . As a result of this decision , the appeal of the test episodes was lower than the target . \n",
      "\n",
      "\n",
      "\n",
      "Result 2:\n",
      "  Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  2.2702\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.3333\n",
      "    Evidence:                   0.2500\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         0.4545\n",
      "    Bayesian Prior:             0.5000\n",
      "    Bayesian Likelihood:        0.3333\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.3667\n",
      "    SNR:                        0.3333\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.0598\n",
      "    Global Normalized Entropy:  0.0000\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        0.5000\n",
      "    Weighted Strength:          0.1667\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           0.8536\n",
      "    TF-IDF Score:               16.1554\n",
      "  Text:  format was not the best way to attract young children 's attention . The growth of home videos during the ' 80s and the increase of thirty @-@ minute children 's shows on cable had demonstrated that children 's attention could be sustained for longer periods of time , but the CTW 's researchers found that their viewers , especially the younger ones , lost attention in Sesame Street after 40 to 45 minutes . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try your own input!\n",
    "input_text = \"The researchers found that\"  # <-- Change this\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    # max_predictions=100,\n",
    "    verbose=True,\n",
    "    verbose_unravel=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    --- Core Quality ---\")\n",
    "    print(f\"    Potential:                  {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:                 {result['confidence']:.4f}\")\n",
    "    print(f\"    Similarity:                 {result['similarity']:.4f}\")\n",
    "    print(f\"    Evidence:                   {result['evidence']:.4f}\")\n",
    "    print(f\"    --- Bayesian ---\")\n",
    "    print(f\"    Bayesian Posterior:         {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Prior:             {result['bayesian_prior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:        {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    --- Information Theory ---\")\n",
    "    print(f\"    Predictive Information:     {result['predictive_information']:.4f}\")\n",
    "    print(f\"    SNR:                        {result['snr']:.4f}\")\n",
    "    print(f\"    Entropy:                    {result['entropy']:.4f}\")\n",
    "    print(f\"    Normalized Entropy:         {result['normalized_entropy']:.4f}\")\n",
    "    print(f\"    Global Normalized Entropy:  {result['global_normalized_entropy']:.4f}\")\n",
    "    print(f\"    --- Pattern Strength ---\")\n",
    "    print(f\"    Frequency:                  {result['frequency']}\")\n",
    "    print(f\"    Pattern Probability:        {result['pattern_probability']:.4f}\")\n",
    "    print(f\"    Weighted Strength:          {result['weighted_strength']:.4f}\")\n",
    "    print(f\"    Fragmentation:              {result['fragmentation']:.4f}\")\n",
    "    print(f\"    ITFDF Similarity:           {result['itfdf_similarity']:.4f}\")\n",
    "    print(f\"    TF-IDF Score:               {result['tfidf_score']:.4f}\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Known sample\n",
    "\n",
    "The following includes exact phrases that are known to have been learned by the agent.\n",
    "\n",
    "First, we'll provide the STM with a little context. This won't produce robust predictions. But, next we'll take the output as predicted, add it to the STM, and request a second prediction.\n",
    "\n",
    "Here's the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_id': 'session-805abf96df4f457c8add72cbb74fe1f4-1770227190343',\n",
       " 'node_id': 'node0',\n",
       " 'created_at': '2026-02-04T17:46:30.343413Z',\n",
       " 'expires_at': '2026-02-04T18:47:10.327325Z',\n",
       " 'ttl_seconds': 3599,\n",
       " 'metadata': {},\n",
       " 'session_config': {'max_pattern_length': 0,\n",
       "  'persistence': 5,\n",
       "  'recall_threshold': 0.3,\n",
       "  'indexer_type': 'VI',\n",
       "  'max_predictions': 10,\n",
       "  'sort_symbols': True,\n",
       "  'process_predictions': False,\n",
       "  'use_token_matching': True,\n",
       "  'stm_mode': 'CLEAR',\n",
       "  'rank_sort_algo': 'potential',\n",
       "  'filter_pipeline': ['jaccard'],\n",
       "  'length_min_ratio': 0.5,\n",
       "  'length_max_ratio': 2.0,\n",
       "  'jaccard_threshold': 0.3,\n",
       "  'jaccard_min_overlap': 2,\n",
       "  'minhash_threshold': 0.7,\n",
       "  'minhash_bands': 20,\n",
       "  'minhash_rows': 5,\n",
       "  'minhash_num_hashes': 100,\n",
       "  'bloom_false_positive_rate': 0.01,\n",
       "  'max_candidates_per_stage': 1000,\n",
       "  'enable_filter_metrics': True}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_session_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input:  Among flukes, the most common\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input:  Among flukes, the most common\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (7): ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 7 tokens\n",
      "  node0 STM: 7 events\n",
      "     STM: [['ĠAmong'], ['Ġfl'], ['ukes'], [','], ['Ġthe'], ['Ġmost'], ['Ġcommon']]\n",
      "✓ Got 1 predictions\n",
      "  Sample predictions:\n",
      "    1. 6850d8ef6abf023e778693c4d5d9986db464e5cd... (conf: 0.857)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 1 pattern names as 1 event:\n",
      "    1. PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd']]\n",
      "✓ Got 1 predictions\n",
      "  Sample predictions:\n",
      "    1. d1a599707a223c611f6547a56077af442f9e1e22... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 1 pattern names as 1 event:\n",
      "    1. PTRN|d1a599707a223c611f6547a56077af442f9e1e22...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|d1a599707a223c611f6547a56077af442f9e1e22']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node2\n",
      "     STM has 1 event(s)\n",
      "     First event: ['PTRN|d1a599707a223c611f6547a56077af442f9e1e22']... (1 symbols)\n",
      "     Possible reasons:\n",
      "       - No node2 patterns in KB match this sequence\n",
      "       - recall_threshold too high\n",
      "⚠ No predictions from node2 - stopping cascade for this chunk\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 1 predictions\n",
      "  node1: 1 predictions\n",
      "  node2: 0 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "✓ Using node1 predictions (1 patterns with usable future data)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/1:\n",
      "  Matched Pattern: d1a599707a223c611f6547a56077af442f9e1e22...\n",
      "  Level: node1\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 7 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  2.1250\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.2222\n",
      "    Evidence:                   0.1250\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         0.0000\n",
      "    Bayesian Prior:             0.0000\n",
      "    Bayesian Likelihood:        0.0000\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.0000\n",
      "    SNR:                        1.0000\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.0000\n",
      "    Global Normalized Entropy:  0.0000\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        0.0000\n",
      "    Weighted Strength:          0.0000\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           0.0000\n",
      "    TF-IDF Score:               0.0000\n",
      "\n",
      "  Extracting PRESENT tokens from pred['present'] field...\n",
      "    Unraveling present pattern: P... (from level 0)\n",
      "    Unraveling node0: P...\n",
      "      ⚠ Pattern not found via KATO API\n",
      "        Searched for: P\n",
      "      Warning: Failed to unravel pattern P...\n",
      "  ⚠ Failed to unravel present pattern\n",
      "\n",
      "  Unraveling FUTURE (predicted next) from level 0...\n",
      "  Unraveling 7 future events from level 0\n",
      "    → Extracted 7 tokens directly\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 0\n",
      "     Future tokens: 7\n",
      "     Total tokens: 7\n",
      "========================================\n",
      "  Present: \n",
      "----------------------------------------\n",
      "  Future: PTRN|b9f48713626a92cd5c533ea1bafea4c1ac50bbf2PTRN|7729f0ed56a13a9373fc1b1c17e34f61d4512ab4PTRN|017e91585ea0db850a387c8756f67da5c1510254PTRN|5426b4ef2a35420155b571dd9144a4a6ee83182dPTRN|cb3ca27cd9600ed6456bf9d66796ced2ab1c9a60PTRN|00701e4427767f25611b22200f81bf747798246bPTRN|fa7e7be5cbabd7dc1f5c0b078a5501d0fd5aa30b\n",
      "========================================\n",
      "  Text preview: PTRN|b9f48713626a92cd5c533ea1bafea4c1ac50bbf2PTRN|7729f0ed56a13a9373fc1b1c17e34f61d4512ab4PTRN|017e91585ea0db850a387c8756f67da5c1510254PTRN|5426b4ef2a35420155b571dd9144a4a6ee83182dPTRN|cb3ca27cd9600ed6456bf9d66796ced2ab1c9a60PTRN|00701e4427767f25611b22200f81bf747798246bPTRN|fa7e7be5cbabd7dc1f5c0b078a5501d0fd5aa30b\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 1 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  2.1250\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.2222\n",
      "    Evidence:                   0.1250\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         0.0000\n",
      "    Bayesian Prior:             0.0000\n",
      "    Bayesian Likelihood:        0.0000\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.0000\n",
      "    SNR:                        1.0000\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.0000\n",
      "    Global Normalized Entropy:  0.0000\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        0.0000\n",
      "    Weighted Strength:          0.0000\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           0.0000\n",
      "    TF-IDF Score:               0.0000\n",
      "  Text length: 315 chars\n",
      "  Text: PTRN|b9f48713626a92cd5c533ea1bafea4c1ac50bbf2PTRN|7729f0ed56a13a9373fc1b1c17e34f61d4512ab4PTRN|017e91585ea0db850a387c8756f67da5c1510254PTRN|5426b4ef2a35420155b571dd9144a4a6ee83182dPTRN|cb3ca27cd9600ed6456bf9d66796ced2ab1c9a60PTRN|00701e4427767f25611b22200f81bf747798246bPTRN|fa7e7be5cbabd7dc1f5c0b078a5501d0fd5aa30b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with various inputs to see generation quality\n",
    "# The system now combines PRESENT (matched pattern) + FUTURE (predicted continuation)\n",
    "\n",
    "# Example inputs - uncomment the one you want to test:\n",
    "\n",
    "# input_text = \"The cat sat\"\n",
    "# input_text = \"the most common in North American wolves\"\n",
    "# input_text = \"Tapeworms generally cause little harm in wolves, though this depends on the number and size of the parasites, and the sensitivity of the host.\"\n",
    "\n",
    "# This input finds a rich pattern about wolf parasites (830 tokens total with present+future):\n",
    "input_text = \" Among flukes, the most common\"  # longer gets more: \" Among flukes, the most common in\"  \n",
    "\n",
    "# Note: The space at the beginning and \"in\" at the end are important for matching the correct pattern\n",
    "# This demonstrates how exact phrasing affects pattern matching\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    verbose=True,\n",
    "    verbose_unravel=True  # Set to True to see detailed unraveling\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    --- Core Quality ---\")\n",
    "    print(f\"    Potential:                  {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:                 {result['confidence']:.4f}\")\n",
    "    print(f\"    Similarity:                 {result['similarity']:.4f}\")\n",
    "    print(f\"    Evidence:                   {result['evidence']:.4f}\")\n",
    "    print(f\"    --- Bayesian ---\")\n",
    "    print(f\"    Bayesian Posterior:         {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Prior:             {result['bayesian_prior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:        {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    --- Information Theory ---\")\n",
    "    print(f\"    Predictive Information:     {result['predictive_information']:.4f}\")\n",
    "    print(f\"    SNR:                        {result['snr']:.4f}\")\n",
    "    print(f\"    Entropy:                    {result['entropy']:.4f}\")\n",
    "    print(f\"    Normalized Entropy:         {result['normalized_entropy']:.4f}\")\n",
    "    print(f\"    Global Normalized Entropy:  {result['global_normalized_entropy']:.4f}\")\n",
    "    print(f\"    --- Pattern Strength ---\")\n",
    "    print(f\"    Frequency:                  {result['frequency']}\")\n",
    "    print(f\"    Pattern Probability:        {result['pattern_probability']:.4f}\")\n",
    "    print(f\"    Weighted Strength:          {result['weighted_strength']:.4f}\")\n",
    "    print(f\"    Fragmentation:              {result['fragmentation']:.4f}\")\n",
    "    print(f\"    ITFDF Similarity:           {result['itfdf_similarity']:.4f}\")\n",
    "    print(f\"    TF-IDF Score:               {result['tfidf_score']:.4f}\")\n",
    "    print(f\"  Text length: {len(result['text'])} chars\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we'll add the predicted output, \"in\", back and get more predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input:  Among flukes, the most common in\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input:  Among flukes, the most common in\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (8): ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 8 tokens\n",
      "  node0 STM: 8 events\n",
      "     STM: [['ĠAmong'], ['Ġfl'], ['ukes'], [','], ['Ġthe'], ['Ġmost'], ['Ġcommon'], ['Ġin']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. 6850d8ef6abf023e778693c4d5d9986db464e5cd... (conf: 0.875)\n",
      "    2. f506059a84fdd8401fa3a74f5fc023ad09046240... (conf: 1.000)\n",
      "    3. a83e489f884347bdaa07738c8f5bc077b6bb3a5b... (conf: 1.000)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd...\n",
      "    2. PTRN|f506059a84fdd8401fa3a74f5fc023ad09046240...\n",
      "    3. PTRN|a83e489f884347bdaa07738c8f5bc077b6bb3a5b...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|16af5fe3b43df1a950294636abe58dd02b99ada0', 'PTRN|1d98518581f3e67fb3ae4a71d09871effa105603', 'PTRN|2d5ee11633b83c53128ca4adaffb9a834958b4b0', 'PTRN|5817b7ceb855b16b6ff514286405c6fb690406f8', 'PTRN|5a4794b77ab147e09cb6eb966f14f3c0323d1908', 'PTRN|61ae7dc87c5c734807511f9e3f1675d16729ad43', 'PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd', 'PTRN|a83e489f884347bdaa07738c8f5bc077b6bb3a5b', 'PTRN|cf6332b8378584e30eb132371ac904bf253c28b0', 'PTRN|f506059a84fdd8401fa3a74f5fc023ad09046240']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. 7f36ae17b40d201a08df4fcb9e3adc21e4e27a60... (conf: 1.000)\n",
      "    2. 4c07b5140d008073ddda2fb5bc0a2da0df843ddf... (conf: 1.000)\n",
      "    3. d1a599707a223c611f6547a56077af442f9e1e22... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|7f36ae17b40d201a08df4fcb9e3adc21e4e27a60...\n",
      "    2. PTRN|4c07b5140d008073ddda2fb5bc0a2da0df843ddf...\n",
      "    3. PTRN|d1a599707a223c611f6547a56077af442f9e1e22...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|20540becec8dfbea4b8bb645f0d3c41f9c7f5d3b', 'PTRN|25e06b7853cf8a7d0660eb729daf8a70480cf505', 'PTRN|3f1e89f2206c5a1e7aba754bd6cd37410f3c38aa', 'PTRN|3fc62e9d112a82c1ce7a2f1e35bbd53f02a18f9c', 'PTRN|49a4dc7cdf7e7d8cda2229b17a39cc9b39c1cfc5', 'PTRN|4c07b5140d008073ddda2fb5bc0a2da0df843ddf', 'PTRN|4cf3bea7fd32a154dfe4755c1d0ffe4e501b1752', 'PTRN|7f36ae17b40d201a08df4fcb9e3adc21e4e27a60', 'PTRN|825efe1d6bb323913dc5cfcccec1b931f1e7d5ca', 'PTRN|d1a599707a223c611f6547a56077af442f9e1e22']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. 9955cea759c4f143fe2d195a2cbe374d60971650... (conf: 1.000)\n",
      "    2. 5f5c5247ab03d7b4f92e4695c5d3a4b2847d544f... (conf: 1.000)\n",
      "    3. 3ee1949e9ca1599be14328df97d1b6a144aad89a... (conf: 1.000)\n",
      "\n",
      "--- NODE3 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|9955cea759c4f143fe2d195a2cbe374d60971650...\n",
      "    2. PTRN|5f5c5247ab03d7b4f92e4695c5d3a4b2847d544f...\n",
      "    3. PTRN|3ee1949e9ca1599be14328df97d1b6a144aad89a...\n",
      "✓ Observed (1 event)\n",
      "  node3 STM: 1 events\n",
      "     STM: [['PTRN|3ad67021b738759271c43a115c6f1958c981bb27', 'PTRN|3ee1949e9ca1599be14328df97d1b6a144aad89a', 'PTRN|5f5c5247ab03d7b4f92e4695c5d3a4b2847d544f', 'PTRN|6da6d667d8ba693e8566ef815bea17b18e15c56a', 'PTRN|96b65a6d6bc0daf66a9122f9e8c95714c0438463', 'PTRN|9955cea759c4f143fe2d195a2cbe374d60971650', 'PTRN|b8b2c2cf26a6ae6cc5c447296bfb7b0337c58bef', 'PTRN|bf234fcd5777ed76ba5dec3266c1e4d25e3e61aa', 'PTRN|d7d2104069ccb7330b122c88c1e85b6fa396c899', 'PTRN|f190e8da36d8730294ee2ecc809b93183426705d']]\n",
      "✓ Got 1 predictions\n",
      "  Sample predictions:\n",
      "    1. 5cf84d345958d19e1927cb579e5a3e8cb472aeaa... (conf: 1.000)\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 10 predictions\n",
      "  node1: 10 predictions\n",
      "  node2: 10 predictions\n",
      "  node3: 1 predictions\n",
      "\n",
      "✓ Using node3 predictions (1 patterns with usable future data)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/1:\n",
      "  Matched Pattern: 5cf84d345958d19e1927cb579e5a3e8cb472aeaa...\n",
      "  Level: node3\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  1.0455\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.1667\n",
      "    Evidence:                   0.5000\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         1.0000\n",
      "    Bayesian Prior:             1.0000\n",
      "    Bayesian Likelihood:        0.1667\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.1667\n",
      "    SNR:                        -0.6364\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.5143\n",
      "    Global Normalized Entropy:  0.0151\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        1.0000\n",
      "    Weighted Strength:          0.1667\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           1.0000\n",
      "    TF-IDF Score:               6.9307\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 512\n",
      "     Future tokens: 160\n",
      "     Total tokens: 672\n",
      "========================================\n",
      "  Present:  Endoparasites known to infect wolves include protozoans and helminths ( flukes , tapeworms , roundworms and thorny @-@ headed worms ) . Of 30 @,@ 000 protozoan species , only a few have been recorded to infect wolves : Isospora , Toxoplasma , Sarcocystis , Babesia , and Giardia . Wolves may carry Neospora caninum , which is of particular concern to farmers , as the disease can be spread to livestock , with infected animals being 3 – 13 times more likely to miscarry than those not infected . Among flukes , the most common in North American wolves is Alaria , which infects small rodents and amphibians that are eaten by wolves . Upon reaching maturity , Alaria migrates to the wolf 's intestine , but harms it little . Metorchis conjunctus , which enters wolves through eating fish , infects the wolf 's liver or gall bladder , causing liver disease , inflammation of the pancreas , and emaciation . Most other fluke species reside in the wolf 's intestine , though Paragonimus westermani lives in the lungs . Tapeworms are commonly found in wolves , as their primary hosts are ungulates , small mammals , and fish , which wolves feed upon . Tapeworms generally cause little harm in wolves , though this depends on the number and size of the parasites , and the sensitivity of the host . Symptoms often include constipation , toxic and allergic reactions , irritation of the intestinal mucosa , and malnutrition . Infections by the tapeworm Echinococcus granulosus in ungulate populations tend to increase in areas with high wolf densities , as wolves can shed Echinoccocus eggs in their feces onto grazing areas . Wolves can carry over 30 roundworm species , though most roundworm infections appear benign , depending on the number of worms and the age of the host . Ancylostoma caninum attaches itself on the intestinal wall to feed on the host 's blood , and can cause hyperchromic anemia , emaciation , diarrhea , and possibly death . Toxocara canis , a hookworm known to infect wolf pups in utero , can cause intestinal irritation , bloating , vomiting , and diarrhea . Wolves may catch Dioctophyma renale from minks , which infects the kidneys , and can grow to lengths of 100 cm . D. renale causes the\n",
      "----------------------------------------\n",
      "  Future:  complete destruction of the kidney 's functional tissue , and can be fatal if both kidneys are infected . Wolves can tolerate low levels of Dirofilaria immitis for many years without showing any ill effects , though high levels can kill wolves through cardiac enlargement and congestive hepatopathy . Wolves probably become infected with Trichinella spiralis by eating infected ungulates . Although T. spiralis isn 't known to produce clinical signs in wolves , it can cause emaciation , salivation , and crippling muscle pains in dogs . Thorny @-@ headed worms rarely infect wolves , though three species have been identified in Russian wolves : Nicolla skrjabini , Macrocantorhynchus catulinus , and Moniliformis moniliformis . \n",
      "========================================\n",
      "  Text preview:  Endoparasites known to infect wolves include protozoans and helminths ( flukes , tapeworms , roundworms and thorny @-@ headed worms ) . Of 30 @,@ 000 protozoan species , only a few have been recorded to infect wolves : Isospora , Toxoplasma , Sarcocystis , Babesia , and Giardia . Wolves may carry Neospora caninum , which is of particular concern to farmers , as the disease can be spread to livestock , with infected animals being 3 – 13 times more likely to miscarry than those not infected . Among flukes , the most common in North American wolves is Alaria , which infects small rodents and amphibians that are eaten by wolves . Upon reaching maturity , Alaria migrates to the wolf 's intestine , but harms it little . Metorchis conjunctus , which enters wolves through eating fish , infects the wolf 's liver or gall bladder , causing liver disease , inflammation of the pancreas , and emaciation . Most other fluke species reside in the wolf 's intestine , though Paragonimus westermani lives in the lungs . Tapeworms are commonly found in wolves , as their primary hosts are ungulates , small mammals , and fish , which wolves feed upon . Tapeworms generally cause little harm in wolves , though this depends on the number and size of the parasites , and the sensitivity of the host . Symptoms often include constipation , toxic and allergic reactions , irritation of the intestinal mucosa , and malnutrition . Infections by the tapeworm Echinococcus granulosus in ungulate populations tend to increase in areas with high wolf densities , as wolves can shed Echinoccocus eggs in their feces onto grazing areas . Wolves can carry over 30 roundworm species , though most roundworm infections appear benign , depending on the number of worms and the age of the host . Ancylostoma caninum attaches itself on the intestinal wall to feed on the host 's blood , and can cause hyperchromic anemia , emaciation , diarrhea , and possibly death . Toxocara canis , a hookworm known to infect wolf pups in utero , can cause intestinal irritation , bloating , vomiting , and diarrhea . Wolves may catch Dioctophyma renale from minks , which infects the kidneys , and can grow to lengths of 100 cm . D. renale causes the complete destruction of the kidney 's functional tissue , and can be fatal if both kidneys are infected . Wolves can tolerate low levels of Dirofilaria immitis for many years without showing any ill effects , though high levels can kill wolves through cardiac enlargement and congestive hepatopathy . Wolves probably become infected with Trichinella spiralis by eating infected ungulates . Although T. spiralis isn 't known to produce clinical signs in wolves , it can cause emaciation , salivation , and crippling muscle pains in dogs . Thorny @-@ headed worms rarely infect wolves , though three species have been identified in Russian wolves : Nicolla skrjabini , Macrocantorhynchus catulinus , and Moniliformis moniliformis . \n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 1 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    --- Core Quality ---\n",
      "    Potential:                  1.0455\n",
      "    Confidence:                 1.0000\n",
      "    Similarity:                 0.1667\n",
      "    Evidence:                   0.5000\n",
      "    --- Bayesian ---\n",
      "    Bayesian Posterior:         1.0000\n",
      "    Bayesian Prior:             1.0000\n",
      "    Bayesian Likelihood:        0.1667\n",
      "    --- Information Theory ---\n",
      "    Predictive Information:     0.1667\n",
      "    SNR:                        -0.6364\n",
      "    Entropy:                    0.0000\n",
      "    Normalized Entropy:         0.5143\n",
      "    Global Normalized Entropy:  0.0151\n",
      "    --- Pattern Strength ---\n",
      "    Frequency:                  1\n",
      "    Pattern Probability:        1.0000\n",
      "    Weighted Strength:          0.1667\n",
      "    Fragmentation:              0.0000\n",
      "    ITFDF Similarity:           1.0000\n",
      "    TF-IDF Score:               6.9307\n",
      "  Text length: 2945 chars\n",
      "  Text:  Endoparasites known to infect wolves include protozoans and helminths ( flukes , tapeworms , roundworms and thorny @-@ headed worms ) . Of 30 @,@ 000 protozoan species , only a few have been recorded to infect wolves : Isospora , Toxoplasma , Sarcocystis , Babesia , and Giardia . Wolves may carry Neospora caninum , which is of particular concern to farmers , as the disease can be spread to livestock , with infected animals being 3 – 13 times more likely to miscarry than those not infected . Among flukes , the most common in North American wolves is Alaria , which infects small rodents and amphibians that are eaten by wolves . Upon reaching maturity , Alaria migrates to the wolf 's intestine , but harms it little . Metorchis conjunctus , which enters wolves through eating fish , infects the wolf 's liver or gall bladder , causing liver disease , inflammation of the pancreas , and emaciation . Most other fluke species reside in the wolf 's intestine , though Paragonimus westermani lives in the lungs . Tapeworms are commonly found in wolves , as their primary hosts are ungulates , small mammals , and fish , which wolves feed upon . Tapeworms generally cause little harm in wolves , though this depends on the number and size of the parasites , and the sensitivity of the host . Symptoms often include constipation , toxic and allergic reactions , irritation of the intestinal mucosa , and malnutrition . Infections by the tapeworm Echinococcus granulosus in ungulate populations tend to increase in areas with high wolf densities , as wolves can shed Echinoccocus eggs in their feces onto grazing areas . Wolves can carry over 30 roundworm species , though most roundworm infections appear benign , depending on the number of worms and the age of the host . Ancylostoma caninum attaches itself on the intestinal wall to feed on the host 's blood , and can cause hyperchromic anemia , emaciation , diarrhea , and possibly death . Toxocara canis , a hookworm known to infect wolf pups in utero , can cause intestinal irritation , bloating , vomiting , and diarrhea . Wolves may catch Dioctophyma renale from minks , which infects the kidneys , and can grow to lengths of 100 cm . D. renale causes the complete destruction of the kidney 's functional tissue , and can be fatal if both kidneys are infected . Wolves can tolerate low levels of Dirofilaria immitis for many years without showing any ill effects , though high levels can kill wolves through cardiac enlargement and congestive hepatopathy . Wolves probably become infected with Trichinella spiralis by eating infected ungulates . Although T. spiralis isn 't known to produce clinical signs in wolves , it can cause emaciation , salivation , and crippling muscle pains in dogs . Thorny @-@ headed worms rarely infect wolves , though three species have been identified in Russian wolves : Nicolla skrjabini , Macrocantorhynchus catulinus , and Moniliformis moniliformis . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with various inputs to see generation quality\n",
    "# The system now combines PRESENT (matched pattern) + FUTURE (predicted continuation)\n",
    "\n",
    "# Example inputs - uncomment the one you want to test:\n",
    "\n",
    "# input_text = \"The cat sat\"\n",
    "# input_text = \"the most common in North American wolves\"\n",
    "# input_text = \"Tapeworms generally cause little harm in wolves, though this depends on the number and size of the parasites, and the sensitivity of the host.\"\n",
    "\n",
    "# This input finds a rich pattern about wolf parasites (830 tokens total with present+future):\n",
    "input_text = \" Among flukes, the most common in\"  \n",
    "\n",
    "# Note: The space at the beginning and \"in\" at the end are important for matching the correct pattern\n",
    "# This demonstrates how exact phrasing affects pattern matching\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    verbose=True,\n",
    "    verbose_unravel=True  # Set to True to see detailed unraveling\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    --- Core Quality ---\")\n",
    "    print(f\"    Potential:                  {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:                 {result['confidence']:.4f}\")\n",
    "    print(f\"    Similarity:                 {result['similarity']:.4f}\")\n",
    "    print(f\"    Evidence:                   {result['evidence']:.4f}\")\n",
    "    print(f\"    --- Bayesian ---\")\n",
    "    print(f\"    Bayesian Posterior:         {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Prior:             {result['bayesian_prior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:        {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    --- Information Theory ---\")\n",
    "    print(f\"    Predictive Information:     {result['predictive_information']:.4f}\")\n",
    "    print(f\"    SNR:                        {result['snr']:.4f}\")\n",
    "    print(f\"    Entropy:                    {result['entropy']:.4f}\")\n",
    "    print(f\"    Normalized Entropy:         {result['normalized_entropy']:.4f}\")\n",
    "    print(f\"    Global Normalized Entropy:  {result['global_normalized_entropy']:.4f}\")\n",
    "    print(f\"    --- Pattern Strength ---\")\n",
    "    print(f\"    Frequency:                  {result['frequency']}\")\n",
    "    print(f\"    Pattern Probability:        {result['pattern_probability']:.4f}\")\n",
    "    print(f\"    Weighted Strength:          {result['weighted_strength']:.4f}\")\n",
    "    print(f\"    Fragmentation:              {result['fragmentation']:.4f}\")\n",
    "    print(f\"    ITFDF Similarity:           {result['itfdf_similarity']:.4f}\")\n",
    "    print(f\"    TF-IDF Score:               {result['tfidf_score']:.4f}\")\n",
    "    print(f\"  Text length: {len(result['text'])} chars\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4b: Adjusting Prediction Ensemble Size\n",
    "\n",
    "Control how many predictions flow between levels using `max_predictions`.\n",
    "\n",
    "**Impact of ensemble size**:\n",
    "- **Larger ensembles** (20-50): More context sent to next level → better quality, but slower and potentially noisy\n",
    "- **Smaller ensembles** (3-5): Less context → faster and cleaner, but may miss important patterns\n",
    "- **Recommended**: 5-15 for balanced quality and speed\n",
    "\n",
    "**Use cases**:\n",
    "- Speed-critical applications: Use small ensembles\n",
    "- Quality-critical applications: Use large ensembles\n",
    "- Novel inputs: Use larger ensembles to catch weak matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: Small Ensembles (max_predictions=[3, 3, 3, 3])\n",
      "================================================================================\n",
      "\n",
      "Results: 1 predictions from node2\n",
      "  1. fd46712761f6833c53015b2fe771bb56c2f9d58e... (conf: 1.000)\n",
      "\n",
      "================================================================================\n",
      "TEST 2: Large Ensembles (max_predictions=[20, 15, 10, 5])\n",
      "================================================================================\n",
      "\n",
      "Results: 1 predictions from node2\n",
      "  1. fd46712761f6833c53015b2fe771bb56c2f9d58e... (conf: 1.000)\n",
      "\n",
      "================================================================================\n",
      "COMPARISON\n",
      "================================================================================\n",
      "Small ensembles: 1 predictions from node2\n",
      "Large ensembles: 1 predictions from node2\n",
      "\n",
      "Larger ensembles provide more context to higher levels.\n",
      "This can lead to different prediction quantities and levels activated.\n"
     ]
    }
   ],
   "source": [
    "# Compare different ensemble sizes using chunk_sizes and max_predictions parameters\n",
    "input_text = \"The cat sat on the mat\"\n",
    "\n",
    "# Test 1: Small ensembles (faster, cleaner)\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: Small Ensembles (max_predictions=[3, 3, 3, 3])\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_small = activate_hierarchy(\n",
    "    input_text=input_text,\n",
    "    verbose=False,\n",
    "    chunk_sizes=CHUNK_SIZES,      # Use global chunk_sizes\n",
    "    max_predictions=[3, 3, 3, 3]  # Small ensembles\n",
    ")\n",
    "\n",
    "# Get predictions from highest level\n",
    "preds_small, level_small = get_prediction_ensemble(results_small, verbose=False)\n",
    "\n",
    "print(f\"\\nResults: {len(preds_small)} predictions from node{level_small}\")\n",
    "for i, pred in enumerate(preds_small[:2]):\n",
    "    print(f\"  {i+1}. {pred['name'][:60]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "\n",
    "# Test 2: Large ensembles (slower, more context)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: Large Ensembles (max_predictions=[20, 15, 10, 5])\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_large = activate_hierarchy(\n",
    "    input_text=input_text,\n",
    "    verbose=False,\n",
    "    chunk_sizes=CHUNK_SIZES,            # Use global chunk_sizes\n",
    "    max_predictions=[20, 15, 10, 5]     # Large ensembles\n",
    ")\n",
    "\n",
    "preds_large, level_large = get_prediction_ensemble(results_large, verbose=False)\n",
    "\n",
    "print(f\"\\nResults: {len(preds_large)} predictions from node{level_large}\")\n",
    "for i, pred in enumerate(preds_large[:2]):\n",
    "    print(f\"  {i+1}. {pred['name'][:60]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Small ensembles: {len(preds_small)} predictions from node{level_small}\")\n",
    "print(f\"Large ensembles: {len(preds_large)} predictions from node{level_large}\")\n",
    "print(\"\\nLarger ensembles provide more context to higher levels.\")\n",
    "print(\"This can lead to different prediction quantities and levels activated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Auto-Adjust recall_threshold (Cold Start Handling)\n",
    "\n",
    "If input is very novel (not seen during training), predictions may be empty with default recall_threshold.\n",
    "\n",
    "**auto_adjust_recall=True** will automatically try progressively lower thresholds (0.5, 0.4, 0.3, 0.2, 0.1) until predictions are found.\n",
    "\n",
    "This is useful for:\n",
    "- Novel inputs not in training data\n",
    "- Cold start scenarios\n",
    "- Graceful degradation (lower quality predictions rather than nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If input is very novel (not seen during training), predictions may be empty\n",
    "# auto_adjust_recall=True will automatically try lower thresholds\n",
    "\n",
    "input_text = \"Quantum entanglement demonstrates\"  # Novel input\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    # max_predictions=3,\n",
    "    auto_adjust_recall=True,  # Enable auto-adjustment\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"  Metrics:\")\n",
    "        print(f\"    --- Core Quality ---\")\n",
    "        print(f\"    Potential:                  {result['potential']:.4f}\")\n",
    "        print(f\"    Confidence:                 {result['confidence']:.4f}\")\n",
    "        print(f\"    Similarity:                 {result['similarity']:.4f}\")\n",
    "        print(f\"    Evidence:                   {result['evidence']:.4f}\")\n",
    "        print(f\"    --- Bayesian ---\")\n",
    "        print(f\"    Bayesian Posterior:         {result['bayesian_posterior']:.4f}\")\n",
    "        print(f\"    Bayesian Prior:             {result['bayesian_prior']:.4f}\")\n",
    "        print(f\"    Bayesian Likelihood:        {result['bayesian_likelihood']:.4f}\")\n",
    "        print(f\"    --- Information Theory ---\")\n",
    "        print(f\"    Predictive Information:     {result['predictive_information']:.4f}\")\n",
    "        print(f\"    SNR:                        {result['snr']:.4f}\")\n",
    "        print(f\"    Entropy:                    {result['entropy']:.4f}\")\n",
    "        print(f\"    Normalized Entropy:         {result['normalized_entropy']:.4f}\")\n",
    "        print(f\"    Global Normalized Entropy:  {result['global_normalized_entropy']:.4f}\")\n",
    "        print(f\"    --- Pattern Strength ---\")\n",
    "        print(f\"    Frequency:                  {result['frequency']}\")\n",
    "        print(f\"    Pattern Probability:        {result['pattern_probability']:.4f}\")\n",
    "        print(f\"    Weighted Strength:          {result['weighted_strength']:.4f}\")\n",
    "        print(f\"    Fragmentation:              {result['fragmentation']:.4f}\")\n",
    "        print(f\"    ITFDF Similarity:           {result['itfdf_similarity']:.4f}\")\n",
    "        print(f\"    TF-IDF Score:               {result['tfidf_score']:.4f}\")\n",
    "        print(f\"  Text: {result['text'][:300]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n⚠ No predictions found. Input is completely novel to the system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging & Inspection\n",
    "\n",
    "Useful cells for inspecting the system state and understanding what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Specific Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a specific pattern using KATO API\n",
    "# Replace with an actual pattern name from your predictions\n",
    "# Pattern name can be with or without 'PTRN|' prefix\n",
    "\n",
    "pattern_name = \"92aefd2182f65233aaa2b975b0cc292f967b745c\"\n",
    "level = 0\n",
    "\n",
    "# Strip prefix before querying (handles both formats)\n",
    "clean_name = strip_pattern_prefix(pattern_name)\n",
    "\n",
    "print(f\"Inspecting pattern: {pattern_name}\")\n",
    "print(f\"Queried as: {clean_name}\")\n",
    "print(f\"Level: node{level}\\n\")\n",
    "\n",
    "# ===== Query via KATO API =====\n",
    "print(\"=\"*80)\n",
    "print(\"Pattern Query via KATO API:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get pattern data via KATO API\n",
    "result = nodes[level].get_pattern(clean_name)\n",
    "inner = result.get('pattern', {})\n",
    "\n",
    "if inner.get('status') == 'okay':\n",
    "    pattern_data = inner.get('pattern', {})\n",
    "    metadata = inner.get('metadata', {})\n",
    "    \n",
    "    print(f\"✓ Pattern found\")\n",
    "    print(f\"  Frequency: {metadata.get('frequency', 0)}\")\n",
    "    \n",
    "    observations = pattern_data.get('observations', [])\n",
    "    print(f\"  Observations: {len(observations)} events\")\n",
    "    \n",
    "    print(f\"\\n  Pattern data (first 10 events):\")\n",
    "    for i, obs in enumerate(observations[:10]):\n",
    "        print(f\"    Event {i}: {obs}\")\n",
    "    \n",
    "    if level == 0:\n",
    "        # Decode tokens\n",
    "        tokens = []\n",
    "        for obs in observations:\n",
    "            token_list = obs.get('strings', [])\n",
    "            tokens.extend(token_list)\n",
    "        print(f\"\\n  Decoded tokens ({len(tokens)} total):\")\n",
    "        print(f\"  {' '.join(tokens)}\")\n",
    "    else:\n",
    "        # Show child patterns\n",
    "        print(f\"\\n  Child patterns (first 10):\")\n",
    "        for i, obs in enumerate(observations[:10]):\n",
    "            child_patterns = obs.get('strings', [])\n",
    "            for pattern in child_patterns:\n",
    "                clean_child = strip_pattern_prefix(pattern)\n",
    "                print(f\"    {i}: {clean_child[:32]}...\")\n",
    "else:\n",
    "    print(f\"✗ Pattern not found or error: {inner.get('status', 'unknown')}\")\n",
    "    if 'error' in inner:\n",
    "        print(f\"  Error: {inner['error']}\")\n",
    "\n",
    "# ===== KATO API Response Structure =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KATO API Response Structure:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Full result keys: {list(result.keys())}\")\n",
    "if 'pattern' in result:\n",
    "    print(f\"Inner pattern keys: {list(inner.keys())}\")\n",
    "    if 'pattern' in inner:\n",
    "        print(f\"Pattern data keys: {list(pattern_data.keys())}\")\n",
    "    if 'metadata' in inner:\n",
    "        print(f\"Metadata keys: {list(metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pattern Unraveling Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test unraveling a specific pattern with verbose output\n",
    "# This is useful for understanding the recursive unraveling process\n",
    "# Pattern name can be with or without 'PTRN|' prefix\n",
    "\n",
    "pattern_name = \"PTRN|04f57fef3b3f9b0f33b9d04987b16f56755d8c63\"  # <-- Put a pattern name from a higher level here\n",
    "level = 3  # Which level (1, 2, or 3 to see recursion)\n",
    "\n",
    "# pattern_name = \"PTRN|811d57463ca70786022e901064c3722794296a24\"\n",
    "# level = 2\n",
    "\n",
    "# pattern_name = \"PTRN|d1a599707a223c611f6547a56077af442f9e1e22\"\n",
    "# level = 1\n",
    "\n",
    "pattern_name = \"PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd\"\n",
    "level = 0\n",
    "\n",
    "print(f\"Unraveling pattern from node{level}:\")\n",
    "print(f\"Input: {pattern_name}\")\n",
    "print(f\"Clean name: {strip_pattern_prefix(pattern_name)}\")\n",
    "\n",
    "tokens = unravel_pattern(\n",
    "    pattern_name,  # Can include 'PTRN|' prefix - will be stripped automatically\n",
    "    level=level,\n",
    "    nodes=nodes,\n",
    "    verbose=True,  # Show detailed unraveling steps\n",
    "    indent=0\n",
    ")\n",
    "\n",
    "print(f\"\\nResult: {len(tokens)} tokens\")\n",
    "print(f\"Text: {' '.join(tokens[:50])}...\")  # Show first 50 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Close all connections when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Close KATO client connections\n",
    "# for i, node in enumerate(nodes):\n",
    "#     node.close()\n",
    "#     print(f\"✓ Closed node{i}\")\n",
    "\n",
    "# print(\"\\n✓ All KATO connections closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What you learned in this notebook:**\n",
    "\n",
    "1. **KATO API Usage**:\n",
    "   - `clear_stm()` - Clear short-term memory\n",
    "   - `observe(strings=[...])` - Send single observation\n",
    "   - `observe_sequence(observations=[...])` - Send batch of observations\n",
    "   - `get_predictions()` - Get predictions from current STM\n",
    "\n",
    "2. **Direct Database Pattern Retrieval**:\n",
    "   - Direct queries from ClickHouse + Redis\n",
    "   - Pattern structure: `pattern_data = [[child1], [child2], ...]`\n",
    "   - Frequency statistics and pattern inspection\n",
    "\n",
    "3. **Hierarchical Generation**:\n",
    "   - **Bottom-up activation**: Input text activates patterns at all levels\n",
    "   - **Top-down unraveling**: High-level patterns recursively expand to tokens\n",
    "   - **Cascading constraints**: Each level constrains the levels below\n",
    "\n",
    "4. **Key Concepts**:\n",
    "   - Each token = one event (KATO ordering constraint)\n",
    "   - Fallback logic for novel inputs\n",
    "   - Recursive pattern unraveling\n",
    "   - Faithful generation (patterns from training data)\n",
    "\n",
    "**Next Steps**:\n",
    "- Experiment with different inputs\n",
    "- Adjust `CHUNK_SIZE` and see how it affects results\n",
    "- Try generation from different hierarchical levels\n",
    "- Inspect patterns to understand what the system learned\n",
    "\n",
    "---\n",
    "\n",
    "**Educational Goal Achieved**: You now understand how to use KATO's API for hierarchical text generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use node3 pattern: \n",
    "b79d1353b9dd97e613e695042a4bb91bb7cc2ca9\n",
    "for a nice hierarchical graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
