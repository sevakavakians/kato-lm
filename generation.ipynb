{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATO Hierarchical Text Generation\n",
    "\n",
    "**Educational Notebook**: Learn how to use KATO's hierarchical architecture for text generation.\n",
    "\n",
    "**Prerequisites**: Train patterns first using `training_v2.ipynb` or `training.ipynb`.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "This notebook demonstrates **hierarchical text generation** through two complementary processes:\n",
    "\n",
    "### 1. Bottom-Up Activation (Input → Predictions)\n",
    "\n",
    "```\n",
    "User Input: \"The cat sat\"\n",
    "     ↓\n",
    "Tokenize: [\"The\", \" cat\", \" sat\"]\n",
    "     ↓\n",
    "node0: Observe tokens → Get predictions (chunk patterns)\n",
    "     ↓\n",
    "node1: Observe node0 pattern names → Get predictions (paragraph patterns)\n",
    "     ↓\n",
    "node2: Observe node1 pattern names → Get predictions (chapter patterns)\n",
    "     ↓\n",
    "node3: Observe node2 pattern names → Get predictions (book patterns)\n",
    "```\n",
    "\n",
    "**Result**: All hierarchy levels are activated by the input.\n",
    "\n",
    "### 2. Top-Down Unraveling (Pattern → Tokens)\n",
    "\n",
    "```\n",
    "node3 prediction: PTRN|book_xyz\n",
    "     ↓\n",
    "Query MongoDB: Get pattern_data → [node2_pattern_A, node2_pattern_B, ...]\n",
    "     ↓\n",
    "For each node2 pattern:\n",
    "    Query MongoDB → [node1_pattern_X, node1_pattern_Y, ...]\n",
    "        ↓\n",
    "    For each node1 pattern:\n",
    "        Query MongoDB → [node0_pattern_1, node0_pattern_2, ...]\n",
    "            ↓\n",
    "        For each node0 pattern:\n",
    "            Query MongoDB → [\"token1\", \"token2\", \"token3\"]\n",
    "                ↓\n",
    "            Decode tokens → \"The cat sat on the mat\"\n",
    "```\n",
    "\n",
    "**Result**: High-level pattern names are recursively unraveled down to actual tokens.\n",
    "\n",
    "### Cascading Constraint Satisfaction\n",
    "\n",
    "The combination creates **exponential search space reduction**:\n",
    "\n",
    "- **node3** says: \"We're generating text from 'Moby Dick' context\"\n",
    "- **node2** says: \"We're in a chapter about whales\"\n",
    "- **node1** says: \"We're in a descriptive paragraph\"\n",
    "- **node0** says: \"Next tokens should describe whale behavior\"\n",
    "\n",
    "Each level constrains the levels below, producing coherent, context-aware text.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook **exposes all KATO API calls** directly (no library abstractions) so you can see:\n",
    "\n",
    "1. How to use `observe()`, `observe_sequence()`, `get_predictions()`, `clear_stm()`\n",
    "2. How to query MongoDB to retrieve pattern structures\n",
    "3. How to recursively unravel patterns from high-level → tokens\n",
    "4. How the hierarchy creates coherent, faithful text generation\n",
    "\n",
    "**Educational Focus**: Understanding KATO's API and hierarchical generation mechanics.\n",
    "\n",
    "**Note**: This notebook complements `training_v2.ipynb` which shows the training process with the same explicit API style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install -q datasets transformers requests numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from tools.kato_client import KATOClient\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json\n",
    "\n",
    "# Note: We're using KATO's get_pattern() API for pattern retrieval\n",
    "# No direct database access needed - KATO combines ClickHouse + Redis data\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters\n",
    "\n",
    "**IMPORTANT**: These must match your training configuration!\n",
    "\n",
    "- `CHUNK_SIZE`: Number of tokens per chunk at node0 (must match training)\n",
    "- `TOKENIZER_NAME`: HuggingFace tokenizer used during training\n",
    "- `NODE_IDS`: KATO node identifiers (check your training manifest or MongoDB databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Chunk sizes: [8, 8, 8, 8]\n",
      "  Max predictions: [10, 10, 10, 10]\n",
      "  Tokenizer: gpt2\n",
      "  Node IDs: ['node0', 'node1', 'node2', 'node3']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - MODIFY THESE TO MATCH YOUR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "# Hierarchical configuration (MUST match training!)\n",
    "# chunk_sizes: How many tokens/patterns to process at each level\n",
    "# max_predictions: How many predictions in each ensemble sent to next level\n",
    "CHUNK_SIZES = [8, 8, 8, 8]      # [node0, node1, node2, node3]\n",
    "MAX_PREDICTIONS = [10, 10, 10, 10]  # [node0, node1, node2, node3]\n",
    "\n",
    "# Tokenizer (must match training)\n",
    "TOKENIZER_NAME = \"gpt2\"  # Options: \"gpt2\", \"bert-base-uncased\", \"roberta-base\", etc.\n",
    "\n",
    "# Recall threshold (pattern matching strictness)\n",
    "# Range: 0.0-1.0\n",
    "#   0.1 = permissive (many matches, lower quality)\n",
    "#   0.9 = strict (few matches, higher quality)\n",
    "# Default: 0.6 (balanced)\n",
    "RECALL_THRESHOLD_DEFAULT = 0.6\n",
    "\n",
    "# KATO server URL\n",
    "BASE_URL = \"http://kato:8000\"\n",
    "\n",
    "# Node identifiers (MUST match your training configuration)\n",
    "# Check your ClickHouse/Redis databases to find the correct node_ids\n",
    "# Format: {node_id}_kato databases in ClickHouse/Redis\n",
    "NODE_IDS = [\n",
    "    \"node0\",  # node0: chunk-level patterns (15 tokens)\n",
    "    \"node1\",  # node1: paragraph-level patterns (~225 tokens)\n",
    "    \"node2\",  # node2: chapter-level patterns (~3,375 tokens)\n",
    "    \"node3\"   # node3: book-level patterns (~50,625 tokens)\n",
    "]\n",
    "\n",
    "# Prediction fallback chain (try node3 first, fallback to node2, then node1, then node0)\n",
    "FALLBACK_LEVELS = [3, 2, 1, 0]\n",
    "\n",
    "# Validate configuration\n",
    "if len(CHUNK_SIZES) != len(NODE_IDS):\n",
    "    raise ValueError(f\"CHUNK_SIZES length ({len(CHUNK_SIZES)}) must match NODE_IDS length ({len(NODE_IDS)})\")\n",
    "if len(MAX_PREDICTIONS) != len(NODE_IDS):\n",
    "    raise ValueError(f\"MAX_PREDICTIONS length ({len(MAX_PREDICTIONS)}) must match NODE_IDS length ({len(NODE_IDS)})\")\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Chunk sizes: {CHUNK_SIZES}\")\n",
    "print(f\"  Max predictions: {MAX_PREDICTIONS}\")\n",
    "print(f\"  Tokenizer: {TOKENIZER_NAME}\")\n",
    "print(f\"  Node IDs: {NODE_IDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Class: TextTokenizer\n",
    "\n",
    "Simple wrapper around HuggingFace tokenizer for:\n",
    "1. Tokenizing input text\n",
    "2. Chunking tokens into fixed-length sequences\n",
    "3. Decoding tokens back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenizer initialized: gpt2\n",
      "\n",
      "Test tokenization:\n",
      "  Input: The cat sat on the mat.\n",
      "  Tokens: ['The', 'Ġcat', 'Ġsat', 'Ġon', 'Ġthe', 'Ġmat', '.']\n",
      "  Decoded: The cat sat on the mat.\n"
     ]
    }
   ],
   "source": [
    "class TextTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer wrapper for KATO hierarchical generation.\n",
    "    \n",
    "    Uses HuggingFace AutoTokenizer for tokenization and decoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer_name: str):\n",
    "        \"\"\"Initialize with HuggingFace tokenizer.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        \n",
    "        # Required for some tokenizers (GPT-2, etc.)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize text into list of token strings.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to tokenize\n",
    "            \n",
    "        Returns:\n",
    "            List of token strings (e.g., [\"The\", \" cat\", \" sat\"])\n",
    "        \"\"\"\n",
    "        # Tokenize using HuggingFace tokenizer\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    def chunk_tokens(self, tokens: List[str], chunk_size: int) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Split tokens into fixed-length chunks.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of token strings\n",
    "            chunk_size: Number of tokens per chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of token chunks (last chunk may be shorter)\n",
    "            \n",
    "        Example:\n",
    "            >>> tokens = [\"The\", \" cat\", \" sat\", \" on\", \" the\", \" mat\"]\n",
    "            >>> chunk_tokens(tokens, chunk_size=3)\n",
    "            [[\"The\", \" cat\", \" sat\"], [\" on\", \" the\", \" mat\"]]\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), chunk_size):\n",
    "            chunk = tokens[i:i + chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    def decode_tokens(self, tokens: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token strings back to text.\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of token strings\n",
    "            \n",
    "        Returns:\n",
    "            Decoded text string\n",
    "        \"\"\"\n",
    "        # Convert token strings to IDs, then decode\n",
    "        # This handles subword tokenization correctly\n",
    "        text = self.tokenizer.convert_tokens_to_string(tokens)\n",
    "        return text\n",
    "\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = TextTokenizer(TOKENIZER_NAME)\n",
    "print(f\"✓ Tokenizer initialized: {TOKENIZER_NAME}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"The cat sat on the mat.\"\n",
    "test_tokens = tokenizer.tokenize(test_text)\n",
    "print(f\"\\nTest tokenization:\")\n",
    "print(f\"  Input: {test_text}\")\n",
    "print(f\"  Tokens: {test_tokens}\")\n",
    "print(f\"  Decoded: {tokenizer.decode_tokens(test_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KATO Client Session Cleanup\n",
    "\n",
    "Why This Is Needed:\n",
    "\n",
    "* Prevents multiple sessions for the same node\n",
    "* Clears stale state from previous runs\n",
    "* Ensures fresh initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up old sessions...\n",
      "  ℹ No existing nodes to clean up (first run)\n",
      "✓ Session cleanup complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SESSION CLEANUP - Ensure Clean State\n",
    "# ============================================================================\n",
    "# Close any existing KATO sessions to prevent stale state issues.\n",
    "# This is especially important when re-running cells after errors.\n",
    "\n",
    "print(\"Cleaning up old sessions...\")\n",
    "\n",
    "try:\n",
    "    # Try to close existing node clients\n",
    "    for node in [node0, node1, node2, node3]:\n",
    "        try:\n",
    "            node.close()\n",
    "            print(f\"  ✓ Closed {node.node_id} session\")\n",
    "        except Exception as e:\n",
    "            # Ignore errors - node may not exist or session already closed\n",
    "            pass\n",
    "except NameError:\n",
    "    # Nodes don't exist yet (first run)\n",
    "    print(\"  ℹ No existing nodes to clean up (first run)\")\n",
    "\n",
    "print(\"✓ Session cleanup complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KATO Client Initialization\n",
    "\n",
    "**Educational**: Create one `KATOClient` instance per hierarchical level.\n",
    "\n",
    "Each client:\n",
    "- Connects to KATO server\n",
    "- Has its own isolated session\n",
    "- Uses `max_pattern_length=0` (prediction mode, not learning)\n",
    "- Maintains its own Short-Term Memory (STM)\n",
    "\n",
    "**Note**: We're using KATO for **generation**, not training, so we won't call `learn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing KATO clients...\n",
      "  ✓ node0: recall=0.6, max_pred=10\n",
      "  ✓ node1: recall=0.6, max_pred=10\n",
      "  ✓ node2: recall=0.6, max_pred=10\n",
      "  ✓ node3: recall=0.6, max_pred=10\n",
      "\n",
      "✓ All KATO clients ready for generation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KATO CLIENT INITIALIZATION - ONE PER NODE\n",
    "# ============================================================================\n",
    "\n",
    "# Create separate KATO clients for each hierarchical level\n",
    "# max_pattern_length=0: Prediction mode (no auto-learning)\n",
    "# recall_threshold: Pattern matching strictness\n",
    "# max_predictions: Number of predictions per ensemble (KATO config)\n",
    "# process_predictions=True: MUST enable predictions (may be disabled from training)\n",
    "\n",
    "print(\"Initializing KATO clients...\")\n",
    "\n",
    "# recall_threshold controls pattern matching strictness:\n",
    "#   High (0.7-0.9): Strict matching, fewer but higher-quality predictions\n",
    "#   Medium (0.4-0.6): Balanced (default: 0.6)\n",
    "#   Low (0.1-0.3): Permissive matching, more predictions (useful for novel inputs)\n",
    "#\n",
    "# max_predictions controls ensemble size:\n",
    "#   - KATO returns top N predictions per call\n",
    "#   - Entire ensemble sent as ONE event to next level\n",
    "#   - KATO's pattern matching handles missing/extra symbols gracefully\n",
    "#   - Higher values: more context but slower, potentially noisy\n",
    "#   - Lower values: faster but may miss important patterns\n",
    "#\n",
    "# process_predictions=True:\n",
    "#   - CRITICAL: Must be True for generation\n",
    "#   - During training, this is often set to False to save computation\n",
    "#   - Explicitly setting True here ensures predictions work regardless of training config\n",
    "\n",
    "node0 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    node_id=NODE_IDS[0],\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=RECALL_THRESHOLD_DEFAULT,\n",
    "    max_predictions=MAX_PREDICTIONS[0],\n",
    "    process_predictions=False,  # Enable prediction processing\n",
    "    timeout=120\n",
    ")\n",
    "print(f\"  ✓ node0: recall={RECALL_THRESHOLD_DEFAULT}, max_pred={MAX_PREDICTIONS[0]}\")\n",
    "\n",
    "node1 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    node_id=NODE_IDS[1],\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=RECALL_THRESHOLD_DEFAULT,\n",
    "    max_predictions=MAX_PREDICTIONS[1],\n",
    "    process_predictions=False,  # Enable prediction processing\n",
    "    timeout=120\n",
    ")\n",
    "print(f\"  ✓ node1: recall={RECALL_THRESHOLD_DEFAULT}, max_pred={MAX_PREDICTIONS[1]}\")\n",
    "\n",
    "node2 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    node_id=NODE_IDS[2],\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=RECALL_THRESHOLD_DEFAULT,\n",
    "    max_predictions=MAX_PREDICTIONS[2],\n",
    "    process_predictions=False,  # Enable prediction processing\n",
    "    timeout=120\n",
    ")\n",
    "print(f\"  ✓ node2: recall={RECALL_THRESHOLD_DEFAULT}, max_pred={MAX_PREDICTIONS[2]}\")\n",
    "\n",
    "node3 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    node_id=NODE_IDS[3],\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=RECALL_THRESHOLD_DEFAULT,\n",
    "    max_predictions=MAX_PREDICTIONS[3],\n",
    "    process_predictions=False,  # Enable prediction processing\n",
    "    timeout=120\n",
    ")\n",
    "print(f\"  ✓ node3: recall={RECALL_THRESHOLD_DEFAULT}, max_pred={MAX_PREDICTIONS[3]}\")\n",
    "\n",
    "# Store in list for easy iteration\n",
    "nodes = [node0, node1, node2, node3]\n",
    "\n",
    "print(\"\\n✓ All KATO clients ready for generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node0: {'status': 'okay', 'message': 'Configuration updated', 'session_id': 'session-e13f042302384871ba8fa9d5570f004c-1763850683400'}\n",
      "node1: {'status': 'okay', 'message': 'Configuration updated', 'session_id': 'session-ac778d2d5c8a4aa9aae2bd543c9dad90-1763850683425'}\n",
      "node2: {'status': 'okay', 'message': 'Configuration updated', 'session_id': 'session-f51db34614c546c281f266a064c3de0e-1763850683442'}\n",
      "node3: {'status': 'okay', 'message': 'Configuration updated', 'session_id': 'session-ef35738a8d084e3aafe30011a3c59e9d-1763850683458'}\n"
     ]
    }
   ],
   "source": [
    "# Configure nodes for better generation based on validated settings\n",
    "# node0 uses jaccard filter for token matching\n",
    "# Higher nodes use empty filter_pipeline for default pattern matching\n",
    "\n",
    "result = node0.update_session_config({\n",
    "      'filter_pipeline': ['jaccard'],\n",
    "      'jaccard_threshold': 0.3,      # Token set overlap\n",
    "      'jaccard_min_overlap': 2,      # At least 2 shared tokens\n",
    "      'recall_threshold': 0.3,       # Sequence similarity (lower for more matches)\n",
    "      'max_predictions': 10,\n",
    "})\n",
    "print(\"node0:\", result)\n",
    "\n",
    "# For node1, node2, node3: Use empty filter_pipeline if there's not a lot of data\n",
    "# Jaccard at higher levels can return nothing if patterns are sparse\n",
    "result = node1.update_session_config({\n",
    "      'filter_pipeline': [],  # Empty = use default pattern matcher\n",
    "      'recall_threshold': 0.1,       # Very permissive for higher levels\n",
    "      'max_predictions': 10,\n",
    "})\n",
    "print(\"node1:\", result)\n",
    "\n",
    "result = node2.update_session_config({\n",
    "      'filter_pipeline': [],  # Empty = use default pattern matcher\n",
    "      'recall_threshold': 0.1,       # Very permissive for higher levels\n",
    "      'max_predictions': 10,\n",
    "})\n",
    "print(\"node2:\", result)\n",
    "\n",
    "result = node3.update_session_config({\n",
    "      'filter_pipeline': [],  # Empty = use default pattern matcher\n",
    "      'recall_threshold': 0.1,       # Very permissive for higher levels\n",
    "      'max_predictions': 10,\n",
    "})\n",
    "print(\"node3:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Retrieval via KATO API\n",
    "\n",
    "**Educational**: KATO provides a `get_pattern()` API that retrieves learned patterns from its hybrid storage architecture.\n",
    "\n",
    "### How KATO Stores Patterns\n",
    "\n",
    "KATO internally uses two databases:\n",
    "- **ClickHouse**: Stores pattern structure (`pattern_data`, `length`, `tokens`, etc.)\n",
    "- **Redis**: Stores metadata (`frequency`, `emotives`, additional context)\n",
    "\n",
    "### Using KATO's API\n",
    "\n",
    "Instead of directly accessing databases, we use KATO's API:\n",
    "\n",
    "```python\n",
    "# Get a pattern via KATO API\n",
    "result = node.get_pattern(pattern_name)\n",
    "\n",
    "# Response structure:\n",
    "{\n",
    "    \"pattern\": {\n",
    "        \"status\": \"okay\",\n",
    "        \"pattern\": {\n",
    "            \"name\": \"abc123...\",\n",
    "            \"pattern_data\": [[\"child1\"], [\"child2\"], ...],\n",
    "            \"length\": 8,\n",
    "            \"frequency\": 42,\n",
    "            \"metadata\": {...},\n",
    "            // Combined ClickHouse + Redis data\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Pattern Data Structure\n",
    "\n",
    "For **node0**, `pattern_data` contains actual tokens: `[[\"The\"], [\" cat\"], [\" sat\"]]`\n",
    "\n",
    "For **higher levels**, `pattern_data` contains child pattern names from the level below.\n",
    "\n",
    "**Benefits of using KATO's API**:\n",
    "- Combines data from both storage systems\n",
    "- Handles errors and missing patterns gracefully\n",
    "- Maintains abstraction - storage can change without breaking code\n",
    "- Provides consistent interface across all levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTION - Pattern Name Prefix Handling\n",
    "# ============================================================================\n",
    "\n",
    "def strip_pattern_prefix(pattern_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip 'PTRN|' prefix from pattern name if present.\n",
    "    \n",
    "    MongoDB stores pattern names WITHOUT the 'PTRN|' prefix in the 'name' field,\n",
    "    but pattern references in pattern_data and KATO predictions may include it.\n",
    "    This function ensures consistent lookups.\n",
    "    \n",
    "    Args:\n",
    "        pattern_name: Pattern name, possibly with 'PTRN|' prefix\n",
    "        \n",
    "    Returns:\n",
    "        Pattern name without 'PTRN|' prefix\n",
    "        \n",
    "    Examples:\n",
    "        >>> strip_pattern_prefix('PTRN|abc123')\n",
    "        'abc123'\n",
    "        >>> strip_pattern_prefix('abc123')\n",
    "        'abc123'\n",
    "    \"\"\"\n",
    "    if pattern_name.startswith('PTRN|'):\n",
    "        return pattern_name[5:]  # Remove 'PTRN|' (5 characters)\n",
    "    return pattern_name\n",
    "\n",
    "print(\"✓ Helper function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ STM inspection helper defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTION - STM Inspection for Debugging\n",
    "# ============================================================================\n",
    "\n",
    "def inspect_stm(node, node_name: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Get and display current STM (Short-Term Memory) state.\n",
    "    \n",
    "    This helper is useful for debugging cascade issues - shows how many\n",
    "    events are in STM and previews the content.\n",
    "    \n",
    "    Args:\n",
    "        node: KATOClient instance\n",
    "        node_name: Display name (e.g., \"node0\", \"node1\")\n",
    "        verbose: Whether to print details\n",
    "        \n",
    "    Returns:\n",
    "        List of STM events\n",
    "        \n",
    "    Example:\n",
    "        >>> inspect_stm(node1, \"node1\")\n",
    "        node1 STM: 3 events\n",
    "          STM: ['PTRN|abc', 'PTRN|def', 'PTRN|xyz']\n",
    "    \"\"\"\n",
    "    stm_data = node.get_stm()\n",
    "    stm = stm_data.get('stm', [])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  {node_name} STM: {len(stm)} events\")\n",
    "        if len(stm) > 0:\n",
    "            print(f\"     STM: {stm}\")    \n",
    "    return stm\n",
    "\n",
    "print(\"✓ STM inspection helper defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ activate_hierarchy() function defined\n"
     ]
    }
   ],
   "source": [
    "def activate_hierarchy(\n",
    "    input_text: str,\n",
    "    verbose: bool = True,\n",
    "    recall_threshold_overrides: Dict[str, Dict[str, Any]] = None,\n",
    "    chunk_sizes: List[int] = None,\n",
    "    max_predictions: List[int] = None\n",
    ") -> Dict[int, Dict]:\n",
    "    \"\"\"\n",
    "    Activate hierarchical KATO system with input text (bottom-up).\n",
    "    \n",
    "    CORRECTED FLOW: For each chunk, cascade predictions through ALL levels.\n",
    "    \n",
    "    For each chunk:\n",
    "        1. Clear node0 STM\n",
    "        2. Observe chunk at node0 → get predictions\n",
    "        3. If predictions exist → observe at node1 → get predictions\n",
    "        4. If predictions exist → observe at node2 → get predictions\n",
    "        5. If predictions exist → observe at node3 → get predictions\n",
    "    \n",
    "    Higher levels (node1, node2, node3) accumulate events across chunks.\n",
    "    \n",
    "    CRITICAL: Pattern names must include \"PTRN|\" prefix when sent to higher levels\n",
    "    to match training format.\n",
    "    \n",
    "    Args:\n",
    "        input_text: User input text\n",
    "        verbose: Print intermediate results (including STM inspection)\n",
    "        recall_threshold_overrides: Per-node recall thresholds\n",
    "        chunk_sizes: Override CHUNK_SIZES for testing\n",
    "        max_predictions: Override MAX_PREDICTIONS for testing\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping level → predictions (from last chunk's cascade)\n",
    "    \"\"\"\n",
    "    # Use global configs if not overridden\n",
    "    if chunk_sizes is None:\n",
    "        chunk_sizes = CHUNK_SIZES\n",
    "    if max_predictions is None:\n",
    "        max_predictions = MAX_PREDICTIONS\n",
    "    \n",
    "    # Validate\n",
    "    if len(chunk_sizes) != len(nodes):\n",
    "        raise ValueError(f\"chunk_sizes must have {len(nodes)} elements, got {len(chunk_sizes)}\")\n",
    "    if len(max_predictions) != len(nodes):\n",
    "        raise ValueError(f\"max_predictions must have {len(nodes)} elements, got {len(max_predictions)}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Input: {input_text}\")\n",
    "        print(f\"Config: chunk_sizes={chunk_sizes}, max_pred={max_predictions}\")\n",
    "    \n",
    "    # Apply recall_threshold overrides if specified\n",
    "    if recall_threshold_overrides:\n",
    "        if verbose:\n",
    "            print(f\"\\n--- APPLYING RECALL_THRESHOLD OVERRIDES ---\")\n",
    "        \n",
    "        for node_name, genes in recall_threshold_overrides.items():\n",
    "            node_idx = int(node_name.replace('node', ''))\n",
    "            nodes[node_idx].update_genes(genes)\n",
    "            \n",
    "            if verbose and 'recall_threshold' in genes:\n",
    "                print(f\"✓ Updated {node_name} recall_threshold: {genes['recall_threshold']}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "    \n",
    "    # Step 1: Tokenize input text\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    if verbose:\n",
    "        print(f\"\\nTokens ({len(tokens)}): {tokens}\")\n",
    "    \n",
    "    # Step 2: Chunk tokens using chunk_sizes[0] (node0's chunk size)\n",
    "    chunks = tokenizer.chunk_tokens(tokens, chunk_sizes[0])\n",
    "    if verbose:\n",
    "        print(f\"\\nChunks ({len(chunks)}) with chunk_size={chunk_sizes[0]}:\")\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            print(f\"  Chunk {idx}: {chunk}\")\n",
    "    \n",
    "    # Clear all nodes' STM before starting\n",
    "    if verbose:\n",
    "        print(f\"\\n--- INITIALIZING: Clearing all nodes' STM ---\")\n",
    "    for i, node in enumerate(nodes):\n",
    "        node.clear_stm()\n",
    "        if verbose:\n",
    "            print(f\"✓ Cleared node{i} STM\")\n",
    "    \n",
    "    # Track predictions from last cascade (for return value)\n",
    "    predictions_0 = {'predictions': []}\n",
    "    predictions_1 = {'predictions': []}\n",
    "    predictions_2 = {'predictions': []}\n",
    "    predictions_3 = {'predictions': []}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PROCESS EACH CHUNK: CASCADE THROUGH ALL LEVELS\n",
    "    # ========================================================================\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"CHUNK {chunk_idx + 1}/{len(chunks)}: {chunk}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        # Clear node0 STM for new chunk\n",
    "        node0.clear_stm()\n",
    "        if verbose:\n",
    "            print(f\"✓ Cleared node0 STM for new chunk\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NODE0: Observe chunk → Get predictions\n",
    "        # ====================================================================\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- NODE0 ---\")\n",
    "        \n",
    "        # Build observations: ONE EVENT PER TOKEN\n",
    "        observations = [{'strings': [token]} for token in chunk]\n",
    "        \n",
    "        # Send chunk to node0\n",
    "        node0.observe_sequence(observations=observations, learn_at_end=False)\n",
    "        if verbose:\n",
    "            print(f\"✓ Observed {len(chunk)} tokens\")\n",
    "            inspect_stm(node0, \"node0\")\n",
    "        \n",
    "        # Get predictions from node0\n",
    "        predictions_0 = node0.get_predictions()\n",
    "        num_preds_0 = len(predictions_0.get('predictions', []))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Got {num_preds_0} predictions\")\n",
    "            if num_preds_0 > 0:\n",
    "                print(f\"  Sample predictions:\")\n",
    "                for i, pred in enumerate(predictions_0['predictions'][:3]):\n",
    "                    print(f\"    {i+1}. {pred['name'][:50]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "            else:\n",
    "                print(f\"  ⚠ No predictions from node0\")\n",
    "                print(f\"     Possible reasons:\")\n",
    "                print(f\"       - No patterns in KB match this token sequence\")\n",
    "                print(f\"       - recall_threshold too high (current: {RECALL_THRESHOLD_DEFAULT})\")\n",
    "        \n",
    "        if num_preds_0 == 0:\n",
    "            if verbose:\n",
    "                print(f\"⚠ No predictions from node0 - stopping cascade for this chunk\")\n",
    "            continue  # No predictions, can't cascade further\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NODE1: Observe node0 predictions → Get predictions\n",
    "        # ====================================================================\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- NODE1 ---\")\n",
    "        \n",
    "        # Send node0 predictions as ONE event to node1\n",
    "        # CRITICAL: Prepend \"PTRN|\" to match training format!\n",
    "        pattern_names_0 = [f\"PTRN|{pred['name']}\" for pred in predictions_0['predictions']]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Sending {len(pattern_names_0)} pattern names as 1 event:\")\n",
    "            for i, name in enumerate(pattern_names_0[:3]):\n",
    "                print(f\"    {i+1}. {name[:50]}...\")\n",
    "        \n",
    "        node1.observe(strings=pattern_names_0)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Observed (1 event)\")\n",
    "            inspect_stm(node1, \"node1\")\n",
    "        \n",
    "        # Get predictions from node1\n",
    "        predictions_1 = node1.get_predictions()\n",
    "        num_preds_1 = len(predictions_1.get('predictions', []))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Got {num_preds_1} predictions\")\n",
    "            if num_preds_1 > 0:\n",
    "                print(f\"  Sample predictions:\")\n",
    "                for i, pred in enumerate(predictions_1['predictions'][:3]):\n",
    "                    print(f\"    {i+1}. {pred['name'][:50]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "            else:\n",
    "                stm = node1.get_stm().get('stm', [])\n",
    "                print(f\"  ⚠ No predictions from node1\")\n",
    "                print(f\"     STM has {len(stm)} event(s)\")\n",
    "                if len(stm) > 0:\n",
    "                    print(f\"     First event: {stm[0][:3]}... ({len(stm[0])} symbols)\")\n",
    "                print(f\"     Possible reasons:\")\n",
    "                print(f\"       - No node1 patterns in KB match this sequence of node0 patterns\")\n",
    "                print(f\"       - recall_threshold too high (current: {RECALL_THRESHOLD_DEFAULT})\")\n",
    "                print(f\"       - Pattern names don't match KB format\")\n",
    "        \n",
    "        if num_preds_1 == 0:\n",
    "            if verbose:\n",
    "                print(f\"⚠ No predictions from node1 - stopping cascade for this chunk\")\n",
    "            continue  # No predictions, can't cascade further\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NODE2: Observe node1 predictions → Get predictions\n",
    "        # ====================================================================\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- NODE2 ---\")\n",
    "        \n",
    "        # Send node1 predictions as ONE event to node2\n",
    "        # CRITICAL: Prepend \"PTRN|\" to match training format!\n",
    "        pattern_names_1 = [f\"PTRN|{pred['name']}\" for pred in predictions_1['predictions']]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Sending {len(pattern_names_1)} pattern names as 1 event:\")\n",
    "            for i, name in enumerate(pattern_names_1[:3]):\n",
    "                print(f\"    {i+1}. {name[:50]}...\")\n",
    "        \n",
    "        node2.observe(strings=pattern_names_1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Observed (1 event)\")\n",
    "            inspect_stm(node2, \"node2\")\n",
    "        \n",
    "        # Get predictions from node2\n",
    "        predictions_2 = node2.get_predictions()\n",
    "        num_preds_2 = len(predictions_2.get('predictions', []))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Got {num_preds_2} predictions\")\n",
    "            if num_preds_2 > 0:\n",
    "                print(f\"  Sample predictions:\")\n",
    "                for i, pred in enumerate(predictions_2['predictions'][:3]):\n",
    "                    print(f\"    {i+1}. {pred['name'][:50]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "            else:\n",
    "                stm = node2.get_stm().get('stm', [])\n",
    "                print(f\"  ⚠ No predictions from node2\")\n",
    "                print(f\"     STM has {len(stm)} event(s)\")\n",
    "                if len(stm) > 0:\n",
    "                    print(f\"     First event: {stm[0][:3]}... ({len(stm[0])} symbols)\")\n",
    "                print(f\"     Possible reasons:\")\n",
    "                print(f\"       - No node2 patterns in KB match this sequence\")\n",
    "                print(f\"       - recall_threshold too high\")\n",
    "        \n",
    "        if num_preds_2 == 0:\n",
    "            if verbose:\n",
    "                print(f\"⚠ No predictions from node2 - stopping cascade for this chunk\")\n",
    "            continue  # No predictions, can't cascade further\n",
    "        \n",
    "        # ====================================================================\n",
    "        # NODE3: Observe node2 predictions → Get predictions\n",
    "        # ====================================================================\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- NODE3 ---\")\n",
    "        \n",
    "        # Send node2 predictions as ONE event to node3\n",
    "        # CRITICAL: Prepend \"PTRN|\" to match training format!\n",
    "        pattern_names_2 = [f\"PTRN|{pred['name']}\" for pred in predictions_2['predictions']]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Sending {len(pattern_names_2)} pattern names as 1 event:\")\n",
    "            for i, name in enumerate(pattern_names_2[:3]):\n",
    "                print(f\"    {i+1}. {name[:50]}...\")\n",
    "        \n",
    "        node3.observe(strings=pattern_names_2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Observed (1 event)\")\n",
    "            inspect_stm(node3, \"node3\")\n",
    "        \n",
    "        # Get predictions from node3\n",
    "        predictions_3 = node3.get_predictions()\n",
    "        num_preds_3 = len(predictions_3.get('predictions', []))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Got {num_preds_3} predictions\")\n",
    "            if num_preds_3 > 0:\n",
    "                print(f\"  Sample predictions:\")\n",
    "                for i, pred in enumerate(predictions_3['predictions'][:3]):\n",
    "                    print(f\"    {i+1}. {pred['name'][:50]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "            else:\n",
    "                stm = node3.get_stm().get('stm', [])\n",
    "                print(f\"  ⚠ No predictions from node3\")\n",
    "                print(f\"     STM has {len(stm)} event(s)\")\n",
    "                if len(stm) > 0:\n",
    "                    print(f\"     First event: {stm[0][:3]}... ({len(stm[0])} symbols)\")\n",
    "    \n",
    "    # Return predictions from last chunk's cascade\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"ACTIVATION COMPLETE\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nFinal prediction counts (from last chunk cascade):\")\n",
    "        print(f\"  node0: {len(predictions_0.get('predictions', []))} predictions\")\n",
    "        print(f\"  node1: {len(predictions_1.get('predictions', []))} predictions\")\n",
    "        print(f\"  node2: {len(predictions_2.get('predictions', []))} predictions\")\n",
    "        print(f\"  node3: {len(predictions_3.get('predictions', []))} predictions\")\n",
    "    \n",
    "    return {\n",
    "        0: predictions_0,\n",
    "        1: predictions_1,\n",
    "        2: predictions_2,\n",
    "        3: predictions_3\n",
    "    }\n",
    "\n",
    "print(\"✓ activate_hierarchy() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Hierarchical Activation Flow (CORRECTED)\n",
    "\n",
    "#### Key Principle: Per-Chunk Cascading\n",
    "\n",
    "**CRITICAL**: For EACH chunk, predictions cascade through ALL hierarchical levels immediately.\n",
    "\n",
    "```\n",
    "Chunk 1 Processing:\n",
    "  Tokens → node0 → predictions → node1 → predictions → node2 → predictions → node3\n",
    "\n",
    "Chunk 2 Processing:\n",
    "  Tokens → node0 → predictions → node1 → predictions → node2 → predictions → node3\n",
    "\n",
    "Chunk 3 Processing:\n",
    "  Tokens → node0 → predictions → node1 → predictions → node2 → predictions → node3\n",
    "```\n",
    "\n",
    "**NOT** (this would be wrong):\n",
    "```\n",
    "❌ All chunks → node0 → THEN all to node1 → THEN all to node2 → THEN all to node3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Detailed Flow for Each Chunk\n",
    "\n",
    "```python\n",
    "# For each chunk in input:\n",
    "\n",
    "1. Clear node0 STM (fresh processing for new chunk)\n",
    "2. node0.observe(chunk_tokens) → get_predictions()\n",
    "   └─ If predictions exist:\n",
    "3.     node1.observe(node0_pattern_names) → get_predictions()\n",
    "       └─ If predictions exist:\n",
    "4.         node2.observe(node1_pattern_names) → get_predictions()\n",
    "           └─ If predictions exist:\n",
    "5.             node3.observe(node2_pattern_names) → get_predictions()\n",
    "\n",
    "# Higher levels (node1, node2, node3) accumulate events across chunks\n",
    "# node0 is cleared between chunks to match training\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters\n",
    "\n",
    "**During Training:**\n",
    "```\n",
    "Chunk 1: Tokens → node0 learns → Sends 1 pattern name to node1\n",
    "Chunk 2: Tokens → node0 learns → Sends 1 pattern name to node1\n",
    "Chunk 3: Tokens → node0 learns → Sends 1 pattern name to node1\n",
    "...\n",
    "node1 sees sequence: [name1, name2, name3, ...] → learns patterns\n",
    "```\n",
    "\n",
    "**During Generation (must match):**\n",
    "```\n",
    "Chunk 1: Tokens → node0 predicts → Sends N pattern names to node1 (ensemble)\n",
    "Chunk 2: Tokens → node0 predicts → Sends N pattern names to node1 (ensemble)\n",
    "Chunk 3: Tokens → node0 predicts → Sends N pattern names to node1 (ensemble)\n",
    "...\n",
    "node1 sees sequence: [ensemble1, ensemble2, ensemble3, ...] → predicts\n",
    "```\n",
    "\n",
    "**Key Insight**: \n",
    "- Each chunk cascades immediately through all levels\n",
    "- Higher levels accumulate events from multiple chunks\n",
    "- node0 is cleared per-chunk (matches training)\n",
    "- node1/node2/node3 maintain their STM across chunks (accumulate context)\n",
    "\n",
    "---\n",
    "\n",
    "#### Example: 3 Chunks\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the mat\" \n",
    "Chunks: [\"The cat sat\", \"on the mat\"]\n",
    "\n",
    "Chunk 1: [\"The\", \"cat\", \"sat\"]\n",
    "  node0: observe → predict [A, B, C]\n",
    "  node1: observe [A, B, C] → predict [X, Y]\n",
    "  node2: observe [X, Y] → predict [P]\n",
    "  node3: observe [P] → predict [M]\n",
    "  \n",
    "Chunk 2: [\"on\", \"the\", \"mat\"]\n",
    "  node0: clear, observe → predict [D, E, F]\n",
    "  node1: observe [D, E, F] → predict [W, Z]  # STM now has 2 events\n",
    "  node2: observe [W, Z] → predict [Q]        # STM now has 2 events\n",
    "  node3: observe [Q] → predict [N]            # STM now has 2 events\n",
    "\n",
    "Final predictions returned: node0=[D,E,F], node1=[W,Z], node2=[Q], node3=[N]\n",
    "```\n",
    "\n",
    "**Why clear node0 but not others?**\n",
    "- node0 processes fixed-length chunks (must match training chunk size)\n",
    "- node1+ accumulate sequences over time (match training's paragraph/chapter/book accumulation)\n",
    "\n",
    "---\n",
    "\n",
    "#### Stopping Cascade Early\n",
    "\n",
    "If any level returns 0 predictions, the cascade stops for that chunk:\n",
    "\n",
    "```\n",
    "Chunk 1:\n",
    "  node0 → 0 predictions ⚠ STOP (no cascade to higher levels)\n",
    "\n",
    "Chunk 2:\n",
    "  node0 → [A, B]\n",
    "  node1 → 0 predictions ⚠ STOP (no cascade to node2/node3)\n",
    "```\n",
    "\n",
    "This prevents propagating \"no match\" signals up the hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2: Prediction Ensemble with Fallback\n",
    "\n",
    "**Purpose**: Select predictions from the highest level that has results **with usable 'future' data**.\n",
    "\n",
    "**Fallback Logic**:\n",
    "1. Try node3 first (book-level context)\n",
    "2. If empty or no 'future' data, try node2 (chapter-level context)\n",
    "3. If empty or no 'future' data, try node1 (paragraph-level context)\n",
    "4. If empty or no 'future' data, try node0 (chunk-level context)\n",
    "\n",
    "**IMPORTANT**: Only predictions with **non-empty 'future' field** can be used for generation!\n",
    "\n",
    "**Why 'future' validation?**: \n",
    "\n",
    "KATO predictions contain:\n",
    "- `'name'`: The matched pattern (what was recognized)\n",
    "- `'future'`: Predicted next symbols/tokens (what comes next)\n",
    "\n",
    "For text generation, we need the `'future'` field. Without it, we can't generate predictions - we'd just be unraveling the matched pattern (regenerating training data).\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Prediction WITH 'future' (usable ✓)\n",
    "{\n",
    "    'name': 'PTRN|abc123',\n",
    "    'future': [['PTRN|def456'], ['PTRN|ghi789']],  # ✓ Can generate from this\n",
    "    'confidence': 0.85\n",
    "}\n",
    "\n",
    "# Prediction WITHOUT 'future' (unusable ❌)\n",
    "{\n",
    "    'name': 'PTRN|abc123',\n",
    "    'future': [],  # ❌ Empty - nothing to generate\n",
    "    'confidence': 0.85\n",
    "}\n",
    "```\n",
    "\n",
    "**Why fallback?**: Novel input may not activate high-level patterns, or high-level patterns may not have 'future' data. Fallback ensures we find predictions with usable 'future' field at some level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ get_prediction_ensemble() function defined\n"
     ]
    }
   ],
   "source": [
    "def get_prediction_ensemble(\n",
    "    all_predictions: Dict[int, Dict],\n",
    "    fallback_levels: List[int] = [3, 2, 1, 0],\n",
    "    verbose: bool = True\n",
    ") -> Tuple[List[Dict], int]:\n",
    "    \"\"\"\n",
    "    Get prediction ensemble from highest available level with usable future data.\n",
    "    \n",
    "    Tries each level in fallback_levels order, returning predictions\n",
    "    from the first level that has non-empty 'future' fields.\n",
    "    \n",
    "    IMPORTANT: Only predictions with non-empty 'future' field can be used\n",
    "    for text generation. The 'future' field contains the predicted next\n",
    "    pattern names or tokens.\n",
    "    \n",
    "    Args:\n",
    "        all_predictions: Dictionary from activate_hierarchy()\n",
    "        fallback_levels: Levels to try in order (default: [3, 2, 1, 0])\n",
    "        verbose: Print which level was used\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (predictions list, level used)\n",
    "    \"\"\"\n",
    "    for level in fallback_levels:\n",
    "        predictions = all_predictions.get(level, {}).get('predictions', [])\n",
    "        \n",
    "        if len(predictions) > 0:\n",
    "            # Filter predictions with non-empty 'future' field\n",
    "            valid_predictions = [\n",
    "                pred for pred in predictions\n",
    "                if pred.get('future') and len(pred.get('future', [])) > 0\n",
    "            ]\n",
    "            \n",
    "            if len(valid_predictions) > 0:\n",
    "                if verbose:\n",
    "                    print(f\"\\n✓ Using node{level} predictions ({len(valid_predictions)} patterns with usable future data)\")\n",
    "                    if len(valid_predictions) < len(predictions):\n",
    "                        filtered = len(predictions) - len(valid_predictions)\n",
    "                        print(f\"  (Filtered out {filtered} predictions with empty 'future' field)\")\n",
    "                return valid_predictions, level\n",
    "            elif verbose:\n",
    "                print(f\"\\n⚠ node{level} has {len(predictions)} predictions but all have empty 'future' fields\")\n",
    "    \n",
    "    # No predictions at any level with usable future data\n",
    "    if verbose:\n",
    "        print(\"\\n⚠ No predictions with non-empty 'future' field at any level\")\n",
    "        print(\"   (Input may be novel, or predictions don't contain future data)\")\n",
    "    \n",
    "    return [], -1\n",
    "\n",
    "print(\"✓ get_prediction_ensemble() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3: Recursive Pattern Unraveling\n",
    "\n",
    "**Purpose**: Recursively unravel a pattern from high-level → tokens (top-down).\n",
    "\n",
    "**How it works**:\n",
    "- **Base case** (level 0): Pattern contains tokens → return them\n",
    "- **Recursive case** (level > 0): Pattern contains child pattern names → unravel each child\n",
    "\n",
    "**MongoDB Query**: This function shows the **exact MongoDB query** used to retrieve pattern structure:\n",
    "```python\n",
    "pattern_doc = db.find_one({'name': pattern_name})\n",
    "pattern_data = pattern_doc['pattern_data']\n",
    "```\n",
    "\n",
    "**Pattern Data Structure**:\n",
    "- `pattern_data = [[\"child1\"], [\"child2\"], [\"child3\"]]`\n",
    "- Each element is an event (list)\n",
    "- For node0: events contain tokens\n",
    "- For higher levels: events contain child pattern names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_pattern(\n",
    "    pattern_name: str,\n",
    "    level: int,\n",
    "    nodes: List[KATOClient],\n",
    "    verbose: bool = False,\n",
    "    indent: int = 0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively unravel a pattern to tokens (top-down traversal).\n",
    "    \n",
    "    This function takes a pattern from any level (1-3) and recursively unravels it\n",
    "    down to the base tokens. Each pattern represents 15 child patterns (fixed chunking),\n",
    "    and at the bottom (node0), each pattern contains 15 tokens.\n",
    "    \n",
    "    Args:\n",
    "        pattern_name: Pattern identifier (e.g., \"PTRN|abc123...\" or \"abc123...\")\n",
    "        level: Which hierarchical level this pattern is from (0-3)\n",
    "        nodes: List of KATOClient instances for each level\n",
    "        verbose: Print unraveling steps\n",
    "        indent: Indentation level for verbose output\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens that the pattern expands to\n",
    "    \n",
    "    Example:\n",
    "        >>> tokens = unravel_pattern(\"PTRN|book_xyz\", level=3, \n",
    "        ...                          nodes=nodes)\n",
    "    \"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    \n",
    "    # Clean the pattern name (remove PTRN| prefix if present)\n",
    "    clean_name = strip_pattern_prefix(pattern_name)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}[Level {level}] Unraveling: {clean_name[:16]}...\")\n",
    "    \n",
    "    # Query pattern from KATO API\n",
    "    result = nodes[level].get_pattern(clean_name)\n",
    "    inner = result.get('pattern', {})\n",
    "    pattern_ch = inner.get('pattern') if inner.get('status') == 'okay' else None\n",
    "    \n",
    "    if pattern_ch is None:\n",
    "        if verbose:\n",
    "            print(f\"{prefix}  ⚠️ Pattern not found, returning empty\")\n",
    "        return []\n",
    "    \n",
    "    # For node0 patterns, return tokens directly\n",
    "    if level == 0:\n",
    "        # Extract tokens from the observations\n",
    "        tokens = []\n",
    "        observations = pattern_ch.get('observations', [])\n",
    "        for obs in observations:\n",
    "            token_list = obs.get('strings', [])\n",
    "            tokens.extend(token_list)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{prefix}  → {len(tokens)} tokens\")\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    # For higher levels, recurse on each child pattern\n",
    "    all_tokens = []\n",
    "    observations = pattern_ch.get('observations', [])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}  Found {len(observations)} child patterns\")\n",
    "    \n",
    "    for obs in observations:\n",
    "        strings = obs.get('strings', [])\n",
    "        \n",
    "        # Child patterns may also have 'PTRN|' prefix - handle properly\n",
    "        for child_pattern in strings:\n",
    "            if verbose:\n",
    "                clean_child = strip_pattern_prefix(child_pattern)\n",
    "                print(f\"{prefix}  Child: {clean_child[:16]}...\")\n",
    "            \n",
    "            # Recursively unravel the child pattern\n",
    "            child_tokens = unravel_pattern(\n",
    "                child_pattern,\n",
    "                level - 1,\n",
    "                nodes=nodes,\n",
    "                verbose=verbose,\n",
    "                indent=indent + 1\n",
    "            )\n",
    "            all_tokens.extend(child_tokens)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}  Total: {len(all_tokens)} tokens\")\n",
    "    \n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_future_list(\n",
    "    future_list: List,\n",
    "    future_level: int,\n",
    "    nodes: List[KATOClient],\n",
    "    verbose: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Unravel a list of future patterns to tokens.\n",
    "    \n",
    "    Unlike the complex unravel_prediction_with_context that was causing topic mixing,\n",
    "    this function ONLY unravels the direct 'future' field from the selected prediction.\n",
    "    It does NOT traverse to other predictions or collect all futures recursively.\n",
    "    \n",
    "    Args:\n",
    "        future_list: List of future patterns from prediction['future']\n",
    "        future_level: Level at which these future patterns exist (0-3)\n",
    "        nodes: List of KATOClient instances for each level\n",
    "        verbose: Print detailed unraveling steps\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens from unraveling the future patterns\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"  Unraveling {len(future_list)} future events from level {future_level}\")\n",
    "    \n",
    "    all_tokens = []\n",
    "    \n",
    "    # Base case: If future_level is 0 or negative, items are already tokens\n",
    "    if future_level <= 0:\n",
    "        # These are already tokens, just extract them\n",
    "        for event in future_list:\n",
    "            if isinstance(event, list) and len(event) > 0:\n",
    "                all_tokens.append(event[0])\n",
    "            elif isinstance(event, str):\n",
    "                all_tokens.append(event)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    → Extracted {len(all_tokens)} tokens directly\")\n",
    "        \n",
    "        return all_tokens\n",
    "    \n",
    "    # Recursive case: future contains pattern names, unravel via KATO API\n",
    "    for event in future_list:\n",
    "        # Extract pattern name from event\n",
    "        # Future events can be: [\"pattern_name\"] or \"pattern_name\"\n",
    "        if isinstance(event, list) and len(event) > 0:\n",
    "            pattern_name = event[0]\n",
    "        elif isinstance(event, str):\n",
    "            pattern_name = event\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            clean_name = strip_pattern_prefix(pattern_name)\n",
    "            print(f\"  Unraveling future pattern: {clean_name[:16]}...\")\n",
    "        \n",
    "        # Unravel this pattern\n",
    "        tokens = unravel_pattern(\n",
    "            pattern_name,\n",
    "            level=future_level,\n",
    "            nodes=nodes,\n",
    "            verbose=verbose,\n",
    "            indent=2 if verbose else 0\n",
    "        )\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Total future tokens: {len(all_tokens)}\")\n",
    "    \n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ unravel_pattern() function defined (using KATO API)\n"
     ]
    }
   ],
   "source": [
    "def unravel_pattern(\n",
    "    pattern_name: str,\n",
    "    level: int,\n",
    "    nodes: List[KATOClient],\n",
    "    verbose: bool = False,\n",
    "    indent: int = 0\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively unravel a pattern to tokens using KATO's API (top-down traversal).\n",
    "    \n",
    "    This function uses KATO's get_pattern() API to retrieve pattern structure, then:\n",
    "    - If level 0 (node0): Return the tokens directly\n",
    "    - If level > 0: Recursively unravel each child pattern\n",
    "    \n",
    "    Args:\n",
    "        pattern_name: Pattern identifier (e.g., \"PTRN|abc123...\" or \"abc123...\")\n",
    "        level: Which hierarchical level this pattern is from (0-3)\n",
    "        nodes: List of KATOClient instances\n",
    "        verbose: Print unraveling steps\n",
    "        indent: Indentation level for verbose output\n",
    "        \n",
    "    Returns:\n",
    "        List of token strings\n",
    "        \n",
    "    Example:\n",
    "        >>> tokens = unravel_pattern(\"PTRN|book_xyz\", level=3, nodes=nodes)\n",
    "    \"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}Unraveling node{level}: {pattern_name[:60]}...\")\n",
    "    \n",
    "    # Strip 'PTRN|' prefix if present\n",
    "    clean_name = strip_pattern_prefix(pattern_name)\n",
    "    \n",
    "    # Get pattern via KATO API (not direct database!)\n",
    "    result = nodes[level].get_pattern(clean_name)\n",
    "    inner = result.get('pattern', {})\n",
    "    pattern_ch = inner.get('pattern') if inner.get('status') == 'okay' else None\n",
    "    \n",
    "    if not pattern_ch:\n",
    "        if verbose:\n",
    "            print(f\"{prefix}  ⚠ Pattern not found via KATO API\")\n",
    "            print(f\"{prefix}    Searched for: {clean_name}\")\n",
    "        return []\n",
    "    \n",
    "    pattern_data = pattern_ch.get('pattern_data', [])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{prefix}  Pattern has {len(pattern_data)} events\")\n",
    "    \n",
    "    # Base case: node0 patterns contain actual tokens\n",
    "    if level == 0:\n",
    "        # Extract tokens from events\n",
    "        # pattern_data = [[\"The\"], [\" cat\"], [\" sat\"]]\n",
    "        tokens = [event[0] for event in pattern_data if len(event) > 0]\n",
    "        \n",
    "        if verbose:\n",
    "            decoded = tokenizer.decode_tokens(tokens)\n",
    "            print(f\"{prefix}  → Tokens: {tokens[:10]}... → '{decoded[:50]}...'\")\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    # Recursive case: higher-level patterns contain child pattern names\n",
    "    else:\n",
    "        # Extract child pattern names from events and strip prefix\n",
    "        child_patterns = [\n",
    "            strip_pattern_prefix(event[0]) \n",
    "            for event in pattern_data \n",
    "            if len(event) > 0\n",
    "        ]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{prefix}  → Has {len(child_patterns)} children at node{level-1}\")\n",
    "        \n",
    "        # Recursively unravel each child pattern\n",
    "        all_tokens = []\n",
    "        for child_pattern in child_patterns:\n",
    "            # Recursive call: unravel child at level-1\n",
    "            child_tokens = unravel_pattern(\n",
    "                child_pattern,\n",
    "                level - 1,\n",
    "                nodes=nodes,\n",
    "                verbose=verbose,\n",
    "                indent=indent + 1\n",
    "            )\n",
    "            all_tokens.extend(child_tokens)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{prefix}  ✓ Unraveled to {len(all_tokens)} total tokens\")\n",
    "        \n",
    "        return all_tokens\n",
    "\n",
    "print(\"✓ unravel_pattern() function defined (using KATO API)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ generate_text() function defined (with PRESENT + FUTURE + FULL METRICS)\n"
     ]
    }
   ],
   "source": [
    "def extract_tokens_from_present(present_events, level=0, nodes=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Extract tokens from pred['present'] field (KATO event format).\n",
    "    \n",
    "    The 'present' field structure varies by hierarchical level:\n",
    "    - At node0: Contains actual tokens [['The'], ['cat'], ['sat']]\n",
    "    - At node1+: Contains pattern names [['PTRN|abc123'], ['PTRN|def456']]\n",
    "    \n",
    "    For higher levels, pattern names must be unraveled recursively to get tokens.\n",
    "    \n",
    "    This avoids using pred['name'] which returns the full stored pattern\n",
    "    (including future tokens from training time), causing repetition.\n",
    "    \n",
    "    Args:\n",
    "        present_events: List of KATO events from pred['present']\n",
    "        level: Hierarchical level (0 = node0, 1 = node1, etc.)\n",
    "        nodes: List of KATOClient instances (required for level > 0)\n",
    "        verbose: Print unraveling details\n",
    "    \n",
    "    Returns:\n",
    "        List of token strings\n",
    "    \"\"\"\n",
    "    if not present_events:\n",
    "        return []\n",
    "    \n",
    "    if level == 0:\n",
    "        # node0: Extract tokens directly (present contains actual tokens)\n",
    "        tokens = []\n",
    "        for event in present_events:\n",
    "            # Each event is a list of strings (could have anomalies)\n",
    "            # For token-level events, typically just one string per event\n",
    "            if event and len(event) > 0:\n",
    "                tokens.append(event[0])  # Take first string from event\n",
    "        return tokens\n",
    "    else:\n",
    "        # node1+: Unravel pattern names recursively (present contains pattern names)\n",
    "        # CRITICAL: pred['present'] at level N contains level N-1 patterns!\n",
    "        # E.g., node2 prediction's present contains node1 pattern names\n",
    "        # We unravel at level-1 because that's where the patterns actually exist\n",
    "        if nodes is None:\n",
    "            raise ValueError(\"nodes parameter required for hierarchical unraveling (level > 0)\")\n",
    "        \n",
    "        all_tokens = []\n",
    "        for event in present_events:\n",
    "            if event and len(event) > 0:\n",
    "                pattern_name = event[0]\n",
    "                \n",
    "                # Strip PTRN| prefix if present\n",
    "                if pattern_name.startswith('PTRN|'):\n",
    "                    pattern_name = pattern_name[5:]  # Remove 'PTRN|' prefix\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"    Unraveling present pattern: {pattern_name[:16]}... (from level {level-1})\")\n",
    "                \n",
    "                # Recursively unravel this pattern to tokens\n",
    "                # Pattern is from level-1 (the child level that was observed)\n",
    "                # level-1 is safe because we only reach this branch if level > 0\n",
    "                tokens = unravel_pattern(\n",
    "                    pattern_name,\n",
    "                    level=level-1,\n",
    "                    nodes=nodes,\n",
    "                    verbose=verbose,\n",
    "                    indent=2 if verbose else 0\n",
    "                )\n",
    "                \n",
    "                if tokens:\n",
    "                    all_tokens.extend(tokens)\n",
    "                elif verbose:\n",
    "                    print(f\"      Warning: Failed to unravel pattern {pattern_name[:16]}...\")\n",
    "        \n",
    "        return all_tokens\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    input_text: str,\n",
    "    max_predictions: int = 5,\n",
    "    verbose: bool = True,\n",
    "    verbose_unravel: bool = False,\n",
    "    recall_threshold_overrides: Dict[str, Dict[str, Any]] = None,\n",
    "    auto_adjust_recall: bool = False\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate text continuations using hierarchical KATO system.\n",
    "    \n",
    "    Complete pipeline:\n",
    "    1. Activate hierarchy with input text (bottom-up)\n",
    "    2. Get prediction ensemble from highest available level with non-empty 'future'\n",
    "    3. Extract BOTH 'present' (matched pattern) and 'future' (predicted next)\n",
    "    4. Unravel both present and future predictions to tokens (top-down)\n",
    "    5. Combine: present tokens + future tokens\n",
    "    6. Decode combined tokens to text\n",
    "    \n",
    "    Args:\n",
    "        input_text: User input to condition generation\n",
    "        max_predictions: Maximum number of predictions to generate\n",
    "        verbose: Print pipeline steps\n",
    "        verbose_unravel: Print detailed unraveling steps\n",
    "        recall_threshold_overrides: Optional per-node recall_threshold overrides\n",
    "            Format: {'node0': {'recall_threshold': 0.3}, 'node1': {'recall_threshold': 0.6}, ...}\n",
    "        auto_adjust_recall: If True, automatically lower recall_threshold if no predictions found\n",
    "        \n",
    "    Returns:\n",
    "        List of dicts with keys:\n",
    "            - 'text': Generated text string\n",
    "            - 'potential': Potential metric (used for ranking)\n",
    "            - 'confidence': Confidence metric\n",
    "            - 'bayesian_posterior': Bayesian posterior probability\n",
    "            - 'bayesian_likelihood': Bayesian likelihood\n",
    "            - 'evidence': Evidence metric\n",
    "            - 'similarity': Similarity metric\n",
    "            - 'snr': Signal-to-noise ratio\n",
    "            - 'predictive_information': Predictive information metric\n",
    "        \n",
    "    Example:\n",
    "        >>> # Generate with custom recall thresholds\n",
    "        >>> results = generate_text(\n",
    "        ...     \"The cat sat\",\n",
    "        ...     recall_threshold_overrides={\n",
    "        ...         'node0': {'recall_threshold': 0.3},\n",
    "        ...         'node1': {'recall_threshold': 0.6}\n",
    "        ...     }\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    print(f\"\\nInput: {input_text}\")\n",
    "    \n",
    "    # Step 1: Activate hierarchy (bottom-up)\n",
    "    all_predictions = activate_hierarchy(\n",
    "        input_text,\n",
    "        verbose=verbose,\n",
    "        recall_threshold_overrides=recall_threshold_overrides\n",
    "    )\n",
    "    \n",
    "    # Step 2: Get prediction ensemble with non-empty 'future' (with fallback)\n",
    "    predictions, used_level = get_prediction_ensemble(\n",
    "        all_predictions,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Auto-adjust recall_threshold if no predictions found\n",
    "    if not predictions and auto_adjust_recall:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"AUTO-ADJUSTING RECALL_THRESHOLD\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(\"⚠ No predictions with usable 'future' data.\")\n",
    "        print(\"Trying progressively lower thresholds...\\n\")\n",
    "        \n",
    "        # Try decreasing thresholds\n",
    "        for threshold in [0.5, 0.4, 0.3, 0.2, 0.1]:\n",
    "            print(f\"Trying recall_threshold={threshold}...\")\n",
    "            \n",
    "            # Create override dict for all nodes\n",
    "            override = {\n",
    "                f'node{i}': {'recall_threshold': threshold}\n",
    "                for i in range(4)\n",
    "            }\n",
    "            \n",
    "            # Clear all STMs before re-activating\n",
    "            # IMPORTANT: Must clear after changing recall_threshold!\n",
    "            for node in nodes:\n",
    "                node.clear_stm()\n",
    "            \n",
    "            # Re-activate hierarchy with new threshold\n",
    "            all_predictions = activate_hierarchy(\n",
    "                input_text,\n",
    "                verbose=False,  # Suppress verbose for retry attempts\n",
    "                recall_threshold_overrides=override\n",
    "            )\n",
    "            \n",
    "            # Check if we got predictions with usable future\n",
    "            predictions, used_level = get_prediction_ensemble(\n",
    "                all_predictions,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            if predictions:\n",
    "                print(f\"✓ Found {len(predictions)} predictions with usable 'future' at recall_threshold={threshold}\\n\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "                if verbose:\n",
    "                    print(f\"✓ Using node{used_level} predictions ({len(predictions)} patterns)\")\n",
    "                break\n",
    "        \n",
    "        if not predictions:\n",
    "            print(\"⚠ No predictions with usable 'future' found even with minimum recall_threshold (0.1)\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    if not predictions:\n",
    "        print(\"\\n⚠ No predictions with usable 'future' field available.\")\n",
    "        print(\"   (Input may be novel, or predictions don't contain future data)\")\n",
    "        return []\n",
    "    \n",
    "    # Limit to max_predictions\n",
    "    predictions = predictions[:max_predictions]\n",
    "    \n",
    "    # Step 3: Unravel BOTH 'present' (matched pattern) and 'future' fields\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TOP-DOWN UNRAVELING (Present + Future)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\")\n",
    "    print(f\"** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        present_events = pred.get('present', [])  # Matched sequence (KATO events)\n",
    "        future_list = pred.get('future', [])  # Predicted next patterns/tokens\n",
    "        \n",
    "        # Extract ALL available metrics from the prediction\n",
    "        metrics = {\n",
    "            'confidence': pred.get('confidence', 0.0),\n",
    "            'potential': pred.get('potential', 0.0),\n",
    "            'predictive_information': pred.get('predictive_information', 0.0),\n",
    "            'evidence': pred.get('evidence', 0.0),\n",
    "            'bayesian_posterior': pred.get('bayesian_posterior', 0.0),\n",
    "            'bayesian_likelihood': pred.get('bayesian_likelihood', 0.0),\n",
    "            'similarity': pred.get('similarity', 0.0),\n",
    "            'snr': pred.get('snr', 0.0),\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nPrediction {i+1}/{len(predictions)}:\")\n",
    "        print(f\"  Matched Pattern: {pred['name'][:60]}...\")\n",
    "        print(f\"  Level: node{used_level}\")\n",
    "        print(f\"  Present Events: {len(present_events)} tokens\")\n",
    "        print(f\"  Future Events: {len(future_list)} events\")\n",
    "        \n",
    "        # Display all metrics\n",
    "        print(f\"\\n  Prediction Metrics:\")\n",
    "        print(f\"    Potential:              {metrics['potential']:.4f}\")\n",
    "        print(f\"    Confidence:             {metrics['confidence']:.4f}\")\n",
    "        print(f\"    Bayesian Posterior:     {metrics['bayesian_posterior']:.4f}\")\n",
    "        print(f\"    Bayesian Likelihood:    {metrics['bayesian_likelihood']:.4f}\")\n",
    "        print(f\"    Evidence:               {metrics['evidence']:.4f}\")\n",
    "        print(f\"    Similarity:             {metrics['similarity']:.4f}\")\n",
    "        print(f\"    SNR:                    {metrics['snr']:.4f}\")\n",
    "        print(f\"    Predictive Information: {metrics['predictive_information']:.4f}\")\n",
    "        \n",
    "        # First, extract the PRESENT tokens (unravel if from higher level)\n",
    "        if verbose_unravel:\n",
    "            print(f\"\\n  Extracting PRESENT tokens from pred['present'] field...\")\n",
    "        \n",
    "        present_tokens = extract_tokens_from_present(\n",
    "            present_events,\n",
    "            level=used_level,\n",
    "            nodes=nodes,\n",
    "            verbose=verbose_unravel\n",
    "        )\n",
    "        \n",
    "        if not present_tokens:\n",
    "            print(f\"  ⚠ Failed to unravel present pattern\")\n",
    "            # Even if present fails, try to get future\n",
    "        \n",
    "        # Then, unravel the FUTURE (predicted next patterns/tokens)\n",
    "        if not future_list:\n",
    "            print(f\"  ⚠ Empty 'future' field\")\n",
    "            if not present_tokens:\n",
    "                continue  # Skip if both present and future are empty\n",
    "            # If we have present but no future, just use present\n",
    "            tokens = present_tokens\n",
    "            future_tokens = []\n",
    "        else:\n",
    "            # Calculate future level\n",
    "            future_level = used_level - 1 if used_level > 0 else -1\n",
    "            \n",
    "            if verbose_unravel:\n",
    "                print(f\"\\n  Unraveling FUTURE (predicted next) from level {future_level}...\")\n",
    "            \n",
    "            future_tokens = unravel_future_list(\n",
    "                future_list,\n",
    "                future_level=future_level,\n",
    "                nodes=nodes,\n",
    "                verbose=verbose_unravel\n",
    "            )\n",
    "            \n",
    "            if not future_tokens:\n",
    "                print(f\"  ⚠ Failed to unravel future\")\n",
    "                # If future fails but we have present, use just present\n",
    "                if present_tokens:\n",
    "                    tokens = present_tokens\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                # Combine present + future tokens\n",
    "                tokens = present_tokens + future_tokens\n",
    "        \n",
    "        # Decode combined tokens to text\n",
    "        generated_text = tokenizer.decode_tokens(tokens)\n",
    "\n",
    "        _present = tokenizer.decode_tokens(present_tokens) if present_tokens else \"\"\n",
    "        _future = tokenizer.decode_tokens(future_tokens) if future_tokens else \"\"\n",
    "        \n",
    "        print(f\"\\n  ✓ Generated:\")\n",
    "        print(f\"     Present tokens: {len(present_tokens)}\")\n",
    "        print(f\"     Future tokens: {len(future_tokens)}\")\n",
    "        print(f\"     Total tokens: {len(tokens)}\")\n",
    "        print(f\"==\"*20)\n",
    "        print(f\"  Present: {_present}\")\n",
    "        print(f\"--\"*20)\n",
    "        print(f\"  Future: {_future}\")\n",
    "        print(f\"==\"*20)\n",
    "        print(f\"  Text preview: {generated_text}\")\n",
    "        \n",
    "        # Store result with all metrics\n",
    "        result_dict = {\n",
    "            'text': generated_text,\n",
    "            **metrics  # Unpack all metrics into the result dict\n",
    "        }\n",
    "        results.append(result_dict)\n",
    "    \n",
    "    # Sort by potential (primary ranking metric), fallback to confidence\n",
    "    results.sort(key=lambda x: x['potential'] if x['potential'] > 0 else x['confidence'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"GENERATION COMPLETE: {len(results)} results\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ generate_text() function defined (with PRESENT + FUTURE + FULL METRICS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input: The cat sat on the \n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input: The cat sat on the \n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (6): ['The', 'Ġcat', 'Ġsat', 'Ġon', 'Ġthe', 'Ġ']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['The', 'Ġcat', 'Ġsat', 'Ġon', 'Ġthe', 'Ġ']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['The', 'Ġcat', 'Ġsat', 'Ġon', 'Ġthe', 'Ġ']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 6 tokens\n",
      "  node0 STM: 6 events\n",
      "     STM: [['The'], ['Ġcat'], ['Ġsat'], ['Ġon'], ['Ġthe'], ['Ġ']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. e3ad92dd06caca4e0ef3156504e8400d64187c85... (conf: 1.000)\n",
      "    2. c0a6516eb75369b71dfd7b99e92bdac02a9d3068... (conf: 1.000)\n",
      "    3. 1ce236690c024936618b1f0a7022ed5064a5e420... (conf: 1.000)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|e3ad92dd06caca4e0ef3156504e8400d64187c85...\n",
      "    2. PTRN|c0a6516eb75369b71dfd7b99e92bdac02a9d3068...\n",
      "    3. PTRN|1ce236690c024936618b1f0a7022ed5064a5e420...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|1ce236690c024936618b1f0a7022ed5064a5e420', 'PTRN|2f4bcf681e1b0073c3e0971e295677f72d0df9b3', 'PTRN|36598f3a658e7c99b658548d8abae710025e3023', 'PTRN|7d32797819881a9dae56f55f23ae1785f7719a31', 'PTRN|ab1599f608e4c60225769411f392e8e54d468e13', 'PTRN|c0a6516eb75369b71dfd7b99e92bdac02a9d3068', 'PTRN|d08cce4508b906b9eec367238010fe92606fddf2', 'PTRN|d8b213a72d102e1bd4562be79bde7f986fd28207', 'PTRN|e3ad92dd06caca4e0ef3156504e8400d64187c85', 'PTRN|e4126c3a13f70f3c97402320ad8a3f0f015e193b']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. 5bd7a7d035deea7eb810d1b2545f6273300c8747... (conf: 1.000)\n",
      "    2. 906d23e40d02cadf2793b99de53cc4fb7f292548... (conf: 1.000)\n",
      "    3. e7ea51506d4ed54ac3c3bde94f806f7361017a27... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|5bd7a7d035deea7eb810d1b2545f6273300c8747...\n",
      "    2. PTRN|906d23e40d02cadf2793b99de53cc4fb7f292548...\n",
      "    3. PTRN|e7ea51506d4ed54ac3c3bde94f806f7361017a27...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|152e3c8dc61f68859469358ed7adc25ade59f8f3', 'PTRN|24a9f4f248c1eccc208489bf99f398cdb8c002f9', 'PTRN|37c18f3c1c5b2ae0863d51eb87e240a0113b146f', 'PTRN|48ac3f14de33a4a2ddb3d438617c5c25a86ce64c', 'PTRN|536bff1b919a64bc12801d4d30c75d55a30f4fe0', 'PTRN|5bd7a7d035deea7eb810d1b2545f6273300c8747', 'PTRN|5ea8482686a42588aac493d19aaca827c4ae4f8d', 'PTRN|906d23e40d02cadf2793b99de53cc4fb7f292548', 'PTRN|e7ea51506d4ed54ac3c3bde94f806f7361017a27', 'PTRN|fd4cef93ea04a028e11d30e191e809be4d9430ae']]\n",
      "✓ Got 8 predictions\n",
      "  Sample predictions:\n",
      "    1. 1b24fb2fd330b0d34938e0d301a137d252e9733b... (conf: 1.000)\n",
      "    2. 525231282744a56dec2cfacaa1f047f2609d0246... (conf: 1.000)\n",
      "    3. 47bcbe30de0558e2cd585dfff4cbd369e89b3b02... (conf: 1.000)\n",
      "\n",
      "--- NODE3 ---\n",
      "Sending 8 pattern names as 1 event:\n",
      "    1. PTRN|1b24fb2fd330b0d34938e0d301a137d252e9733b...\n",
      "    2. PTRN|525231282744a56dec2cfacaa1f047f2609d0246...\n",
      "    3. PTRN|47bcbe30de0558e2cd585dfff4cbd369e89b3b02...\n",
      "✓ Observed (1 event)\n",
      "  node3 STM: 1 events\n",
      "     STM: [['PTRN|1b24fb2fd330b0d34938e0d301a137d252e9733b', 'PTRN|32c6dfe8ceca1c2302ffeea648654df999ade886', 'PTRN|473606b91c7e9c71b39b7e865fb35465ee9dbb82', 'PTRN|47bcbe30de0558e2cd585dfff4cbd369e89b3b02', 'PTRN|525231282744a56dec2cfacaa1f047f2609d0246', 'PTRN|753e8caaee895f7e365b2a6d3f9e61a6ade7360c', 'PTRN|ef54514bf33b785041c0ac0749e9839a81957403', 'PTRN|fb7b39ad2706d00415c8b349d4abf32957caaa3b']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node3\n",
      "     STM has 1 event(s)\n",
      "     First event: ['PTRN|1b24fb2fd330b0d34938e0d301a137d252e9733b', 'PTRN|32c6dfe8ceca1c2302ffeea648654df999ade886', 'PTRN|473606b91c7e9c71b39b7e865fb35465ee9dbb82']... (8 symbols)\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 10 predictions\n",
      "  node1: 10 predictions\n",
      "  node2: 8 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "✓ Using node2 predictions (7 patterns with usable future data)\n",
      "  (Filtered out 1 predictions with empty 'future' field)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/5:\n",
      "  Matched Pattern: 1b24fb2fd330b0d34938e0d301a137d252e9733b...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 3 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.0795\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1135\n",
      "    Bayesian Likelihood:    0.1429\n",
      "    Evidence:               0.2500\n",
      "    Similarity:             0.1429\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 180\n",
      "     Total tokens: 244\n",
      "========================================\n",
      "  Present: Rhodes spent the next few months in London , seeking out supporters for his cause in the West End , the City and , occasionally , the rural estates of the landed gentry . These efforts yielded the public backing of the prominent imperialist Harry Johnston , Alexander Livingstone Bruce ( who sat on the board of the East Africa\n",
      "----------------------------------------\n",
      "  Future:  Company ) , and Lord Balfour of Burleigh , among others . Along with Grey 's active involvement and Lord Salisbury 's continuing favour , the weight of this opinion seemed to be reaping dividends for Rhodes by June 1889 . The amalgamation with the London syndicate was complete , and Whitehall appeared to have dropped its reservations regarding the Rudd Concession 's validity . Opposition to the charter in parliament and elsewhere had been for the most part silenced , and , with the help of Rhodes 's press contacts , prominently William Thomas Stead , editor of the Pall Mall Gazette , opinion in the media was starting to back the idea of a chartered company for south @-@ central Africa . But in June 1889 , just as the Colonial Office looked poised to grant the royal charter , Lobengula 's letter repudiating the Rudd Concession , written two months previously , arrived in London .\n",
      "========================================\n",
      "  Text preview: Rhodes spent the next few months in London , seeking out supporters for his cause in the West End , the City and , occasionally , the rural estates of the landed gentry . These efforts yielded the public backing of the prominent imperialist Harry Johnston , Alexander Livingstone Bruce ( who sat on the board of the East Africa Company ) , and Lord Balfour of Burleigh , among others . Along with Grey 's active involvement and Lord Salisbury 's continuing favour , the weight of this opinion seemed to be reaping dividends for Rhodes by June 1889 . The amalgamation with the London syndicate was complete , and Whitehall appeared to have dropped its reservations regarding the Rudd Concession 's validity . Opposition to the charter in parliament and elsewhere had been for the most part silenced , and , with the help of Rhodes 's press contacts , prominently William Thomas Stead , editor of the Pall Mall Gazette , opinion in the media was starting to back the idea of a chartered company for south @-@ central Africa . But in June 1889 , just as the Colonial Office looked poised to grant the royal charter , Lobengula 's letter repudiating the Rudd Concession , written two months previously , arrived in London .\n",
      "\n",
      "Prediction 2/5:\n",
      "  Matched Pattern: 525231282744a56dec2cfacaa1f047f2609d0246...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 2 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.0265\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1538\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.1538\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 89\n",
      "     Total tokens: 153\n",
      "========================================\n",
      "  Present: The table on the right lists the ten longest tributaries of the Missouri , along with their respective catchment areas and flows . Length is measured to the hydrologic source , regardless of naming convention . The main stem of the Kansas River , for example , is 148 miles ( 238 km ) long . However , including\n",
      "----------------------------------------\n",
      "  Future:  the longest headwaters tributaries , the 453 @-@ mile ( 729 km ) Republican River and the 156 @-@ mile ( 251 km ) Arikaree River , brings the total length to 749 miles ( 1 @,@ 205 km ) . Similar naming issues are encountered with the Platte River , whose longest tributary , the North Platte River , is more than twice as long as its mainstream .\n",
      "========================================\n",
      "  Text preview: The table on the right lists the ten longest tributaries of the Missouri , along with their respective catchment areas and flows . Length is measured to the hydrologic source , regardless of naming convention . The main stem of the Kansas River , for example , is 148 miles ( 238 km ) long . However , including the longest headwaters tributaries , the 453 @-@ mile ( 729 km ) Republican River and the 156 @-@ mile ( 251 km ) Arikaree River , brings the total length to 749 miles ( 1 @,@ 205 km ) . Similar naming issues are encountered with the Platte River , whose longest tributary , the North Platte River , is more than twice as long as its mainstream .\n",
      "\n",
      "Prediction 3/5:\n",
      "  Matched Pattern: 32c6dfe8ceca1c2302ffeea648654df999ade886...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 2 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.0265\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1538\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.1538\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 128\n",
      "     Total tokens: 192\n",
      "========================================\n",
      "  Present: The stands on the east side of the field are generally reserved for student seating ; behind them is a path @-@ defined tailgating area called \" The Hill \" . The seating behind the north end zone forms a distinctive \" V \" shape intended to resemble an eagle 's wings in flight . The tips of the\n",
      "----------------------------------------\n",
      "  Future:  \" wings \" reach 106 feet ( 32 m ) above the field . There is no seating behind the south end zone , but the area includes a 47 @-@ by @-@ 27 @-@ foot ( 14 @.@ 3 m × 8 @.@ 2 m ) scoreboard and a 5 @-@ foot ( 1 @.@ 5 m ) bronze bust of an eagle . The bust is named \" Spiriki \" , and was donated by members of the Geezles , the school 's first social fraternity . On game days , the area also includes a scale replica cannon named \" Boomer \" , which is fired\n",
      "========================================\n",
      "  Text preview: The stands on the east side of the field are generally reserved for student seating ; behind them is a path @-@ defined tailgating area called \" The Hill \" . The seating behind the north end zone forms a distinctive \" V \" shape intended to resemble an eagle 's wings in flight . The tips of the \" wings \" reach 106 feet ( 32 m ) above the field . There is no seating behind the south end zone , but the area includes a 47 @-@ by @-@ 27 @-@ foot ( 14 @.@ 3 m × 8 @.@ 2 m ) scoreboard and a 5 @-@ foot ( 1 @.@ 5 m ) bronze bust of an eagle . The bust is named \" Spiriki \" , and was donated by members of the Geezles , the school 's first social fraternity . On game days , the area also includes a scale replica cannon named \" Boomer \" , which is fired\n",
      "\n",
      "Prediction 4/5:\n",
      "  Matched Pattern: 473606b91c7e9c71b39b7e865fb35465ee9dbb82...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.0265\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1538\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.1538\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 34\n",
      "     Total tokens: 98\n",
      "========================================\n",
      "  Present:  Bomar sat on the bench for the rest of the season 's games . Known as a devastating blocker and \" lightning fast , \" he was the first Commodore football player elected to the College Football Hall of Fame in 1956 . At his induction Bomar said , \" I just wish all the men who played with me at\n",
      "----------------------------------------\n",
      "  Future:  Vanderbilt between 1921 and 1924 could also receive this coveted award . They deserve it more than I do . After all , they made it possible for me to be chosen . \"\n",
      "========================================\n",
      "  Text preview:  Bomar sat on the bench for the rest of the season 's games . Known as a devastating blocker and \" lightning fast , \" he was the first Commodore football player elected to the College Football Hall of Fame in 1956 . At his induction Bomar said , \" I just wish all the men who played with me at Vanderbilt between 1921 and 1924 could also receive this coveted award . They deserve it more than I do . After all , they made it possible for me to be chosen . \"\n",
      "\n",
      "Prediction 5/5:\n",
      "  Matched Pattern: ef54514bf33b785041c0ac0749e9839a81957403...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              0.9205\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1325\n",
      "    Bayesian Likelihood:    0.1667\n",
      "    Evidence:               0.5000\n",
      "    Similarity:             0.1667\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 24\n",
      "     Total tokens: 88\n",
      "========================================\n",
      "  Present: The table on the right shows the ACE for each storm in the season . ACE is , broadly speaking , a measure of the power of the hurricane multiplied by the length of time it existed , so storms that last a long time , as well as particularly strong hurricanes , have high ACEs . ACE is only calculated for full\n",
      "----------------------------------------\n",
      "  Future:  advisories on tropical systems at or exceeding 34 knots ( 39 mph , 63 km / h ) or tropical storm strength .\n",
      "========================================\n",
      "  Text preview: The table on the right shows the ACE for each storm in the season . ACE is , broadly speaking , a measure of the power of the hurricane multiplied by the length of time it existed , so storms that last a long time , as well as particularly strong hurricanes , have high ACEs . ACE is only calculated for full advisories on tropical systems at or exceeding 34 knots ( 39 mph , 63 km / h ) or tropical storm strength .\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 5 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    Potential:              1.0795\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1135\n",
      "    Bayesian Likelihood:    0.1429\n",
      "    Evidence:               0.2500\n",
      "    Similarity:             0.1429\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "  Text: Rhodes spent the next few months in London , seeking out supporters for his cause in the West End , the City and , occasionally , the rural estates of the landed gentry . These efforts yielded the public backing of the prominent imperialist Harry Johnston , Alexander Livingstone Bruce ( who sat on the board of the East Africa Company ) , and Lord Balfour of Burleigh , among others . Along with Grey 's active involvement and Lord Salisbury 's continuing favour , the weight of this opinion seemed to be reaping dividends for Rhodes by June 1889 . The amalgamation with the London syndicate was complete , and Whitehall appeared to have dropped its reservations regarding the Rudd Concession 's validity . Opposition to the charter in parliament and elsewhere had been for the most part silenced , and , with the help of Rhodes 's press contacts , prominently William Thomas Stead , editor of the Pall Mall Gazette , opinion in the media was starting to back the idea of a chartered company for south @-@ central Africa . But in June 1889 , just as the Colonial Office looked poised to grant the royal charter , Lobengula 's letter repudiating the Rudd Concession , written two months previously , arrived in London .\n",
      "\n",
      "\n",
      "Result 2:\n",
      "  Metrics:\n",
      "    Potential:              1.0265\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1538\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.1538\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "  Text: The table on the right lists the ten longest tributaries of the Missouri , along with their respective catchment areas and flows . Length is measured to the hydrologic source , regardless of naming convention . The main stem of the Kansas River , for example , is 148 miles ( 238 km ) long . However , including the longest headwaters tributaries , the 453 @-@ mile ( 729 km ) Republican River and the 156 @-@ mile ( 251 km ) Arikaree River , brings the total length to 749 miles ( 1 @,@ 205 km ) . Similar naming issues are encountered with the Platte River , whose longest tributary , the North Platte River , is more than twice as long as its mainstream .\n",
      "\n",
      "\n",
      "Result 3:\n",
      "  Metrics:\n",
      "    Potential:              1.0265\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1538\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.1538\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "  Text: The stands on the east side of the field are generally reserved for student seating ; behind them is a path @-@ defined tailgating area called \" The Hill \" . The seating behind the north end zone forms a distinctive \" V \" shape intended to resemble an eagle 's wings in flight . The tips of the \" wings \" reach 106 feet ( 32 m ) above the field . There is no seating behind the south end zone , but the area includes a 47 @-@ by @-@ 27 @-@ foot ( 14 @.@ 3 m × 8 @.@ 2 m ) scoreboard and a 5 @-@ foot ( 1 @.@ 5 m ) bronze bust of an eagle . The bust is named \" Spiriki \" , and was donated by members of the Geezles , the school 's first social fraternity . On game days , the area also includes a scale replica cannon named \" Boomer \" , which is fired\n",
      "\n",
      "\n",
      "Result 4:\n",
      "  Metrics:\n",
      "    Potential:              1.0265\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1538\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.1538\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "  Text:  Bomar sat on the bench for the rest of the season 's games . Known as a devastating blocker and \" lightning fast , \" he was the first Commodore football player elected to the College Football Hall of Fame in 1956 . At his induction Bomar said , \" I just wish all the men who played with me at Vanderbilt between 1921 and 1924 could also receive this coveted award . They deserve it more than I do . After all , they made it possible for me to be chosen . \"\n",
      "\n",
      "\n",
      "Result 5:\n",
      "  Metrics:\n",
      "    Potential:              0.9205\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1325\n",
      "    Bayesian Likelihood:    0.1667\n",
      "    Evidence:               0.5000\n",
      "    Similarity:             0.1667\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1380\n",
      "  Text: The table on the right shows the ACE for each storm in the season . ACE is , broadly speaking , a measure of the power of the hurricane multiplied by the length of time it existed , so storms that last a long time , as well as particularly strong hurricanes , have high ACEs . ACE is only calculated for full advisories on tropical systems at or exceeding 34 knots ( 39 mph , 63 km / h ) or tropical storm strength .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple short input\n",
    "input_text = \"The cat sat on the \"\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    # max_predictions=100,\n",
    "    verbose=True,\n",
    "    verbose_unravel=False  # Set True to see detailed unraveling\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    Potential:              {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:             {result['confidence']:.4f}\")\n",
    "    print(f\"    Bayesian Posterior:     {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:    {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    Evidence:               {result['evidence']:.4f}\")\n",
    "    print(f\"    Similarity:             {result['similarity']:.4f}\")\n",
    "    print(f\"    SNR:                    {result['snr']:.4f}\")\n",
    "    print(f\"    Predictive Information: {result['predictive_information']:.4f}\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Longer Input (More Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input: Machine learning is a field of artificial intelligence that\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input: Machine learning is a field of artificial intelligence that\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (9): ['Machine', 'Ġlearning', 'Ġis', 'Ġa', 'Ġfield', 'Ġof', 'Ġartificial', 'Ġintelligence', 'Ġthat']\n",
      "\n",
      "Chunks (2) with chunk_size=8:\n",
      "  Chunk 0: ['Machine', 'Ġlearning', 'Ġis', 'Ġa', 'Ġfield', 'Ġof', 'Ġartificial', 'Ġintelligence']\n",
      "  Chunk 1: ['Ġthat']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/2: ['Machine', 'Ġlearning', 'Ġis', 'Ġa', 'Ġfield', 'Ġof', 'Ġartificial', 'Ġintelligence']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 8 tokens\n",
      "  node0 STM: 8 events\n",
      "     STM: [['Machine'], ['Ġlearning'], ['Ġis'], ['Ġa'], ['Ġfield'], ['Ġof'], ['Ġartificial'], ['Ġintelligence']]\n",
      "✓ Got 2 predictions\n",
      "  Sample predictions:\n",
      "    1. f9b44c1ec1d7e8f5d9107720072a54c37f768aea... (conf: 1.000)\n",
      "    2. a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f... (conf: 1.000)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 2 pattern names as 1 event:\n",
      "    1. PTRN|f9b44c1ec1d7e8f5d9107720072a54c37f768aea...\n",
      "    2. PTRN|a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|a6dd9b5cbba7b644cfc671af13d5d8f2f835d80f', 'PTRN|f9b44c1ec1d7e8f5d9107720072a54c37f768aea']]\n",
      "✓ Got 2 predictions\n",
      "  Sample predictions:\n",
      "    1. c62b62977e4e3cf1f6359923ef1dd5421fc03c26... (conf: 1.000)\n",
      "    2. a134a2f225bb8d031deedeb8e88ef8d13c869f48... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 2 pattern names as 1 event:\n",
      "    1. PTRN|c62b62977e4e3cf1f6359923ef1dd5421fc03c26...\n",
      "    2. PTRN|a134a2f225bb8d031deedeb8e88ef8d13c869f48...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|a134a2f225bb8d031deedeb8e88ef8d13c869f48', 'PTRN|c62b62977e4e3cf1f6359923ef1dd5421fc03c26']]\n",
      "✓ Got 1 predictions\n",
      "  Sample predictions:\n",
      "    1. 65247b6e024d8f9c2529e0842cab8fe48661aefb... (conf: 1.000)\n",
      "\n",
      "--- NODE3 ---\n",
      "Sending 1 pattern names as 1 event:\n",
      "    1. PTRN|65247b6e024d8f9c2529e0842cab8fe48661aefb...\n",
      "✓ Observed (1 event)\n",
      "  node3 STM: 1 events\n",
      "     STM: [['PTRN|65247b6e024d8f9c2529e0842cab8fe48661aefb']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node3\n",
      "     STM has 1 event(s)\n",
      "     First event: ['PTRN|65247b6e024d8f9c2529e0842cab8fe48661aefb']... (1 symbols)\n",
      "\n",
      "================================================================================\n",
      "CHUNK 2/2: ['Ġthat']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 1 tokens\n",
      "  node0 STM: 1 events\n",
      "     STM: [['Ġthat']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node0\n",
      "     Possible reasons:\n",
      "       - No patterns in KB match this token sequence\n",
      "       - recall_threshold too high (current: 0.6)\n",
      "⚠ No predictions from node0 - stopping cascade for this chunk\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 0 predictions\n",
      "  node1: 2 predictions\n",
      "  node2: 1 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "✓ Using node2 predictions (1 patterns with usable future data)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/1:\n",
      "  Matched Pattern: 65247b6e024d8f9c2529e0842cab8fe48661aefb...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.4444\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     1.0000\n",
      "    Bayesian Likelihood:    0.4000\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.4000\n",
      "    SNR:                    0.3333\n",
      "    Predictive Information: 0.4000\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 33\n",
      "     Total tokens: 97\n",
      "========================================\n",
      "  Present:  characteristic of the field . Suppose that F is a field of characteristic p , and consider the function <formula> that raises each element of F to the power p . This is called the Frobenius automorphism of F. It is an automorphism of the field because of the Freshman 's dream\n",
      "----------------------------------------\n",
      "  Future:  identity <formula> . The Frobenius automorphism is important in number theory because it generates the Galois group of F over its prime subfield .\n",
      "========================================\n",
      "  Text preview:  characteristic of the field . Suppose that F is a field of characteristic p , and consider the function <formula> that raises each element of F to the power p . This is called the Frobenius automorphism of F. It is an automorphism of the field because of the Freshman 's dream identity <formula> . The Frobenius automorphism is important in number theory because it generates the Galois group of F over its prime subfield .\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 1 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    Potential:              1.4444\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     1.0000\n",
      "    Bayesian Likelihood:    0.4000\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.4000\n",
      "    SNR:                    0.3333\n",
      "    Predictive Information: 0.4000\n",
      "  Text:  characteristic of the field . Suppose that F is a field of characteristic p , and consider the function <formula> that raises each element of F to the power p . This is called the Frobenius automorphism of F. It is an automorphism of the field because of the Freshman 's dream identity <formula> . The Frobenius automorphism is important in number theory because it generates the Galois group of F over its prime subfield .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Longer input with more context\n",
    "input_text = \"Machine learning is a field of artificial intelligence that\"\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    # max_predictions=3,\n",
    "    verbose=True,\n",
    "    verbose_unravel=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    Potential:              {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:             {result['confidence']:.4f}\")\n",
    "    print(f\"    Bayesian Posterior:     {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:    {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    Evidence:               {result['evidence']:.4f}\")\n",
    "    print(f\"    Similarity:             {result['similarity']:.4f}\")\n",
    "    print(f\"    SNR:                    {result['snr']:.4f}\")\n",
    "    print(f\"    Predictive Information: {result['predictive_information']:.4f}\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Custom Input (Try Your Own!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input: The researchers found that\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input: The researchers found that\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (4): ['The', 'Ġresearchers', 'Ġfound', 'Ġthat']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['The', 'Ġresearchers', 'Ġfound', 'Ġthat']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['The', 'Ġresearchers', 'Ġfound', 'Ġthat']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 4 tokens\n",
      "  node0 STM: 4 events\n",
      "     STM: [['The'], ['Ġresearchers'], ['Ġfound'], ['Ġthat']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. 43d30465d6248c07718f645080cb15201d29d84b... (conf: 1.000)\n",
      "    2. 300c6e512d5233eb418c3b481883d4c715e47dad... (conf: 1.000)\n",
      "    3. 8b2a94c1baa4558cddfba52a1108b40ef182043f... (conf: 1.000)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|43d30465d6248c07718f645080cb15201d29d84b...\n",
      "    2. PTRN|300c6e512d5233eb418c3b481883d4c715e47dad...\n",
      "    3. PTRN|8b2a94c1baa4558cddfba52a1108b40ef182043f...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|0f6eaa6d2e39cb72ea6b4b4ba101db4856ddfb95', 'PTRN|1967e8489f2c5e298c4006662da3cbe8709bf715', 'PTRN|300c6e512d5233eb418c3b481883d4c715e47dad', 'PTRN|43d30465d6248c07718f645080cb15201d29d84b', 'PTRN|7908935ce2e44dfea038b90fb8adeceeb4ff7df6', 'PTRN|8b2a94c1baa4558cddfba52a1108b40ef182043f', 'PTRN|a0764982bf6651643a3c0532d73ec66f5c61c335', 'PTRN|a0ed8cbf008c071c1fb2b685aa1a8ece276ec4a9', 'PTRN|a486676c021ccaf20694d96ab54d844baec724c5', 'PTRN|b1dd06d5a28de6b60cf6f64e1f26a74a77e7d12c']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. 8f2cf23fc73c2db43510d96d78f9dd09e1b23721... (conf: 1.000)\n",
      "    2. 64fa5a807feb4a40587d6dc5d6f7a71d9680ef94... (conf: 1.000)\n",
      "    3. 857bb4ca6d6752357a481a5381535c5a4f79c0e8... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|8f2cf23fc73c2db43510d96d78f9dd09e1b23721...\n",
      "    2. PTRN|64fa5a807feb4a40587d6dc5d6f7a71d9680ef94...\n",
      "    3. PTRN|857bb4ca6d6752357a481a5381535c5a4f79c0e8...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|092dabe0fb4aec9124b21e23514b7d3dab7e197c', 'PTRN|19df9bcea6d3133ceca8416c17938f539527b26d', 'PTRN|1b269ce406f97e51754b6f9879c9ad30944490d0', 'PTRN|2178dda8bf735bc17865fa67b59e55d699b8ded7', 'PTRN|3977e9cd8b4c6609c403337dbf1ef2a57bdf89c3', 'PTRN|64fa5a807feb4a40587d6dc5d6f7a71d9680ef94', 'PTRN|857bb4ca6d6752357a481a5381535c5a4f79c0e8', 'PTRN|8f2cf23fc73c2db43510d96d78f9dd09e1b23721', 'PTRN|b84f489561ce85cab3f84c44250cca1fe6a0c1b6', 'PTRN|fbf492fd059adee90cacbcf27be321fdf146b5cd']]\n",
      "✓ Got 8 predictions\n",
      "  Sample predictions:\n",
      "    1. 8a538e430ac00ce340bdc4dde79493f38238ef0e... (conf: 1.000)\n",
      "    2. d66b644a508c071459e9064cbf204ac75184fd1f... (conf: 1.000)\n",
      "    3. 2ce68bd950fe46090f889dfe63690c2778d7eca8... (conf: 1.000)\n",
      "\n",
      "--- NODE3 ---\n",
      "Sending 8 pattern names as 1 event:\n",
      "    1. PTRN|8a538e430ac00ce340bdc4dde79493f38238ef0e...\n",
      "    2. PTRN|d66b644a508c071459e9064cbf204ac75184fd1f...\n",
      "    3. PTRN|2ce68bd950fe46090f889dfe63690c2778d7eca8...\n",
      "✓ Observed (1 event)\n",
      "  node3 STM: 1 events\n",
      "     STM: [['PTRN|2ce68bd950fe46090f889dfe63690c2778d7eca8', 'PTRN|412270be5aeae632c1d40fb844496128bb76d96f', 'PTRN|5a65e576d90e77b8943398d561ba7b4b65dc2562', 'PTRN|859e0343ab9602d2e4f3e60648463037ea5f8b44', 'PTRN|8a07dd0ecc334710ab84c6c90b771b8a2eacfdda', 'PTRN|8a538e430ac00ce340bdc4dde79493f38238ef0e', 'PTRN|d66b644a508c071459e9064cbf204ac75184fd1f', 'PTRN|fe6b43ee117e54da919029a03c39b71e6d10e49f']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node3\n",
      "     STM has 1 event(s)\n",
      "     First event: ['PTRN|2ce68bd950fe46090f889dfe63690c2778d7eca8', 'PTRN|412270be5aeae632c1d40fb844496128bb76d96f', 'PTRN|5a65e576d90e77b8943398d561ba7b4b65dc2562']... (8 symbols)\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 10 predictions\n",
      "  node1: 10 predictions\n",
      "  node2: 8 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "✓ Using node2 predictions (8 patterns with usable future data)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/5:\n",
      "  Matched Pattern: 8a538e430ac00ce340bdc4dde79493f38238ef0e...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 4 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.1114\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1142\n",
      "    Bayesian Likelihood:    0.1333\n",
      "    Evidence:               0.2000\n",
      "    Similarity:             0.1333\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "\n",
      "  Extracting PRESENT tokens from pred['present'] field...\n",
      "    Unraveling present pattern: 8f2cf23fc73c2db4... (from level 1)\n",
      "    Unraveling node1: 8f2cf23fc73c2db43510d96d78f9dd09e1b23721...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 0f6eaa6d2e39cb72ea6b4b4ba101db4856ddfb95...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['The', 'Ġminers', 'Ġfound', 'Ġthat', 'Ġthe', 'Ġrow', 'Ġof', 'Ġc']... → 'The miners found that the row of c...'\n",
      "      Unraveling node0: 6160efe7fc633f6ac293940472c7140099076bce...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ott', 'ages', 'Ġthat', 'Ġserved', 'Ġas', 'Ġtheir', 'Ġhomes', 'Ġhad']... → 'ottages that served as their homes had...'\n",
      "      Unraveling node0: 035ec84219565e8e98c8ca9d60e3409caf0fc683...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġbeen', 'Ġdevastated', 'Ġand', 'Ġsome', 'Ġof', 'Ġtheir', 'Ġfamilies', 'Ġkilled']... → ' been devastated and some of their families killed...'\n",
      "      Unraveling node0: 075c3f15605e1c2c0faad189fa97e24cca417781...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġseemingly', 'Ġat', 'Ġrandom', 'Ġ.', 'ĠOne', 'Ġfound', 'Ġhis']... → ' , seemingly at random . One found his...'\n",
      "      Unraveling node0: c20fc36f9a1a3bb0591a564505ef47a783971aee...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġfamily', 'Ġalive', 'Ġand', 'Ġsafe', 'Ġin', 'Ġa', 'Ġmakeshift', 'Ġhospital']... → ' family alive and safe in a makeshift hospital...'\n",
      "      Unraveling node0: 0389f3c2dee8c9a9efe2551871c26fde1a6318a7...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġbut', 'Ġanother', 'Ġemerged', 'Ġto', 'Ġdiscover', 'Ġhis', 'Ġwife']... → ' , but another emerged to discover his wife...'\n",
      "      Unraveling node0: e63f580491dfb15c33a3d15d77ff1ca85f9fd4c3...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġand', 'Ġfour', 'Ġchildren', 'Ġhad', 'Ġdied', 'Ġ.', 'ĠFif', 'teen']... → ' and four children had died . Fifteen...'\n",
      "      Unraveling node0: 3e237384aea699324d6cd15b3326a193efec0b4f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ@', '-', '@', 'Ġyear', 'Ġ@', '-', '@', 'Ġold']... → ' @-@ year @-@ old...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "\n",
      "  Unraveling FUTURE (predicted next) from level 1...\n",
      "  Unraveling 4 future events from level 1\n",
      "  Unraveling future pattern: d878fe5bc6394874...\n",
      "    Unraveling node1: PTRN|d878fe5bc63948741c5e094a8624768f54692435...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 534ddda176d9d8e3cf36cdcb56aa5974d93e5199...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠL', 'illian', 'ĠClark', 'Ġ,', 'Ġworking', 'Ġa', 'Ġlate', 'Ġshift']... → ' Lillian Clark , working a late shift...'\n",
      "      Unraveling node0: e927640d6d3ab41a2992ef78a870048a1b07cb81...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthat', 'Ġnight', 'Ġin', 'Ġthe', 'Ġtown', \"Ġ'\", 's', 'Ġboarding']... → ' that night in the town 's boarding...'\n",
      "      Unraveling node0: 60c9ebe808856db3618632a9a1d41455de225421...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġhouse', 'Ġ,', 'Ġhad', 'Ġbeen', 'Ġgiven', 'Ġpermission', 'Ġto', 'Ġstay']... → ' house , had been given permission to stay...'\n",
      "      Unraveling node0: 4fee5b81a8ca637c377cdae8feb8fc30ccb6bfc0...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġovernight', 'Ġfor', 'Ġthe', 'Ġfirst', 'Ġtime', 'Ġ.', 'ĠShe', 'Ġwas']... → ' overnight for the first time . She was...'\n",
      "      Unraveling node0: d5027195a88ec3bfb739181b581d23fe85a432aa...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthe', 'Ġonly', 'Ġmember', 'Ġof', 'Ġher', 'Ġfamily', 'Ġto', 'Ġsurvive']... → ' the only member of her family to survive...'\n",
      "      Unraveling node0: 26f63674de5552230a1bd90bd2a1ff71cbb5a27f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ.', 'ĠHer', 'Ġfather', 'Ġwas', 'Ġworking', 'Ġoutside', 'Ġthe', 'Ġmine']... → ' . Her father was working outside the mine...'\n",
      "      Unraveling node0: fd383c2db8a4a4344b4852e04dc8b69ea3f9998e...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġwhen', 'Ġthe', 'Ġslide', 'Ġhit', 'Ġ,', 'Ġwhile', 'Ġher', 'Ġmother']... → ' when the slide hit , while her mother...'\n",
      "      Unraveling node0: 982e3a1d8efe09f1b546c0f9821b9ba6bb72ee2a...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġand', 'Ġsix', 'Ġsiblings', 'Ġwere', 'Ġburied', 'Ġin', 'Ġtheir', 'Ġhome']... → ' and six siblings were buried in their home...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "  Unraveling future pattern: cb41642c2395c0bc...\n",
      "    Unraveling node1: PTRN|cb41642c2395c0bc85dac3bd1a52e4007ae71819...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: ced716ce80f882fcf45a70593c27ac7e62cb84c6...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ.', 'ĠAll', 'Ġ12', 'Ġmen', 'Ġliving', 'Ġat', 'Ġthe', 'ĠCPR']... → ' . All 12 men living at the CPR...'\n",
      "      Unraveling node0: 52c268c30111f6dcfe33638eb37d7544a5e8d53a...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġwork', 'Ġcamp', 'Ġwere', 'Ġkilled', 'Ġ,', 'Ġbut', 'Ġ128', 'Ġmore']... → ' work camp were killed , but 128 more...'\n",
      "      Unraveling node0: 5e6c2fe2d49886954e4a20c0aca62902ca45ad45...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġwho', 'Ġwere', 'Ġscheduled', 'Ġto', 'Ġmove', 'Ġinto', 'Ġthe', 'Ġcamp']... → ' who were scheduled to move into the camp...'\n",
      "      Unraveling node0: 30f9d95ea082b985f9f80569ab65c7e3b208b4b4...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthe', 'Ġday', 'Ġbefore', 'Ġthe', 'Ġslide', 'Ġhad', 'Ġnot', 'Ġarrived']... → ' the day before the slide had not arrived...'\n",
      "      Unraveling node0: a1df70db902d5475ad55abdd381e3b35ceef4909...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠâĢĶ', 'Ġthe', 'Ġtrain', 'Ġthat', 'Ġwas', 'Ġsupposed', 'Ġto', 'Ġtake']... → ' — the train that was supposed to take...'\n",
      "      Unraveling node0: d79796625fb844eb6b1e869e2c5c53900aa74024...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthem', 'Ġthere', 'Ġfrom', 'ĠMorris', 'sey', 'Ġ,', 'ĠBritish', 'ĠColumbia']... → ' them there from Morrissey , British Columbia...'\n",
      "      Unraveling node0: fb9c9a3b60f3e1459bfd03b2830fe124455d9e57...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġfailed', 'Ġto', 'Ġpick', 'Ġthem', 'Ġup', 'Ġ.', 'ĠThe']... → ' , failed to pick them up . The...'\n",
      "      Unraveling node0: 91687aab157c68c7ad1d38993399409836ef5a91...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠSpokane', 'ĠFly', 'er', 'Ġ,', 'Ġa', 'Ġpassenger', 'Ġtrain', 'Ġheading']... → ' Spokane Flyer , a passenger train heading...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "  Unraveling future pattern: fa939e21b87f6245...\n",
      "    Unraveling node1: PTRN|fa939e21b87f62450da2374f940eae3d64d0cd03...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 3a5efda1562c2f29afddd9595b0696f168ed8252...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġwest', 'Ġfrom', 'ĠLeth', 'bridge', 'Ġ,', 'Ġwas', 'Ġsaved', 'Ġby']... → ' west from Lethbridge , was saved by...'\n",
      "      Unraveling node0: 26d51033bca976b195156c6038428793fa608dc9...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠCPR', 'Ġbrake', 'man', 'ĠSid', 'ĠCho', 'qu', 'ette', 'Ġ,']... → ' CPR brakeman Sid Choquette ,...'\n",
      "      Unraveling node0: 15e33669b85ebd0c4a508b22f4d97030f44564f5...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġone', 'Ġof', 'Ġtwo', 'Ġmen', 'Ġwho', 'Ġrushed', 'Ġacross', 'Ġthe']... → ' one of two men who rushed across the...'\n",
      "      Unraveling node0: f583c1cb25c2793ba5a1519872e2a4322a52a12f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġrock', 'Ġ@', '-', '@', 'Ġstre', 'wn', 'Ġground', 'Ġto']... → ' rock @-@ strewn ground to...'\n",
      "      Unraveling node0: 0e1f6864b108f7ed7538c33f95c65db1ba0af86b...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġwarn', 'Ġthe', 'Ġtrain', 'Ġthat', 'Ġthe', 'Ġtrack', 'Ġhad', 'Ġbeen']... → ' warn the train that the track had been...'\n",
      "      Unraveling node0: ef7dc23be24c9166115092f8340baf1e4115bbd5...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġburied', 'Ġunder', 'Ġthe', 'Ġslide', 'Ġ.', 'ĠThrough', 'Ġfalling', 'Ġrocks']... → ' buried under the slide . Through falling rocks...'\n",
      "      Unraveling node0: 61dc34bde7384b485056dd20d469a0e71a7c44ea...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġand', 'Ġa', 'Ġdust', 'Ġcloud', 'Ġthat', 'Ġimpaired', 'Ġhis', 'Ġvisibility']... → ' and a dust cloud that impaired his visibility...'\n",
      "      Unraveling node0: 0f43cfe36d84e98e783736450bbc1a1b17e29f74...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'ĠCho', 'qu', 'ette', 'Ġran', 'Ġfor', 'Ġ2', 'Ġkilometres']... → ' , Choquette ran for 2 kilometres...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "  Unraveling future pattern: 73d7987c3c15d8f3...\n",
      "    Unraveling node1: PTRN|73d7987c3c15d8f38148419f9e77c1b2dde0b21c...\n",
      "      Pattern has 5 events\n",
      "      → Has 5 children at node0\n",
      "      Unraveling node0: 735d50b4363300326bca808b2ad90e2d4c47275a...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ(', 'Ġ1', 'Ġ@', '.', '@', 'Ġ2', 'Ġmi', 'Ġ)']... → ' ( 1 @.@ 2 mi )...'\n",
      "      Unraveling node0: 822fc9072fe4bee0b0da937fd78570ad6227b058...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġto', 'Ġwarn', 'Ġthe', 'Ġon', 'coming', 'Ġlocom', 'otive', 'Ġof']... → ' to warn the oncoming locomotive of...'\n",
      "      Unraveling node0: ba2a67a175009151243fdd6f888bae5e9aa5698a...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthe', 'Ġdanger', 'Ġ.', 'ĠThe', 'ĠCPR', 'Ġgave', 'Ġhim', 'Ġa']... → ' the danger . The CPR gave him a...'\n",
      "      Unraveling node0: fc290733fdc2af1f71ae6b13f4ed2d985d85801d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġletter', 'Ġof', 'Ġcommend', 'ation', 'Ġand', 'Ġa', 'Ġ$', 'Ġ25']... → ' letter of commendation and a $ 25...'\n",
      "      Unraveling node0: 3043301420aef01cf004994f381acbdceb1adc02...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġche', 'que', 'Ġin', 'Ġrecognition', 'Ġof', 'Ġhis', 'Ġheroism', 'Ġ.']... → ' cheque in recognition of his heroism ....'\n",
      "      ✓ Unraveled to 40 total tokens\n",
      "  Total future tokens: 232\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 232\n",
      "     Total tokens: 296\n",
      "========================================\n",
      "  Present: The miners found that the row of cottages that served as their homes had been devastated and some of their families killed , seemingly at random . One found his family alive and safe in a makeshift hospital , but another emerged to discover his wife and four children had died . Fifteen @-@ year @-@ old\n",
      "----------------------------------------\n",
      "  Future:  Lillian Clark , working a late shift that night in the town 's boarding house , had been given permission to stay overnight for the first time . She was the only member of her family to survive . Her father was working outside the mine when the slide hit , while her mother and six siblings were buried in their home . All 12 men living at the CPR work camp were killed , but 128 more who were scheduled to move into the camp the day before the slide had not arrived — the train that was supposed to take them there from Morrissey , British Columbia , failed to pick them up . The Spokane Flyer , a passenger train heading west from Lethbridge , was saved by CPR brakeman Sid Choquette , one of two men who rushed across the rock @-@ strewn ground to warn the train that the track had been buried under the slide . Through falling rocks and a dust cloud that impaired his visibility , Choquette ran for 2 kilometres ( 1 @.@ 2 mi ) to warn the oncoming locomotive of the danger . The CPR gave him a letter of commendation and a $ 25 cheque in recognition of his heroism .\n",
      "========================================\n",
      "  Text preview: The miners found that the row of cottages that served as their homes had been devastated and some of their families killed , seemingly at random . One found his family alive and safe in a makeshift hospital , but another emerged to discover his wife and four children had died . Fifteen @-@ year @-@ old Lillian Clark , working a late shift that night in the town 's boarding house , had been given permission to stay overnight for the first time . She was the only member of her family to survive . Her father was working outside the mine when the slide hit , while her mother and six siblings were buried in their home . All 12 men living at the CPR work camp were killed , but 128 more who were scheduled to move into the camp the day before the slide had not arrived — the train that was supposed to take them there from Morrissey , British Columbia , failed to pick them up . The Spokane Flyer , a passenger train heading west from Lethbridge , was saved by CPR brakeman Sid Choquette , one of two men who rushed across the rock @-@ strewn ground to warn the train that the track had been buried under the slide . Through falling rocks and a dust cloud that impaired his visibility , Choquette ran for 2 kilometres ( 1 @.@ 2 mi ) to warn the oncoming locomotive of the danger . The CPR gave him a letter of commendation and a $ 25 cheque in recognition of his heroism .\n",
      "\n",
      "Prediction 2/5:\n",
      "  Matched Pattern: d66b644a508c071459e9064cbf204ac75184fd1f...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 4 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.1114\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1142\n",
      "    Bayesian Likelihood:    0.1333\n",
      "    Evidence:               0.2000\n",
      "    Similarity:             0.1333\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "\n",
      "  Extracting PRESENT tokens from pred['present'] field...\n",
      "    Unraveling present pattern: 857bb4ca6d675235... (from level 1)\n",
      "    Unraveling node1: 857bb4ca6d6752357a481a5381535c5a4f79c0e8...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: a0ed8cbf008c071c1fb2b685aa1a8ece276ec4a9...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['The', 'Ġinvestigation', 'Ġfound', 'Ġthat', 'Ġas', 'Ġthe', 'ĠB', 'Ġ@']... → 'The investigation found that as the B @...'\n",
      "      Unraveling node0: 04ce4a94db59660647454bd2fb113735b760fbc0...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['-', '@', 'Ġ52', 'Ġentered', 'Ġits', 'Ġfinal', 'Ġturn', 'Ġsequence']... → '-@ 52 entered its final turn sequence...'\n",
      "      Unraveling node0: b1ddf418ef7470da58637123696dcb33a5081277...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġaround', 'Ġthe', 'Ġtower', 'Ġ,', 'Ġits', 'Ġindicated', 'Ġair', 'speed']... → ' around the tower , its indicated airspeed...'\n",
      "      Unraveling node0: 8db84fb596f3ba1e4bd58925363413c29a34398f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ(', 'ĠI', 'AS', 'Ġ)', 'Ġwas', 'Ġ182', 'Ġknots', 'Ġ(']... → ' ( IAS ) was 182 knots (...'\n",
      "      Unraveling node0: 933067ee7289e94c9acdd8d5b4cf0cb3c2522771...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ337', 'Ġkm', 'Ġ/', 'Ġh', 'Ġ;', 'Ġ209', 'Ġmph', 'Ġ)']... → ' 337 km / h ; 209 mph )...'\n",
      "      Unraveling node0: d7473700b05fdfbb3db313c0cc165aed0227f65f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ.', 'ĠAlthough', 'ĠHolland', 'Ġincreased', 'Ġthe', 'Ġengine', 'Ġpower', 'Ġafter']... → ' . Although Holland increased the engine power aft...'\n",
      "      Unraveling node0: 55600c88e0e2fa4eea6748fbdddbfa36a52cc4e4...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġstarting', 'Ġthe', 'Ġturn', 'Ġ,', 'Ġhis', 'Ġinput', 'Ġcame', 'Ġtoo']... → ' starting the turn , his input came too...'\n",
      "      Unraveling node0: f0302ee7c86a6b96038109ce7f8dad870f2dedcc...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġlate', 'Ġto', 'Ġmaintain', 'Ġthe', 'Ġaircraft', \"Ġ'\", 's', 'Ġair']... → ' late to maintain the aircraft 's air...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "\n",
      "  Unraveling FUTURE (predicted next) from level 1...\n",
      "  Unraveling 4 future events from level 1\n",
      "  Unraveling future pattern: 498ed7f7b8195c63...\n",
      "    Unraveling node1: PTRN|498ed7f7b8195c636cbb38886911ef9891aa02e8...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: dd9717231b26417e08e3a3a31847e0e5633e2d8d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['speed', 'Ġ,', 'Ġas', 'Ġthe', 'ĠB', 'Ġ@', '-', '@']... → 'speed , as the B @-@...'\n",
      "      Unraveling node0: 9df872779d9868845175d27337428b02ae85de14...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ52', 'Ġturb', 'of', 'an', 'Ġengines', 'Ġtake', 'Ġup', 'Ġto']... → ' 52 turbofan engines take up to...'\n",
      "      Unraveling node0: c6d78bb368eeea9cf0071fd3cc26f588b91563f1...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġeight', 'Ġseconds', 'Ġto', 'Ġrespond', 'Ġto', 'Ġthrottle', 'Ġcommands', 'Ġ.']... → ' eight seconds to respond to throttle commands ....'\n",
      "      Unraveling node0: 46e3e21b12d1f79f2a0ef5e2c4d3438aa308e384...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠEven', 'Ġthough', 'Ġthe', 'Ġair', 'speed', 'Ġindicator', 'Ġwas', 'Ġavailable']... → ' Even though the airspeed indicator was available...'\n",
      "      Unraveling node0: 17988dcf188ac7220be59bdc7943d6a52ae081a6...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġto', 'Ġall', 'Ġfour', 'Ġair', 'crew', 'Ġmembers', 'Ġ,', 'Ġthe']... → ' to all four aircrew members , the...'\n",
      "      Unraveling node0: 73e4334ef1d03c194b34c5620445dc2a7f9214b7...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġaircraft', \"Ġ'\", 's', 'Ġair', 'speed', 'Ġwas', 'Ġallowed', 'Ġto']... → ' aircraft 's airspeed was allowed to...'\n",
      "      Unraveling node0: 8b0b85892db2990a85b5619498477401db4311bd...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġcontinue', 'Ġto', 'Ġdecrease', 'Ġ.', 'ĠEight', 'Ġseconds', 'Ġbefore', 'Ġimpact']... → ' continue to decrease . Eight seconds before impac...'\n",
      "      Unraveling node0: f4f41b9e3911dc9a3c4eb254b49ba9d37b2c2609...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġthe', 'Ġaircraft', \"Ġ'\", 's', 'ĠI', 'AS', 'Ġhad']... → ' , the aircraft 's IAS had...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "  Unraveling future pattern: 3ca986692058f4c4...\n",
      "    Unraveling node1: PTRN|3ca986692058f4c4e43b7712dc03f990118f02f3...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: df07d2a7bb13c5f2cd3f8ea4e24c86b1445e1709...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġdeteriorated', 'Ġto', 'Ġ145', 'Ġknots', 'Ġ(', 'Ġ269', 'Ġkm', 'Ġ/']... → ' deteriorated to 145 knots ( 269 km /...'\n",
      "      Unraveling node0: 0357ffd302e69250b01be9a35ca34cc142167919...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġh', 'Ġ;', 'Ġ167', 'Ġmph', 'Ġ)', 'Ġand', 'Ġthe', 'Ġaircraft']... → ' h ; 167 mph ) and the aircraft...'\n",
      "      Unraveling node0: 60f9a9b4e9a449737fdc2e1a95662eb0f7fe3292...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: [\"Ġ'\", 's', 'Ġbank', 'Ġangle', 'Ġincreased', 'Ġpast', 'Ġ60', 'ĠÂ°']... → ' 's bank angle increased past 60 °...'\n",
      "      Unraveling node0: 18ff4999610eef3662694aaa6b313994a31d7cff...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ.', 'ĠAt', 'Ġthis', 'Ġtime', 'ĠHolland', 'Ġor', 'ĠMcGee', 'han']... → ' . At this time Holland or McGeehan...'\n",
      "      Unraveling node0: 1a195f8c9fad89e6af2efd7dfa55db7dd3bea302...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġapplied', 'Ġfull', 'Ġright', 'Ġspoiler', 'Ġ,', 'Ġright', 'Ġrud', 'der']... → ' applied full right spoiler , right rudder...'\n",
      "      Unraveling node0: 3c5fe5bdf2ac99a5b6078755bfe2b46a71aaeb8d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġand', 'Ġnose', 'Ġ@', '-', '@', 'Ġup', 'Ġelevator']... → ' , and nose @-@ up elevator...'\n",
      "      Unraveling node0: c0c15fa21fd51ad0ee9800904a71594ad1e32338...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġand', 'Ġthe', 'Ġaircraft', 'Ġentered', 'Ġa', 'Ġturning', 'Ġflight']... → ' , and the aircraft entered a turning flight...'\n",
      "      Unraveling node0: 00c6277818751b036903b231991e5f99d64c329a...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġstall', 'Ġ(', 'Ġalso', 'Ġcalled', 'Ġaccelerated', 'Ġstall', 'Ġ)', 'Ġ.']... → ' stall ( also called accelerated stall ) ....'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "  Unraveling future pattern: eccd982a3f9cb6e6...\n",
      "    Unraveling node1: PTRN|eccd982a3f9cb6e662413013e42ec28ac081d952...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 7793db77b80e2a9fef97a2ac9e14bf2ae730304a...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠThis', 'Ġphenomenon', 'Ġis', 'Ġa', 'Ġstall', 'Ġthat', 'Ġoccurs', 'Ġat']... → ' This phenomenon is a stall that occurs at...'\n",
      "      Unraveling node0: a140acd3ebd18c18331b1e8c2c78c6c0c3090c05...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġa', 'Ġhigher', 'Ġair', 'speed', 'Ġthan', 'Ġthe', 'Ġdesign', 'Ġstall']... → ' a higher airspeed than the design stall...'\n",
      "      Unraveling node0: f5404d2a7b7a4a2393780de4803f6a3dc19708f4...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġspeed', 'ĠâĢĵ', 'Ġwhich', 'Ġalways', 'Ġrefers', 'Ġto', 'Ġstraight', 'Ġand']... → ' speed – which always refers to straight and...'\n",
      "      Unraveling node0: 3a348f4ae9aca87f715c0cdbe3330ad0ed3b1500...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġlevel', 'Ġflight', 'ĠâĢĵ', 'Ġbecause', 'Ġof', 'Ġthe', 'Ġfact', 'Ġthat']... → ' level flight – because of the fact that...'\n",
      "      Unraveling node0: 90d93160894fd39367d3bde3e8e5bd4da4e6db6f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthe', 'Ġaircraft', 'Ġis', 'Ġturning', 'Ġ.', 'ĠDue', 'Ġto', 'Ġthe']... → ' the aircraft is turning . Due to the...'\n",
      "      Unraveling node0: 22f8dba9492d1c0c57a33adfc1151bb8abd0b5b7...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġbank', 'Ġof', 'Ġ60', 'ĠÂ°', 'Ġor', 'Ġmore', 'Ġ,', 'Ġthe']... → ' bank of 60 ° or more , the...'\n",
      "      Unraveling node0: 18764c7740865d69a6aae2319b788380b4a2439e...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġstall', 'Ġspeed', 'Ġfor', 'Ġthe', 'Ġaircraft', 'Ġat', 'Ġthat', 'Ġmoment']... → ' stall speed for the aircraft at that moment...'\n",
      "      Unraveling node0: 4147ed2cd41ff464e73f1697d9ad115aa9755b5c...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġwas', 'Ġ147', 'Ġknots', 'Ġ(', 'Ġ272', 'Ġkm', 'Ġ/', 'Ġh']... → ' was 147 knots ( 272 km / h...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "  Unraveling future pattern: 8939c796a7dced2f...\n",
      "    Unraveling node1: PTRN|8939c796a7dced2f30cd885b3d44b2c236e7d187...\n",
      "      Pattern has 4 events\n",
      "      → Has 4 children at node0\n",
      "      Unraveling node0: e16bbefb4fc8471263ad609016b906bf89702c9d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ;', 'Ġ169', 'Ġmph', 'Ġ)', 'Ġ.', 'ĠThus', 'Ġ,', 'Ġflying']... → ' ; 169 mph ) . Thus , flying...'\n",
      "      Unraveling node0: 6cd93b574d0fffb9eb074307d042a4aa5681f895...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ2', 'Ġknots', 'Ġslower', 'Ġ,', 'Ġthe', 'Ġaircraft', 'Ġstalled', 'Ġ,']... → ' 2 knots slower , the aircraft stalled ,...'\n",
      "      Unraveling node0: 4812938f8fc36cfc0bd4095532cbfc35605b3d03...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġwithout', 'Ġhaving', 'Ġsufficient', 'Ġaltitude', 'Ġto', 'Ġrecover', 'Ġbefore', 'Ġstriking']... → ' without having sufficient altitude to recover bef...'\n",
      "      Unraveling node0: 25d5bc0edf3541cccc8dfea71910f5bfdd2bfaa7...\n",
      "        Pattern has 3 events\n",
      "        → Tokens: ['Ġthe', 'Ġground', 'Ġ.']... → ' the ground ....'\n",
      "      ✓ Unraveled to 27 total tokens\n",
      "  Total future tokens: 219\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 219\n",
      "     Total tokens: 283\n",
      "========================================\n",
      "  Present: The investigation found that as the B @-@ 52 entered its final turn sequence around the tower , its indicated airspeed ( IAS ) was 182 knots ( 337 km / h ; 209 mph ) . Although Holland increased the engine power after starting the turn , his input came too late to maintain the aircraft 's air\n",
      "----------------------------------------\n",
      "  Future: speed , as the B @-@ 52 turbofan engines take up to eight seconds to respond to throttle commands . Even though the airspeed indicator was available to all four aircrew members , the aircraft 's airspeed was allowed to continue to decrease . Eight seconds before impact , the aircraft 's IAS had deteriorated to 145 knots ( 269 km / h ; 167 mph ) and the aircraft 's bank angle increased past 60 ° . At this time Holland or McGeehan applied full right spoiler , right rudder , and nose @-@ up elevator , and the aircraft entered a turning flight stall ( also called accelerated stall ) . This phenomenon is a stall that occurs at a higher airspeed than the design stall speed – which always refers to straight and level flight – because of the fact that the aircraft is turning . Due to the bank of 60 ° or more , the stall speed for the aircraft at that moment was 147 knots ( 272 km / h ; 169 mph ) . Thus , flying 2 knots slower , the aircraft stalled , without having sufficient altitude to recover before striking the ground .\n",
      "========================================\n",
      "  Text preview: The investigation found that as the B @-@ 52 entered its final turn sequence around the tower , its indicated airspeed ( IAS ) was 182 knots ( 337 km / h ; 209 mph ) . Although Holland increased the engine power after starting the turn , his input came too late to maintain the aircraft 's airspeed , as the B @-@ 52 turbofan engines take up to eight seconds to respond to throttle commands . Even though the airspeed indicator was available to all four aircrew members , the aircraft 's airspeed was allowed to continue to decrease . Eight seconds before impact , the aircraft 's IAS had deteriorated to 145 knots ( 269 km / h ; 167 mph ) and the aircraft 's bank angle increased past 60 ° . At this time Holland or McGeehan applied full right spoiler , right rudder , and nose @-@ up elevator , and the aircraft entered a turning flight stall ( also called accelerated stall ) . This phenomenon is a stall that occurs at a higher airspeed than the design stall speed – which always refers to straight and level flight – because of the fact that the aircraft is turning . Due to the bank of 60 ° or more , the stall speed for the aircraft at that moment was 147 knots ( 272 km / h ; 169 mph ) . Thus , flying 2 knots slower , the aircraft stalled , without having sufficient altitude to recover before striking the ground .\n",
      "\n",
      "Prediction 3/5:\n",
      "  Matched Pattern: 2ce68bd950fe46090f889dfe63690c2778d7eca8...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 3 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.0795\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1429\n",
      "    Evidence:               0.2500\n",
      "    Similarity:             0.1429\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "\n",
      "  Extracting PRESENT tokens from pred['present'] field...\n",
      "    Unraveling present pattern: 19df9bcea6d3133c... (from level 1)\n",
      "    Unraveling node1: 19df9bcea6d3133ceca8416c17938f539527b26d...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 7908935ce2e44dfea038b90fb8adeceeb4ff7df6...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['The', 'Ġcommission', 'Ġfound', 'Ġthat', 'Ġthe', 'ĠNewport', 'ĠNews', 'ĠShip']... → 'The commission found that the Newport News Ship...'\n",
      "      Unraveling node0: b7f535906778953c98fa3890bbc958b056c5d841...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['building', 'Ġand', 'ĠDry', 'd', 'ock', 'ĠCompany', 'Ġbid', 'Ġwas']... → 'building and Drydock Company bid was...'\n",
      "      Unraveling node0: 79d031dacc4402ba9136d2eb9143e5e1e2134a29...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġlowest', 'Ġon', 'Ġone', 'Ġbattles', 'hip', 'Ġ,', 'Ġand', 'Ġthe']... → ' lowest on one battleship , and the...'\n",
      "      Unraveling node0: 31b83aa9ebfe2db25d7d996cf3c78261b2acbca8...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠFore', 'ĠRiver', 'ĠShip', 'building', 'ĠCompany', 'Ġwas', 'Ġlowest', 'Ġon']... → ' Fore River Shipbuilding Company was lowest on...'\n",
      "      Unraveling node0: 4c7eb9ac8085b51e2d8c2b5929f3363d1c51463f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthe', 'Ġother', 'Ġ.', 'ĠDespite', 'Ġa', 'ĠBritish', 'Ġattempt', 'Ġto']... → ' the other . Despite a British attempt to...'\n",
      "      Unraveling node0: 5b1fc8bb6d3dca22a9a3817840d2406007feb617...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġallow', 'Ġthe', 'ĠArmstrong', 'ĠWhit', 'worth', 'Ġ@', '-', '@']... → ' allow the Armstrong Whitworth @-@...'\n",
      "      Unraveling node0: 778f37ff0af25b26170b32682b7173b02d86739a...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠV', 'ickers', 'Ġteam', 'Ġto', 'Ġlower', 'Ġtheir', 'Ġprice', 'Ġby']... → ' Vickers team to lower their price by...'\n",
      "      Unraveling node0: c15b5ce57bf486d560f1150c135a39e719f4c278...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ$', 'Ġ570', 'Ġ@', ',', '@', 'Ġ000', 'Ġ,', 'Ġprompt']... → ' $ 570 @,@ 000 , prompt...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "\n",
      "  Unraveling FUTURE (predicted next) from level 1...\n",
      "  Unraveling 3 future events from level 1\n",
      "  Unraveling future pattern: 9b6c7bf6978e3729...\n",
      "    Unraveling node1: PTRN|9b6c7bf6978e37295530c966010cf1c9a2a15819...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: c42f62a2d898814f2e47459f57f902ddec087b9e...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠAmerican', 'Ġdiplomacy', 'Ġgranting', 'Ġvarious', 'Ġassurances', 'Ġregarding', 'Ġrecent', 'Ġevents']... → ' American diplomacy granting various assurances re...'\n",
      "      Unraveling node0: 614feebb1458a4e4cd76de1109d21f81b59a036f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġbetween', 'Ġthe', 'ĠUnited', 'ĠStates', 'Ġand', 'ĠBrazil', 'Ġ,', 'Ġthe']... → ' between the United States and Brazil , the...'\n",
      "      Unraveling node0: 6c5366c9f7ac44970263e374cea144dc073803b8...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġupcoming', 'Ġ1910', 'ĠPan', 'Ġ@', '-', '@', 'ĠAmerican', 'ĠConference']... → ' upcoming 1910 Pan @-@ American Conference...'\n",
      "      Unraveling node0: e9e53ebb33cdda504251af9465a8fb46e3e935da...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġand', 'Ġa', 'Ġguarantee', 'Ġof', 'ĠAmerican', 'Ġparticipation', 'Ġin']... → ' , and a guarantee of American participation in...'\n",
      "      Unraveling node0: 00a97d4ac08d13c493f7c13d7845669f6e11de90...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthe', 'ĠArgentine', 'Ġcent', 'ennial', 'Ġcelebrations', 'Ġsecured', 'Ġthe', 'Ġbattles']... → ' the Argentine centennial celebrations secured the...'\n",
      "      Unraveling node0: ce15812483e2582e8e5e9a1493c4d87e606593ad...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['hip', 'Ġcontracts', 'Ġfor', 'ĠFore', 'ĠRiver', 'Ġon', 'Ġ21', 'ĠJanuary']... → 'hip contracts for Fore River on 21 January...'\n",
      "      Unraveling node0: 8de511572f0e67170c7e68e64c7221a622b2fc52...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ1910', 'Ġ.', 'ĠThe', 'Ġmaximum', 'Ġprice', 'ĠFore', 'ĠRiver', 'Ġtend']... → ' 1910 . The maximum price Fore River tend...'\n",
      "      Unraveling node0: 6b11a9e5b1b0c4f950ded567e5929e863cc65a79...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ered', 'Ġ,', 'Ġ$', 'Ġ10', 'Ġ@', '.', '@', 'Ġ7']... → 'ered , $ 10 @.@ 7...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "  Unraveling future pattern: 5974e4e63e8c836d...\n",
      "    Unraveling node1: PTRN|5974e4e63e8c836dfd9ba38b369fa8a9ea8f44f0...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: e896890d6096efbd421b0661a3284e3edb5f63b6...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġmillion', 'Ġ,', 'Ġunder', 'bid', 'Ġthe', 'ĠBritish', 'Ġby', 'Ġmore']... → ' million , underbid the British by more...'\n",
      "      Unraveling node0: 441f6734cd1117de9f2f267d4abf47cc8fa55e7d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthan', 'Ġ$', 'Ġ9', '73', 'Ġ@', ',', '@', 'Ġ000']... → ' than $ 973 @,@ 000...'\n",
      "      Unraveling node0: 3ff12d7f8c9bcb397ae7117ff49a031a1a3413ab...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġbut', 'Ġtheir', 'Ġship', \"Ġ'\", 's', 'Ġdisplacement', 'Ġwas']... → ' , but their ship 's displacement was...'\n",
      "      Unraveling node0: 8318a9366b6c4ad3fa486e989302356b83b49f8e...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ2', 'Ġ@', ',', '@', 'Ġ000', 'Ġlong', 'Ġtons', 'Ġ(']... → ' 2 @,@ 000 long tons (...'\n",
      "      Unraveling node0: 2261e9586cfc176a73036cfa4e768b2deef886e4...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ2', 'Ġ@', ',', '@', 'Ġ000', 'Ġt', 'Ġ)', 'Ġsmaller']... → ' 2 @,@ 000 t ) smaller...'\n",
      "      Unraveling node0: a5db4d52cb3aaa0afe38e85e44f97003c728915c...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ,', 'Ġthe', 'Ġbelt', 'Ġarmor', 'Ġwas', 'Ġ2', 'Ġinches', 'Ġ(']... → ' , the belt armor was 2 inches (...'\n",
      "      Unraveling node0: 04bec1f67c9fdcdd73b2b87be4ce0abcdd2ebf2d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ51', 'Ġmm', 'Ġ)', 'Ġthinner', 'Ġ,', 'Ġand', 'Ġthe', 'Ġtop']... → ' 51 mm ) thinner , and the top...'\n",
      "      Unraveling node0: 684464b7d668c569823fd0c67a2248a97212af3c...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġspeed', 'Ġwas', 'Ġslightly', 'Ġlower', 'Ġ.', 'ĠOrders', 'Ġfor', 'Ġthe']... → ' speed was slightly lower . Orders for the...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "  Unraveling future pattern: 2450d1e45c18279c...\n",
      "    Unraveling node1: PTRN|2450d1e45c18279ca41138ef0458fe0c3616f5cb...\n",
      "      Pattern has 2 events\n",
      "      → Has 2 children at node0\n",
      "      Unraveling node0: 2efb20ac77e64fe38dca60aea0a7431df111a93e...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġtwelve', 'Ġdestroy', 'ers', 'Ġwere', 'Ġdivided', 'Ġamong', 'ĠBritain', 'Ġ,']... → ' twelve destroyers were divided among Britain ,...'\n",
      "      Unraveling node0: 9a48cb851e816a99cd14b946a7900f2a2ec26c82...\n",
      "        Pattern has 5 events\n",
      "        → Tokens: ['ĠFrance', 'Ġ,', 'Ġand', 'ĠGermany', 'Ġ.']... → ' France , and Germany ....'\n",
      "      ✓ Unraveled to 13 total tokens\n",
      "  Total future tokens: 141\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 141\n",
      "     Total tokens: 205\n",
      "========================================\n",
      "  Present: The commission found that the Newport News Shipbuilding and Drydock Company bid was lowest on one battleship , and the Fore River Shipbuilding Company was lowest on the other . Despite a British attempt to allow the Armstrong Whitworth @-@ Vickers team to lower their price by $ 570 @,@ 000 , prompt\n",
      "----------------------------------------\n",
      "  Future:  American diplomacy granting various assurances regarding recent events between the United States and Brazil , the upcoming 1910 Pan @-@ American Conference , and a guarantee of American participation in the Argentine centennial celebrations secured the battleship contracts for Fore River on 21 January 1910 . The maximum price Fore River tendered , $ 10 @.@ 7 million , underbid the British by more than $ 973 @,@ 000 , but their ship 's displacement was 2 @,@ 000 long tons ( 2 @,@ 000 t ) smaller , the belt armor was 2 inches ( 51 mm ) thinner , and the top speed was slightly lower . Orders for the twelve destroyers were divided among Britain , France , and Germany .\n",
      "========================================\n",
      "  Text preview: The commission found that the Newport News Shipbuilding and Drydock Company bid was lowest on one battleship , and the Fore River Shipbuilding Company was lowest on the other . Despite a British attempt to allow the Armstrong Whitworth @-@ Vickers team to lower their price by $ 570 @,@ 000 , prompt American diplomacy granting various assurances regarding recent events between the United States and Brazil , the upcoming 1910 Pan @-@ American Conference , and a guarantee of American participation in the Argentine centennial celebrations secured the battleship contracts for Fore River on 21 January 1910 . The maximum price Fore River tendered , $ 10 @.@ 7 million , underbid the British by more than $ 973 @,@ 000 , but their ship 's displacement was 2 @,@ 000 long tons ( 2 @,@ 000 t ) smaller , the belt armor was 2 inches ( 51 mm ) thinner , and the top speed was slightly lower . Orders for the twelve destroyers were divided among Britain , France , and Germany .\n",
      "\n",
      "Prediction 4/5:\n",
      "  Matched Pattern: 859e0343ab9602d2e4f3e60648463037ea5f8b44...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.0795\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1429\n",
      "    Evidence:               0.2500\n",
      "    Similarity:             0.1429\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "\n",
      "  Extracting PRESENT tokens from pred['present'] field...\n",
      "    Unraveling present pattern: 092dabe0fb4aec91... (from level 1)\n",
      "    Unraveling node1: 092dabe0fb4aec9124b21e23514b7d3dab7e197c...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 30c9d50a761da15f781df4ef02262eca6b7be62d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġformat', 'Ġwas', 'Ġnot', 'Ġthe', 'Ġbest', 'Ġway', 'Ġto', 'Ġattract']... → ' format was not the best way to attract...'\n",
      "      Unraveling node0: 9578a3b865d8b27a6b45733c1a13ef7e1c6db7d1...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġyoung', 'Ġchildren', \"Ġ'\", 's', 'Ġattention', 'Ġ.', 'ĠThe', 'Ġgrowth']... → ' young children 's attention . The growth...'\n",
      "      Unraveling node0: b350452057e078af6b57ec5590adb9ad5901192d...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġof', 'Ġhome', 'Ġvideos', 'Ġduring', 'Ġthe', \"Ġ'\", 'Ġ80', 's']... → ' of home videos during the ' 80s...'\n",
      "      Unraveling node0: 99f4db378561cd7505e4b2d90543b4fd780bd361...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġand', 'Ġthe', 'Ġincrease', 'Ġof', 'Ġthirty', 'Ġ@', '-', '@']... → ' and the increase of thirty @-@...'\n",
      "      Unraveling node0: 9916b372670ab6a5e0fd8efc2323b778debc236b...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġminute', 'Ġchildren', \"Ġ'\", 's', 'Ġshows', 'Ġon', 'Ġcable', 'Ġhad']... → ' minute children 's shows on cable had...'\n",
      "      Unraveling node0: f12ecc66075dd5f7d61585e1db6cdcce372be3b1...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġdemonstrated', 'Ġthat', 'Ġchildren', \"Ġ'\", 's', 'Ġattention', 'Ġcould', 'Ġbe']... → ' demonstrated that children 's attention could be...'\n",
      "      Unraveling node0: 1ca716cc8290d17fabbc28a731485cfb55cc7c12...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġsustained', 'Ġfor', 'Ġlonger', 'Ġperiods', 'Ġof', 'Ġtime', 'Ġ,', 'Ġbut']... → ' sustained for longer periods of time , but...'\n",
      "      Unraveling node0: 43d30465d6248c07718f645080cb15201d29d84b...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġthe', 'ĠCT', 'W', \"Ġ'\", 's', 'Ġresearchers', 'Ġfound', 'Ġthat']... → ' the CTW 's researchers found that...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "\n",
      "  Unraveling FUTURE (predicted next) from level 1...\n",
      "  Unraveling 1 future events from level 1\n",
      "  Unraveling future pattern: f71047edce4eec0c...\n",
      "    Unraveling node1: PTRN|f71047edce4eec0c01a0d0275efc75a2086ef2e3...\n",
      "      Pattern has 3 events\n",
      "      → Has 3 children at node0\n",
      "      Unraveling node0: c85da68d9acb10e6c201815bb36e5979a2510a85...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġtheir', 'Ġviewers', 'Ġ,', 'Ġespecially', 'Ġthe', 'Ġyounger', 'Ġones', 'Ġ,']... → ' their viewers , especially the younger ones ,...'\n",
      "      Unraveling node0: a6071da290774c10f30eab76a4cc4806e7681466...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġlost', 'Ġattention', 'Ġin', 'ĠS', 'esame', 'ĠStreet', 'Ġafter', 'Ġ40']... → ' lost attention in Sesame Street after 40...'\n",
      "      Unraveling node0: ccb10d1533dd7d7f092a616eeee8ec95a09b628e...\n",
      "        Pattern has 4 events\n",
      "        → Tokens: ['Ġto', 'Ġ45', 'Ġminutes', 'Ġ.']... → ' to 45 minutes ....'\n",
      "      ✓ Unraveled to 20 total tokens\n",
      "  Total future tokens: 20\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 20\n",
      "     Total tokens: 84\n",
      "========================================\n",
      "  Present:  format was not the best way to attract young children 's attention . The growth of home videos during the ' 80s and the increase of thirty @-@ minute children 's shows on cable had demonstrated that children 's attention could be sustained for longer periods of time , but the CTW 's researchers found that\n",
      "----------------------------------------\n",
      "  Future:  their viewers , especially the younger ones , lost attention in Sesame Street after 40 to 45 minutes .\n",
      "========================================\n",
      "  Text preview:  format was not the best way to attract young children 's attention . The growth of home videos during the ' 80s and the increase of thirty @-@ minute children 's shows on cable had demonstrated that children 's attention could be sustained for longer periods of time , but the CTW 's researchers found that their viewers , especially the younger ones , lost attention in Sesame Street after 40 to 45 minutes .\n",
      "\n",
      "Prediction 5/5:\n",
      "  Matched Pattern: 412270be5aeae632c1d40fb844496128bb76d96f...\n",
      "  Level: node2\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.0265\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1317\n",
      "    Bayesian Likelihood:    0.1538\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.1538\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "\n",
      "  Extracting PRESENT tokens from pred['present'] field...\n",
      "    Unraveling present pattern: fbf492fd059adee9... (from level 1)\n",
      "    Unraveling node1: fbf492fd059adee90cacbcf27be321fdf146b5cd...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 286f0957705d68406faa72f8d351e79f206d82c3...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠCalifornia', 'ĠâĢ', 'Ļ', 'Ġs', 'Ġmajor', 'Ġmetro', 'Ġmarkets', 'Ġ.']... → ' California ’ s major metro markets ....'\n",
      "      Unraveling node0: c2b83453aced0ae55dbeb448c9dd61469e9eb681...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠThe', 'Ġstudy', 'Ġalso', 'Ġanalyzed', 'Ġnational', 'Ġand', 'Ġstate', 'Ġ@']... → ' The study also analyzed national and state @...'\n",
      "      Unraveling node0: cc4a60c4478611695c30a47196c162f9e96d5c4c...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['-', '@', 'Ġlevel', 'ĠJ', '.', 'D', '.', 'ĠPower']... → '-@ level J.D. Power...'\n",
      "      Unraveling node0: cbd7876dd17d636ad55a6e5bb7d19d13834da617...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ2013', 'ĠSales', 'ĠSatisf', 'action', 'ĠIndex', 'Ġ(', 'ĠSS', 'I']... → ' 2013 Sales Satisfaction Index ( SSI...'\n",
      "      Unraveling node0: e6a3ae510c4f1fe70cfb124f6dd55442ed829063...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġ)', 'Ġstudy', 'Ġdata', 'Ġon', 'Ġcustomer', 'Ġsatisfaction', 'Ġwith', 'Ġnew']... → ' ) study data on customer satisfaction with new...'\n",
      "      Unraveling node0: 160f17431ddfd4afaf90053ef9b8a5f5bb11c441...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġcar', 'Ġdealers', 'hips', 'Ġand', 'ĠTesla', 'Ġretail', 'Ġstores', 'Ġ.']... → ' car dealerships and Tesla retail stores ....'\n",
      "      Unraveling node0: 8b2a94c1baa4558cddfba52a1108b40ef182043f...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['ĠThe', 'Ġresearchers', 'Ġfound', 'Ġthat', 'Ġbuyers', 'Ġof', 'Ġplug', 'Ġ@']... → ' The researchers found that buyers of plug @...'\n",
      "      Unraveling node0: 136cd712e8b9afaa5a860da23aa1064ad8feccde...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['-', '@', 'Ġin', 'Ġelectric', 'Ġvehicles', 'Ġwere', 'Ġsignificantly', 'Ġless']... → '-@ in electric vehicles were significantly less...'\n",
      "      ✓ Unraveled to 64 total tokens\n",
      "\n",
      "  Unraveling FUTURE (predicted next) from level 1...\n",
      "  Unraveling 1 future events from level 1\n",
      "  Unraveling future pattern: dd9c57a9b8843a1e...\n",
      "    Unraveling node1: PTRN|dd9c57a9b8843a1ebb95b0fc7d1b13b779ecdd2e...\n",
      "      Pattern has 8 events\n",
      "      → Has 8 children at node0\n",
      "      Unraveling node0: 9ddc7102d023e85d0e85c93efb66311409a340de...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġsatisfied', 'Ġand', 'Ġrated', 'Ġthe', 'Ġdealer', 'Ġpurchase', 'Ġexperience', 'Ġmuch']... → ' satisfied and rated the dealer purchase experienc...'\n",
      "      Unraveling node0: 7ad11541061e71e68b6c551205924e14fb185651...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġlower', 'Ġthan', 'Ġbuyers', 'Ġof', 'Ġnon', 'Ġ@', '-', '@']... → ' lower than buyers of non @-@...'\n",
      "      Unraveling node0: 8c0a0b89e7a95ef8e7acb6b5272e6035e376c268...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġpremium', 'Ġconventional', 'Ġcars', 'Ġ,', 'Ġwhile', 'ĠTesla', 'ĠMotors', 'Ġearned']... → ' premium conventional cars , while Tesla Motors ea...'\n",
      "      Unraveling node0: 49842d0d5173602f4e8277ac6edd79e7a40923e0...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġindustry', 'Ġ@', '-', '@', 'Ġhigh', 'Ġscores', 'Ġ.', 'ĠAccording']... → ' industry @-@ high scores . According...'\n",
      "      Unraveling node0: d9168f8b9044048f824d12a46c398410432ed6c9...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġto', 'Ġthe', 'Ġfindings', 'Ġ,', 'Ġplug', 'Ġ@', '-', '@']... → ' to the findings , plug @-@...'\n",
      "      Unraveling node0: 2e3b16eeb6cbf580cca5ed384a7fd40de77a851a...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġin', 'Ġbuyers', 'Ġexpect', 'Ġmore', 'Ġfrom', 'Ġdealers', 'Ġthan', 'Ġconventional']... → ' in buyers expect more from dealers than conventio...'\n",
      "      Unraveling node0: ca382bd5d16a19c4170b0b80df1564c91ac625f7...\n",
      "        Pattern has 8 events\n",
      "        → Tokens: ['Ġbuyers', 'Ġ,', 'Ġincluding', 'Ġproduct', 'Ġknowledge', 'Ġand', 'Ġsupport', 'Ġthat']... → ' buyers , including product knowledge and support ...'\n",
      "      Unraveling node0: 652d0056f2048d4d82b3bfee19fa657a1b098fbe...\n",
      "        Pattern has 5 events\n",
      "        → Tokens: ['Ġextends', 'Ġbeyond', 'Ġtraditional', 'Ġofferings', 'Ġ.']... → ' extends beyond traditional offerings ....'\n",
      "      ✓ Unraveled to 61 total tokens\n",
      "  Total future tokens: 61\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 64\n",
      "     Future tokens: 61\n",
      "     Total tokens: 125\n",
      "========================================\n",
      "  Present:  California ’ s major metro markets . The study also analyzed national and state @-@ level J.D. Power 2013 Sales Satisfaction Index ( SSI ) study data on customer satisfaction with new car dealerships and Tesla retail stores . The researchers found that buyers of plug @-@ in electric vehicles were significantly less\n",
      "----------------------------------------\n",
      "  Future:  satisfied and rated the dealer purchase experience much lower than buyers of non @-@ premium conventional cars , while Tesla Motors earned industry @-@ high scores . According to the findings , plug @-@ in buyers expect more from dealers than conventional buyers , including product knowledge and support that extends beyond traditional offerings .\n",
      "========================================\n",
      "  Text preview:  California ’ s major metro markets . The study also analyzed national and state @-@ level J.D. Power 2013 Sales Satisfaction Index ( SSI ) study data on customer satisfaction with new car dealerships and Tesla retail stores . The researchers found that buyers of plug @-@ in electric vehicles were significantly less satisfied and rated the dealer purchase experience much lower than buyers of non @-@ premium conventional cars , while Tesla Motors earned industry @-@ high scores . According to the findings , plug @-@ in buyers expect more from dealers than conventional buyers , including product knowledge and support that extends beyond traditional offerings .\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 5 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    Potential:              1.1114\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1142\n",
      "    Bayesian Likelihood:    0.1333\n",
      "    Evidence:               0.2000\n",
      "    Similarity:             0.1333\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "  Text: The miners found that the row of cottages that served as their homes had been devastated and some of their families killed , seemingly at random . One found his family alive and safe in a makeshift hospital , but another emerged to discover his wife and four children had died . Fifteen @-@ year @-@ old Lillian Clark , working a late shift that night in the town 's boarding house , had been given permission to stay overnight for the first time . She was the only member of her family to survive . Her father was working outside the mine when the slide hit , while her mother and six siblings were buried in their home . All 12 men living at the CPR work camp were killed , but 128 more who were scheduled to move into the camp the day before the slide had not arrived — the train that was supposed to take them there from Morrissey , British Columbia , failed to pick them up . The Spokane Flyer , a passenger train heading west from Lethbridge , was saved by CPR brakeman Sid Choquette , one of two men who rushed across the rock @-@ strewn ground to warn the train that the track had been buried under the slide . Through falling rocks and a dust cloud that impaired his visibility , Choquette ran for 2 kilometres ( 1 @.@ 2 mi ) to warn the oncoming locomotive of the danger . The CPR gave him a letter of commendation and a $ 25 cheque in recognition of his heroism .\n",
      "\n",
      "\n",
      "Result 2:\n",
      "  Metrics:\n",
      "    Potential:              1.1114\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1142\n",
      "    Bayesian Likelihood:    0.1333\n",
      "    Evidence:               0.2000\n",
      "    Similarity:             0.1333\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "  Text: The investigation found that as the B @-@ 52 entered its final turn sequence around the tower , its indicated airspeed ( IAS ) was 182 knots ( 337 km / h ; 209 mph ) . Although Holland increased the engine power after starting the turn , his input came too late to maintain the aircraft 's airspeed , as the B @-@ 52 turbofan engines take up to eight seconds to respond to throttle commands . Even though the airspeed indicator was available to all four aircrew members , the aircraft 's airspeed was allowed to continue to decrease . Eight seconds before impact , the aircraft 's IAS had deteriorated to 145 knots ( 269 km / h ; 167 mph ) and the aircraft 's bank angle increased past 60 ° . At this time Holland or McGeehan applied full right spoiler , right rudder , and nose @-@ up elevator , and the aircraft entered a turning flight stall ( also called accelerated stall ) . This phenomenon is a stall that occurs at a higher airspeed than the design stall speed – which always refers to straight and level flight – because of the fact that the aircraft is turning . Due to the bank of 60 ° or more , the stall speed for the aircraft at that moment was 147 knots ( 272 km / h ; 169 mph ) . Thus , flying 2 knots slower , the aircraft stalled , without having sufficient altitude to recover before striking the ground .\n",
      "\n",
      "\n",
      "Result 3:\n",
      "  Metrics:\n",
      "    Potential:              1.0795\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1429\n",
      "    Evidence:               0.2500\n",
      "    Similarity:             0.1429\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "  Text: The commission found that the Newport News Shipbuilding and Drydock Company bid was lowest on one battleship , and the Fore River Shipbuilding Company was lowest on the other . Despite a British attempt to allow the Armstrong Whitworth @-@ Vickers team to lower their price by $ 570 @,@ 000 , prompt American diplomacy granting various assurances regarding recent events between the United States and Brazil , the upcoming 1910 Pan @-@ American Conference , and a guarantee of American participation in the Argentine centennial celebrations secured the battleship contracts for Fore River on 21 January 1910 . The maximum price Fore River tendered , $ 10 @.@ 7 million , underbid the British by more than $ 973 @,@ 000 , but their ship 's displacement was 2 @,@ 000 long tons ( 2 @,@ 000 t ) smaller , the belt armor was 2 inches ( 51 mm ) thinner , and the top speed was slightly lower . Orders for the twelve destroyers were divided among Britain , France , and Germany .\n",
      "\n",
      "\n",
      "Result 4:\n",
      "  Metrics:\n",
      "    Potential:              1.0795\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1223\n",
      "    Bayesian Likelihood:    0.1429\n",
      "    Evidence:               0.2500\n",
      "    Similarity:             0.1429\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "  Text:  format was not the best way to attract young children 's attention . The growth of home videos during the ' 80s and the increase of thirty @-@ minute children 's shows on cable had demonstrated that children 's attention could be sustained for longer periods of time , but the CTW 's researchers found that their viewers , especially the younger ones , lost attention in Sesame Street after 40 to 45 minutes .\n",
      "\n",
      "\n",
      "Result 5:\n",
      "  Metrics:\n",
      "    Potential:              1.0265\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     0.1317\n",
      "    Bayesian Likelihood:    0.1538\n",
      "    Evidence:               0.3333\n",
      "    Similarity:             0.1538\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1460\n",
      "  Text:  California ’ s major metro markets . The study also analyzed national and state @-@ level J.D. Power 2013 Sales Satisfaction Index ( SSI ) study data on customer satisfaction with new car dealerships and Tesla retail stores . The researchers found that buyers of plug @-@ in electric vehicles were significantly less satisfied and rated the dealer purchase experience much lower than buyers of non @-@ premium conventional cars , while Tesla Motors earned industry @-@ high scores . According to the findings , plug @-@ in buyers expect more from dealers than conventional buyers , including product knowledge and support that extends beyond traditional offerings .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try your own input!\n",
    "input_text = \"The researchers found that\"  # <-- Change this\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    # max_predictions=100,\n",
    "    verbose=True,\n",
    "    verbose_unravel=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    Potential:              {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:             {result['confidence']:.4f}\")\n",
    "    print(f\"    Bayesian Posterior:     {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:    {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    Evidence:               {result['evidence']:.4f}\")\n",
    "    print(f\"    Similarity:             {result['similarity']:.4f}\")\n",
    "    print(f\"    SNR:                    {result['snr']:.4f}\")\n",
    "    print(f\"    Predictive Information: {result['predictive_information']:.4f}\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Known sample\n",
    "\n",
    "The following includes exact phrases that are known to have been learned by the agent.\n",
    "\n",
    "First, we'll provide the STM with a little context. This won't produce robust predictions. But, next we'll take the output as predicted, add it to the STM, and request a second prediction.\n",
    "\n",
    "Here's the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input:  Among flukes, the most common\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input:  Among flukes, the most common\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (7): ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 7 tokens\n",
      "  node0 STM: 7 events\n",
      "     STM: [['ĠAmong'], ['Ġfl'], ['ukes'], [','], ['Ġthe'], ['Ġmost'], ['Ġcommon']]\n",
      "✓ Got 1 predictions\n",
      "  Sample predictions:\n",
      "    1. 6850d8ef6abf023e778693c4d5d9986db464e5cd... (conf: 0.857)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 1 pattern names as 1 event:\n",
      "    1. PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node1\n",
      "     STM has 1 event(s)\n",
      "     First event: ['PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd']... (1 symbols)\n",
      "     Possible reasons:\n",
      "       - No node1 patterns in KB match this sequence of node0 patterns\n",
      "       - recall_threshold too high (current: 0.6)\n",
      "       - Pattern names don't match KB format\n",
      "⚠ No predictions from node1 - stopping cascade for this chunk\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 1 predictions\n",
      "  node1: 0 predictions\n",
      "  node2: 0 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "✓ Using node0 predictions (1 patterns with usable future data)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/1:\n",
      "  Matched Pattern: 6850d8ef6abf023e778693c4d5d9986db464e5cd...\n",
      "  Level: node0\n",
      "  Present Events: 7 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              1.8599\n",
      "    Confidence:             0.8571\n",
      "    Bayesian Posterior:     1.0000\n",
      "    Bayesian Likelihood:    0.8000\n",
      "    Evidence:               0.7500\n",
      "    Similarity:             0.8000\n",
      "    SNR:                    0.8462\n",
      "    Predictive Information: 0.8000\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 7\n",
      "     Future tokens: 1\n",
      "     Total tokens: 8\n",
      "========================================\n",
      "  Present:  Among flukes , the most common\n",
      "----------------------------------------\n",
      "  Future:  in\n",
      "========================================\n",
      "  Text preview:  Among flukes , the most common in\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 1 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    Potential:              1.8599\n",
      "    Confidence:             0.8571\n",
      "    Bayesian Posterior:     1.0000\n",
      "    Bayesian Likelihood:    0.8000\n",
      "    Evidence:               0.7500\n",
      "    Similarity:             0.8000\n",
      "    SNR:                    0.8462\n",
      "    Predictive Information: 0.8000\n",
      "  Text length: 34 chars\n",
      "  Text:  Among flukes , the most common in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with various inputs to see generation quality\n",
    "# The system now combines PRESENT (matched pattern) + FUTURE (predicted continuation)\n",
    "\n",
    "# Example inputs - uncomment the one you want to test:\n",
    "\n",
    "# input_text = \"The cat sat\"\n",
    "# input_text = \"the most common in North American wolves\"\n",
    "# input_text = \"Tapeworms generally cause little harm in wolves, though this depends on the number and size of the parasites, and the sensitivity of the host.\"\n",
    "\n",
    "# This input finds a rich pattern about wolf parasites (830 tokens total with present+future):\n",
    "input_text = \" Among flukes, the most common\"  # longer gets more: \" Among flukes, the most common in\"  \n",
    "\n",
    "# Note: The space at the beginning and \"in\" at the end are important for matching the correct pattern\n",
    "# This demonstrates how exact phrasing affects pattern matching\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    verbose=True,\n",
    "    verbose_unravel=False  # Set to True to see detailed unraveling\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    Potential:              {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:             {result['confidence']:.4f}\")\n",
    "    print(f\"    Bayesian Posterior:     {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:    {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    Evidence:               {result['evidence']:.4f}\")\n",
    "    print(f\"    Similarity:             {result['similarity']:.4f}\")\n",
    "    print(f\"    SNR:                    {result['snr']:.4f}\")\n",
    "    print(f\"    Predictive Information: {result['predictive_information']:.4f}\")\n",
    "    print(f\"  Text length: {len(result['text'])} chars\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we'll add the predicted output, \"in\", back and get more predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input:  Among flukes, the most common in\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input:  Among flukes, the most common in\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (8): ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 8 tokens\n",
      "  node0 STM: 8 events\n",
      "     STM: [['ĠAmong'], ['Ġfl'], ['ukes'], [','], ['Ġthe'], ['Ġmost'], ['Ġcommon'], ['Ġin']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. 6850d8ef6abf023e778693c4d5d9986db464e5cd... (conf: 0.875)\n",
      "    2. a83e489f884347bdaa07738c8f5bc077b6bb3a5b... (conf: 1.000)\n",
      "    3. f506059a84fdd8401fa3a74f5fc023ad09046240... (conf: 1.000)\n",
      "\n",
      "--- NODE1 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd...\n",
      "    2. PTRN|a83e489f884347bdaa07738c8f5bc077b6bb3a5b...\n",
      "    3. PTRN|f506059a84fdd8401fa3a74f5fc023ad09046240...\n",
      "✓ Observed (1 event)\n",
      "  node1 STM: 1 events\n",
      "     STM: [['PTRN|0388f2fc9a8818fec1a6b3a720e12f7c365c3a7e', 'PTRN|1d98518581f3e67fb3ae4a71d09871effa105603', 'PTRN|2d5ee11633b83c53128ca4adaffb9a834958b4b0', 'PTRN|5a4794b77ab147e09cb6eb966f14f3c0323d1908', 'PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd', 'PTRN|9441a927cabc7fda12b7cd0af9c2cdf1c7bff18a', 'PTRN|a83e489f884347bdaa07738c8f5bc077b6bb3a5b', 'PTRN|cf6332b8378584e30eb132371ac904bf253c28b0', 'PTRN|f39f81efa356928e33e9ee954f538564d01956e2', 'PTRN|f506059a84fdd8401fa3a74f5fc023ad09046240']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. d1a599707a223c611f6547a56077af442f9e1e22... (conf: 1.000)\n",
      "    2. 4cf3bea7fd32a154dfe4755c1d0ffe4e501b1752... (conf: 1.000)\n",
      "    3. 825efe1d6bb323913dc5cfcccec1b931f1e7d5ca... (conf: 1.000)\n",
      "\n",
      "--- NODE2 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|d1a599707a223c611f6547a56077af442f9e1e22...\n",
      "    2. PTRN|4cf3bea7fd32a154dfe4755c1d0ffe4e501b1752...\n",
      "    3. PTRN|825efe1d6bb323913dc5cfcccec1b931f1e7d5ca...\n",
      "✓ Observed (1 event)\n",
      "  node2 STM: 1 events\n",
      "     STM: [['PTRN|38d6f2b5cdb9cb901e67812e7c5490a6ca463c83', 'PTRN|3fc62e9d112a82c1ce7a2f1e35bbd53f02a18f9c', 'PTRN|4cf3bea7fd32a154dfe4755c1d0ffe4e501b1752', 'PTRN|62bbcfb988bd6ac9fd251186f8891d893f668d60', 'PTRN|825efe1d6bb323913dc5cfcccec1b931f1e7d5ca', 'PTRN|9f489ad7c55df7ab01616c47fe0f8696daae354d', 'PTRN|d1a599707a223c611f6547a56077af442f9e1e22', 'PTRN|d27569023de113580fcb561020bd5f54a22bb5c3', 'PTRN|d99df2e570ec37175e64e01695b6ec97c58c2526', 'PTRN|ee519ec3fae2d18fc3d4b16d506f87b35bb2788d']]\n",
      "✓ Got 10 predictions\n",
      "  Sample predictions:\n",
      "    1. 1b40aeba6a261e59a94cd90628ae0fd196990ab9... (conf: 1.000)\n",
      "    2. 295810dd747d630302c0c372e50ce05f871ea91e... (conf: 1.000)\n",
      "    3. 09a47b544bea1e086f91108af65318c142fc1ab5... (conf: 1.000)\n",
      "\n",
      "--- NODE3 ---\n",
      "Sending 10 pattern names as 1 event:\n",
      "    1. PTRN|1b40aeba6a261e59a94cd90628ae0fd196990ab9...\n",
      "    2. PTRN|295810dd747d630302c0c372e50ce05f871ea91e...\n",
      "    3. PTRN|09a47b544bea1e086f91108af65318c142fc1ab5...\n",
      "✓ Observed (1 event)\n",
      "  node3 STM: 1 events\n",
      "     STM: [['PTRN|09a47b544bea1e086f91108af65318c142fc1ab5', 'PTRN|1b40aeba6a261e59a94cd90628ae0fd196990ab9', 'PTRN|295810dd747d630302c0c372e50ce05f871ea91e', 'PTRN|3eb8a9dc41aac5ae72b23e12142165d73989736b', 'PTRN|5f520053ce5d919f0c57c0f1cdaa3b7690ac7eef', 'PTRN|62cd4463097d7a34d0c6a269eaf16ed812cb586e', 'PTRN|8a99e741416114cec14bbe91da750c4971058a60', 'PTRN|b3da2ef1712b60d2b461b19873b9d780d017dae3', 'PTRN|b8ce2f92f2f6b52195f4e67a41edb8d63f7d0cb1', 'PTRN|f89db2a6db5a02859d5ed3303cd868a9a9e0d99f']]\n",
      "✓ Got 1 predictions\n",
      "  Sample predictions:\n",
      "    1. 04f57fef3b3f9b0f33b9d04987b16f56755d8c63... (conf: 1.000)\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 10 predictions\n",
      "  node1: 10 predictions\n",
      "  node2: 10 predictions\n",
      "  node3: 1 predictions\n",
      "\n",
      "✓ Using node3 predictions (1 patterns with usable future data)\n",
      "\n",
      "================================================================================\n",
      "TOP-DOWN UNRAVELING (Present + Future)\n",
      "================================================================================\n",
      "\n",
      "** KEY: Combining pred['present'] (matched tokens) + pred['future'] (predicted next)\n",
      "** pred['present'] contains exact matched tokens, pred['future'] contains predicted next\n",
      "\n",
      "Prediction 1/1:\n",
      "  Matched Pattern: 04f57fef3b3f9b0f33b9d04987b16f56755d8c63...\n",
      "  Level: node3\n",
      "  Present Events: 1 tokens\n",
      "  Future Events: 1 events\n",
      "\n",
      "  Prediction Metrics:\n",
      "    Potential:              0.0455\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     1.0000\n",
      "    Bayesian Likelihood:    0.1667\n",
      "    Evidence:               0.5000\n",
      "    Similarity:             0.1667\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1667\n",
      "\n",
      "  ✓ Generated:\n",
      "     Present tokens: 512\n",
      "     Future tokens: 159\n",
      "     Total tokens: 671\n",
      "========================================\n",
      "  Present: Endoparasites known to infect wolves include protozoans and helminths ( flukes , tapeworms , roundworms and thorny @-@ headed worms ) . Of 30 @,@ 000 protozoan species , only a few have been recorded to infect wolves : Isospora , Toxoplasma , Sarcocystis , Babesia , and Giardia . Wolves may carry Neospora caninum , which is of particular concern to farmers , as the disease can be spread to livestock , with infected animals being 3 – 13 times more likely to miscarry than those not infected . Among flukes , the most common in North American wolves is Alaria , which infects small rodents and amphibians that are eaten by wolves . Upon reaching maturity , Alaria migrates to the wolf 's intestine , but harms it little . Metorchis conjunctus , which enters wolves through eating fish , infects the wolf 's liver or gall bladder , causing liver disease , inflammation of the pancreas , and emaciation . Most other fluke species reside in the wolf 's intestine , though Paragonimus westermani lives in the lungs . Tapeworms are commonly found in wolves , as their primary hosts are ungulates , small mammals , and fish , which wolves feed upon . Tapeworms generally cause little harm in wolves , though this depends on the number and size of the parasites , and the sensitivity of the host . Symptoms often include constipation , toxic and allergic reactions , irritation of the intestinal mucosa , and malnutrition . Infections by the tapeworm Echinococcus granulosus in ungulate populations tend to increase in areas with high wolf densities , as wolves can shed Echinoccocus eggs in their feces onto grazing areas . Wolves can carry over 30 roundworm species , though most roundworm infections appear benign , depending on the number of worms and the age of the host . Ancylostoma caninum attaches itself on the intestinal wall to feed on the host 's blood , and can cause hyperchromic anemia , emaciation , diarrhea , and possibly death . Toxocara canis , a hookworm known to infect wolf pups in utero , can cause intestinal irritation , bloating , vomiting , and diarrhea . Wolves may catch Dioctophyma renale from minks , which infects the kidneys , and can grow to lengths of 100 cm . D. renale causes the\n",
      "----------------------------------------\n",
      "  Future:  complete destruction of the kidney 's functional tissue , and can be fatal if both kidneys are infected . Wolves can tolerate low levels of Dirofilaria immitis for many years without showing any ill effects , though high levels can kill wolves through cardiac enlargement and congestive hepatopathy . Wolves probably become infected with Trichinella spiralis by eating infected ungulates . Although T. spiralis isn 't known to produce clinical signs in wolves , it can cause emaciation , salivation , and crippling muscle pains in dogs . Thorny @-@ headed worms rarely infect wolves , though three species have been identified in Russian wolves : Nicolla skrjabini , Macrocantorhynchus catulinus , and Moniliformis moniliformis .\n",
      "========================================\n",
      "  Text preview: Endoparasites known to infect wolves include protozoans and helminths ( flukes , tapeworms , roundworms and thorny @-@ headed worms ) . Of 30 @,@ 000 protozoan species , only a few have been recorded to infect wolves : Isospora , Toxoplasma , Sarcocystis , Babesia , and Giardia . Wolves may carry Neospora caninum , which is of particular concern to farmers , as the disease can be spread to livestock , with infected animals being 3 – 13 times more likely to miscarry than those not infected . Among flukes , the most common in North American wolves is Alaria , which infects small rodents and amphibians that are eaten by wolves . Upon reaching maturity , Alaria migrates to the wolf 's intestine , but harms it little . Metorchis conjunctus , which enters wolves through eating fish , infects the wolf 's liver or gall bladder , causing liver disease , inflammation of the pancreas , and emaciation . Most other fluke species reside in the wolf 's intestine , though Paragonimus westermani lives in the lungs . Tapeworms are commonly found in wolves , as their primary hosts are ungulates , small mammals , and fish , which wolves feed upon . Tapeworms generally cause little harm in wolves , though this depends on the number and size of the parasites , and the sensitivity of the host . Symptoms often include constipation , toxic and allergic reactions , irritation of the intestinal mucosa , and malnutrition . Infections by the tapeworm Echinococcus granulosus in ungulate populations tend to increase in areas with high wolf densities , as wolves can shed Echinoccocus eggs in their feces onto grazing areas . Wolves can carry over 30 roundworm species , though most roundworm infections appear benign , depending on the number of worms and the age of the host . Ancylostoma caninum attaches itself on the intestinal wall to feed on the host 's blood , and can cause hyperchromic anemia , emaciation , diarrhea , and possibly death . Toxocara canis , a hookworm known to infect wolf pups in utero , can cause intestinal irritation , bloating , vomiting , and diarrhea . Wolves may catch Dioctophyma renale from minks , which infects the kidneys , and can grow to lengths of 100 cm . D. renale causes the complete destruction of the kidney 's functional tissue , and can be fatal if both kidneys are infected . Wolves can tolerate low levels of Dirofilaria immitis for many years without showing any ill effects , though high levels can kill wolves through cardiac enlargement and congestive hepatopathy . Wolves probably become infected with Trichinella spiralis by eating infected ungulates . Although T. spiralis isn 't known to produce clinical signs in wolves , it can cause emaciation , salivation , and crippling muscle pains in dogs . Thorny @-@ headed worms rarely infect wolves , though three species have been identified in Russian wolves : Nicolla skrjabini , Macrocantorhynchus catulinus , and Moniliformis moniliformis .\n",
      "\n",
      "================================================================================\n",
      "GENERATION COMPLETE: 1 results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Metrics:\n",
      "    Potential:              0.0455\n",
      "    Confidence:             1.0000\n",
      "    Bayesian Posterior:     1.0000\n",
      "    Bayesian Likelihood:    0.1667\n",
      "    Evidence:               0.5000\n",
      "    Similarity:             0.1667\n",
      "    SNR:                    -0.6364\n",
      "    Predictive Information: 0.1667\n",
      "  Text length: 2943 chars\n",
      "  Text: Endoparasites known to infect wolves include protozoans and helminths ( flukes , tapeworms , roundworms and thorny @-@ headed worms ) . Of 30 @,@ 000 protozoan species , only a few have been recorded to infect wolves : Isospora , Toxoplasma , Sarcocystis , Babesia , and Giardia . Wolves may carry Neospora caninum , which is of particular concern to farmers , as the disease can be spread to livestock , with infected animals being 3 – 13 times more likely to miscarry than those not infected . Among flukes , the most common in North American wolves is Alaria , which infects small rodents and amphibians that are eaten by wolves . Upon reaching maturity , Alaria migrates to the wolf 's intestine , but harms it little . Metorchis conjunctus , which enters wolves through eating fish , infects the wolf 's liver or gall bladder , causing liver disease , inflammation of the pancreas , and emaciation . Most other fluke species reside in the wolf 's intestine , though Paragonimus westermani lives in the lungs . Tapeworms are commonly found in wolves , as their primary hosts are ungulates , small mammals , and fish , which wolves feed upon . Tapeworms generally cause little harm in wolves , though this depends on the number and size of the parasites , and the sensitivity of the host . Symptoms often include constipation , toxic and allergic reactions , irritation of the intestinal mucosa , and malnutrition . Infections by the tapeworm Echinococcus granulosus in ungulate populations tend to increase in areas with high wolf densities , as wolves can shed Echinoccocus eggs in their feces onto grazing areas . Wolves can carry over 30 roundworm species , though most roundworm infections appear benign , depending on the number of worms and the age of the host . Ancylostoma caninum attaches itself on the intestinal wall to feed on the host 's blood , and can cause hyperchromic anemia , emaciation , diarrhea , and possibly death . Toxocara canis , a hookworm known to infect wolf pups in utero , can cause intestinal irritation , bloating , vomiting , and diarrhea . Wolves may catch Dioctophyma renale from minks , which infects the kidneys , and can grow to lengths of 100 cm . D. renale causes the complete destruction of the kidney 's functional tissue , and can be fatal if both kidneys are infected . Wolves can tolerate low levels of Dirofilaria immitis for many years without showing any ill effects , though high levels can kill wolves through cardiac enlargement and congestive hepatopathy . Wolves probably become infected with Trichinella spiralis by eating infected ungulates . Although T. spiralis isn 't known to produce clinical signs in wolves , it can cause emaciation , salivation , and crippling muscle pains in dogs . Thorny @-@ headed worms rarely infect wolves , though three species have been identified in Russian wolves : Nicolla skrjabini , Macrocantorhynchus catulinus , and Moniliformis moniliformis .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with various inputs to see generation quality\n",
    "# The system now combines PRESENT (matched pattern) + FUTURE (predicted continuation)\n",
    "\n",
    "# Example inputs - uncomment the one you want to test:\n",
    "\n",
    "# input_text = \"The cat sat\"\n",
    "# input_text = \"the most common in North American wolves\"\n",
    "# input_text = \"Tapeworms generally cause little harm in wolves, though this depends on the number and size of the parasites, and the sensitivity of the host.\"\n",
    "\n",
    "# This input finds a rich pattern about wolf parasites (830 tokens total with present+future):\n",
    "input_text = \" Among flukes, the most common in\"  \n",
    "\n",
    "# Note: The space at the beginning and \"in\" at the end are important for matching the correct pattern\n",
    "# This demonstrates how exact phrasing affects pattern matching\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    verbose=True,\n",
    "    verbose_unravel=False  # Set to True to see detailed unraveling\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"  Metrics:\")\n",
    "    print(f\"    Potential:              {result['potential']:.4f}\")\n",
    "    print(f\"    Confidence:             {result['confidence']:.4f}\")\n",
    "    print(f\"    Bayesian Posterior:     {result['bayesian_posterior']:.4f}\")\n",
    "    print(f\"    Bayesian Likelihood:    {result['bayesian_likelihood']:.4f}\")\n",
    "    print(f\"    Evidence:               {result['evidence']:.4f}\")\n",
    "    print(f\"    Similarity:             {result['similarity']:.4f}\")\n",
    "    print(f\"    SNR:                    {result['snr']:.4f}\")\n",
    "    print(f\"    Predictive Information: {result['predictive_information']:.4f}\")\n",
    "    print(f\"  Text length: {len(result['text'])} chars\")\n",
    "    print(f\"  Text: {result['text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4b: Adjusting Prediction Ensemble Size\n",
    "\n",
    "Control how many predictions flow between levels using `max_predictions`.\n",
    "\n",
    "**Impact of ensemble size**:\n",
    "- **Larger ensembles** (20-50): More context sent to next level → better quality, but slower and potentially noisy\n",
    "- **Smaller ensembles** (3-5): Less context → faster and cleaner, but may miss important patterns\n",
    "- **Recommended**: 5-15 for balanced quality and speed\n",
    "\n",
    "**Use cases**:\n",
    "- Speed-critical applications: Use small ensembles\n",
    "- Quality-critical applications: Use large ensembles\n",
    "- Novel inputs: Use larger ensembles to catch weak matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: Small Ensembles (max_predictions=[3, 3, 3, 3])\n",
      "================================================================================\n",
      "\n",
      "Results: 7 predictions from node2\n",
      "  1. 1b24fb2fd330b0d34938e0d301a137d252e9733b... (conf: 1.000)\n",
      "  2. 525231282744a56dec2cfacaa1f047f2609d0246... (conf: 1.000)\n",
      "\n",
      "================================================================================\n",
      "TEST 2: Large Ensembles (max_predictions=[20, 15, 10, 5])\n",
      "================================================================================\n",
      "\n",
      "Results: 7 predictions from node2\n",
      "  1. 1b24fb2fd330b0d34938e0d301a137d252e9733b... (conf: 1.000)\n",
      "  2. 525231282744a56dec2cfacaa1f047f2609d0246... (conf: 1.000)\n",
      "\n",
      "================================================================================\n",
      "COMPARISON\n",
      "================================================================================\n",
      "Small ensembles: 7 predictions from node2\n",
      "Large ensembles: 7 predictions from node2\n",
      "\n",
      "Larger ensembles provide more context to higher levels.\n",
      "This can lead to different prediction quantities and levels activated.\n"
     ]
    }
   ],
   "source": [
    "# Compare different ensemble sizes using chunk_sizes and max_predictions parameters\n",
    "input_text = \"The cat sat on the mat\"\n",
    "\n",
    "# Test 1: Small ensembles (faster, cleaner)\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: Small Ensembles (max_predictions=[3, 3, 3, 3])\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_small = activate_hierarchy(\n",
    "    input_text=input_text,\n",
    "    verbose=False,\n",
    "    chunk_sizes=CHUNK_SIZES,      # Use global chunk_sizes\n",
    "    max_predictions=[3, 3, 3, 3]  # Small ensembles\n",
    ")\n",
    "\n",
    "# Get predictions from highest level\n",
    "preds_small, level_small = get_prediction_ensemble(results_small, verbose=False)\n",
    "\n",
    "print(f\"\\nResults: {len(preds_small)} predictions from node{level_small}\")\n",
    "for i, pred in enumerate(preds_small[:2]):\n",
    "    print(f\"  {i+1}. {pred['name'][:60]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "\n",
    "# Test 2: Large ensembles (slower, more context)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: Large Ensembles (max_predictions=[20, 15, 10, 5])\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_large = activate_hierarchy(\n",
    "    input_text=input_text,\n",
    "    verbose=False,\n",
    "    chunk_sizes=CHUNK_SIZES,            # Use global chunk_sizes\n",
    "    max_predictions=[20, 15, 10, 5]     # Large ensembles\n",
    ")\n",
    "\n",
    "preds_large, level_large = get_prediction_ensemble(results_large, verbose=False)\n",
    "\n",
    "print(f\"\\nResults: {len(preds_large)} predictions from node{level_large}\")\n",
    "for i, pred in enumerate(preds_large[:2]):\n",
    "    print(f\"  {i+1}. {pred['name'][:60]}... (conf: {pred.get('confidence', 0):.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Small ensembles: {len(preds_small)} predictions from node{level_small}\")\n",
    "print(f\"Large ensembles: {len(preds_large)} predictions from node{level_large}\")\n",
    "print(\"\\nLarger ensembles provide more context to higher levels.\")\n",
    "print(\"This can lead to different prediction quantities and levels activated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Auto-Adjust recall_threshold (Cold Start Handling)\n",
    "\n",
    "If input is very novel (not seen during training), predictions may be empty with default recall_threshold.\n",
    "\n",
    "**auto_adjust_recall=True** will automatically try progressively lower thresholds (0.5, 0.4, 0.3, 0.2, 0.1) until predictions are found.\n",
    "\n",
    "This is useful for:\n",
    "- Novel inputs not in training data\n",
    "- Cold start scenarios\n",
    "- Graceful degradation (lower quality predictions rather than nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# HIERARCHICAL TEXT GENERATION (PRESENT + FUTURE)\n",
      "################################################################################\n",
      "\n",
      "Input: Quantum entanglement demonstrates\n",
      "\n",
      "================================================================================\n",
      "BOTTOM-UP ACTIVATION (Chunk-by-Chunk Cascading)\n",
      "================================================================================\n",
      "Input: Quantum entanglement demonstrates\n",
      "Config: chunk_sizes=[8, 8, 8, 8], max_pred=[10, 10, 10, 10]\n",
      "\n",
      "Tokens (6): ['Quant', 'um', 'Ġent', 'ang', 'lement', 'Ġdemonstrates']\n",
      "\n",
      "Chunks (1) with chunk_size=8:\n",
      "  Chunk 0: ['Quant', 'um', 'Ġent', 'ang', 'lement', 'Ġdemonstrates']\n",
      "\n",
      "--- INITIALIZING: Clearing all nodes' STM ---\n",
      "✓ Cleared node0 STM\n",
      "✓ Cleared node1 STM\n",
      "✓ Cleared node2 STM\n",
      "✓ Cleared node3 STM\n",
      "\n",
      "================================================================================\n",
      "CHUNK 1/1: ['Quant', 'um', 'Ġent', 'ang', 'lement', 'Ġdemonstrates']\n",
      "================================================================================\n",
      "✓ Cleared node0 STM for new chunk\n",
      "\n",
      "--- NODE0 ---\n",
      "✓ Observed 6 tokens\n",
      "  node0 STM: 6 events\n",
      "     STM: [['Quant'], ['um'], ['Ġent'], ['ang'], ['lement'], ['Ġdemonstrates']]\n",
      "✓ Got 0 predictions\n",
      "  ⚠ No predictions from node0\n",
      "     Possible reasons:\n",
      "       - No patterns in KB match this token sequence\n",
      "       - recall_threshold too high (current: 0.6)\n",
      "⚠ No predictions from node0 - stopping cascade for this chunk\n",
      "\n",
      "================================================================================\n",
      "ACTIVATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Final prediction counts (from last chunk cascade):\n",
      "  node0: 0 predictions\n",
      "  node1: 0 predictions\n",
      "  node2: 0 predictions\n",
      "  node3: 0 predictions\n",
      "\n",
      "⚠ No predictions with non-empty 'future' field at any level\n",
      "   (Input may be novel, or predictions don't contain future data)\n",
      "\n",
      "================================================================================\n",
      "AUTO-ADJUSTING RECALL_THRESHOLD\n",
      "================================================================================\n",
      "⚠ No predictions with usable 'future' data.\n",
      "Trying progressively lower thresholds...\n",
      "\n",
      "Trying recall_threshold=0.5...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KATOClient' object has no attribute 'update_genes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If input is very novel (not seen during training), predictions may be empty\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# auto_adjust_recall=True will automatically try lower thresholds\u001b[39;00m\n\u001b[1;32m      4\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantum entanglement demonstrates\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Novel input\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_predictions=3,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_adjust_recall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable auto-adjustment\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINAL RESULTS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 166\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(input_text, max_predictions, verbose, verbose_unravel, recall_threshold_overrides, auto_adjust_recall)\u001b[0m\n\u001b[1;32m    163\u001b[0m     node\u001b[38;5;241m.\u001b[39mclear_stm()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Re-activate hierarchy with new threshold\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mactivate_hierarchy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Suppress verbose for retry attempts\u001b[39;49;00m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecall_threshold_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Check if we got predictions with usable future\u001b[39;00m\n\u001b[1;32m    173\u001b[0m predictions, used_level \u001b[38;5;241m=\u001b[39m get_prediction_ensemble(\n\u001b[1;32m    174\u001b[0m     all_predictions,\n\u001b[1;32m    175\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    176\u001b[0m )\n",
      "Cell \u001b[0;32mIn[10], line 61\u001b[0m, in \u001b[0;36mactivate_hierarchy\u001b[0;34m(input_text, verbose, recall_threshold_overrides, chunk_sizes, max_predictions)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_name, genes \u001b[38;5;129;01min\u001b[39;00m recall_threshold_overrides\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     60\u001b[0m     node_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(node_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_genes\u001b[49m(genes)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m genes:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Updated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m recall_threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KATOClient' object has no attribute 'update_genes'"
     ]
    }
   ],
   "source": [
    "# If input is very novel (not seen during training), predictions may be empty\n",
    "# auto_adjust_recall=True will automatically try lower thresholds\n",
    "\n",
    "input_text = \"Quantum entanglement demonstrates\"  # Novel input\n",
    "\n",
    "results = generate_text(\n",
    "    input_text=input_text,\n",
    "    # max_predictions=3,\n",
    "    auto_adjust_recall=True,  # Enable auto-adjustment\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if results:\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"  Metrics:\")\n",
    "        print(f\"    Potential:              {result['potential']:.4f}\")\n",
    "        print(f\"    Confidence:             {result['confidence']:.4f}\")\n",
    "        print(f\"    Bayesian Posterior:     {result['bayesian_posterior']:.4f}\")\n",
    "        print(f\"    Bayesian Likelihood:    {result['bayesian_likelihood']:.4f}\")\n",
    "        print(f\"    Evidence:               {result['evidence']:.4f}\")\n",
    "        print(f\"    Similarity:             {result['similarity']:.4f}\")\n",
    "        print(f\"    SNR:                    {result['snr']:.4f}\")\n",
    "        print(f\"    Predictive Information: {result['predictive_information']:.4f}\")\n",
    "        print(f\"  Text: {result['text'][:300]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n⚠ No predictions found. Input is completely novel to the system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging & Inspection\n",
    "\n",
    "Useful cells for inspecting the system state and understanding what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Specific Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a specific pattern using KATO API\n",
    "# Replace with an actual pattern name from your predictions\n",
    "# Pattern name can be with or without 'PTRN|' prefix\n",
    "\n",
    "pattern_name = \"92aefd2182f65233aaa2b975b0cc292f967b745c\"\n",
    "level = 0\n",
    "\n",
    "# Strip prefix before querying (handles both formats)\n",
    "clean_name = strip_pattern_prefix(pattern_name)\n",
    "\n",
    "print(f\"Inspecting pattern: {pattern_name}\")\n",
    "print(f\"Queried as: {clean_name}\")\n",
    "print(f\"Level: node{level}\\n\")\n",
    "\n",
    "# ===== Query via KATO API =====\n",
    "print(\"=\"*80)\n",
    "print(\"Pattern Query via KATO API:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get pattern data via KATO API\n",
    "result = nodes[level].get_pattern(clean_name)\n",
    "inner = result.get('pattern', {})\n",
    "\n",
    "if inner.get('status') == 'okay':\n",
    "    pattern_data = inner.get('pattern', {})\n",
    "    metadata = inner.get('metadata', {})\n",
    "    \n",
    "    print(f\"✓ Pattern found\")\n",
    "    print(f\"  Frequency: {metadata.get('frequency', 0)}\")\n",
    "    \n",
    "    observations = pattern_data.get('observations', [])\n",
    "    print(f\"  Observations: {len(observations)} events\")\n",
    "    \n",
    "    print(f\"\\n  Pattern data (first 10 events):\")\n",
    "    for i, obs in enumerate(observations[:10]):\n",
    "        print(f\"    Event {i}: {obs}\")\n",
    "    \n",
    "    if level == 0:\n",
    "        # Decode tokens\n",
    "        tokens = []\n",
    "        for obs in observations:\n",
    "            token_list = obs.get('strings', [])\n",
    "            tokens.extend(token_list)\n",
    "        print(f\"\\n  Decoded tokens ({len(tokens)} total):\")\n",
    "        print(f\"  {' '.join(tokens)}\")\n",
    "    else:\n",
    "        # Show child patterns\n",
    "        print(f\"\\n  Child patterns (first 10):\")\n",
    "        for i, obs in enumerate(observations[:10]):\n",
    "            child_patterns = obs.get('strings', [])\n",
    "            for pattern in child_patterns:\n",
    "                clean_child = strip_pattern_prefix(pattern)\n",
    "                print(f\"    {i}: {clean_child[:32]}...\")\n",
    "else:\n",
    "    print(f\"✗ Pattern not found or error: {inner.get('status', 'unknown')}\")\n",
    "    if 'error' in inner:\n",
    "        print(f\"  Error: {inner['error']}\")\n",
    "\n",
    "# ===== KATO API Response Structure =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KATO API Response Structure:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Full result keys: {list(result.keys())}\")\n",
    "if 'pattern' in result:\n",
    "    print(f\"Inner pattern keys: {list(inner.keys())}\")\n",
    "    if 'pattern' in inner:\n",
    "        print(f\"Pattern data keys: {list(pattern_data.keys())}\")\n",
    "    if 'metadata' in inner:\n",
    "        print(f\"Metadata keys: {list(metadata.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pattern Unraveling Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test unraveling a specific pattern with verbose output\n",
    "# This is useful for understanding the recursive unraveling process\n",
    "# Pattern name can be with or without 'PTRN|' prefix\n",
    "\n",
    "pattern_name = \"PTRN|04f57fef3b3f9b0f33b9d04987b16f56755d8c63\"  # <-- Put a pattern name from a higher level here\n",
    "level = 3  # Which level (1, 2, or 3 to see recursion)\n",
    "\n",
    "# pattern_name = \"PTRN|811d57463ca70786022e901064c3722794296a24\"\n",
    "# level = 2\n",
    "\n",
    "# pattern_name = \"PTRN|d1a599707a223c611f6547a56077af442f9e1e22\"\n",
    "# level = 1\n",
    "\n",
    "pattern_name = \"PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd\"\n",
    "level = 0\n",
    "\n",
    "print(f\"Unraveling pattern from node{level}:\")\n",
    "print(f\"Input: {pattern_name}\")\n",
    "print(f\"Clean name: {strip_pattern_prefix(pattern_name)}\")\n",
    "\n",
    "tokens = unravel_pattern(\n",
    "    pattern_name,  # Can include 'PTRN|' prefix - will be stripped automatically\n",
    "    level=level,\n",
    "    nodes=nodes,\n",
    "    verbose=True,  # Show detailed unraveling steps\n",
    "    indent=0\n",
    ")\n",
    "\n",
    "print(f\"\\nResult: {len(tokens)} tokens\")\n",
    "print(f\"Text: {' '.join(tokens[:50])}...\")  # Show first 50 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Close all connections when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Close KATO client connections\n",
    "# for i, node in enumerate(nodes):\n",
    "#     node.close()\n",
    "#     print(f\"✓ Closed node{i}\")\n",
    "\n",
    "# print(\"\\n✓ All KATO connections closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What you learned in this notebook:**\n",
    "\n",
    "1. **KATO API Usage**:\n",
    "   - `clear_stm()` - Clear short-term memory\n",
    "   - `observe(strings=[...])` - Send single observation\n",
    "   - `observe_sequence(observations=[...])` - Send batch of observations\n",
    "   - `get_predictions()` - Get predictions from current STM\n",
    "\n",
    "2. **Direct Database Pattern Retrieval**:\n",
    "   - Direct queries from ClickHouse + Redis\n",
    "   - Pattern structure: `pattern_data = [[child1], [child2], ...]`\n",
    "   - Frequency statistics and pattern inspection\n",
    "\n",
    "3. **Hierarchical Generation**:\n",
    "   - **Bottom-up activation**: Input text activates patterns at all levels\n",
    "   - **Top-down unraveling**: High-level patterns recursively expand to tokens\n",
    "   - **Cascading constraints**: Each level constrains the levels below\n",
    "\n",
    "4. **Key Concepts**:\n",
    "   - Each token = one event (KATO ordering constraint)\n",
    "   - Fallback logic for novel inputs\n",
    "   - Recursive pattern unraveling\n",
    "   - Faithful generation (patterns from training data)\n",
    "\n",
    "**Next Steps**:\n",
    "- Experiment with different inputs\n",
    "- Adjust `CHUNK_SIZE` and see how it affects results\n",
    "- Try generation from different hierarchical levels\n",
    "- Inspect patterns to understand what the system learned\n",
    "\n",
    "---\n",
    "\n",
    "**Educational Goal Achieved**: You now understand how to use KATO's API for hierarchical text generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
