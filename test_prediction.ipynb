{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies if not already installed\n",
    "# Uncomment these lines if running for the first time:\n",
    "!pip install clickhouse-connect redis datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from tools.kato_client import KATOClient\n",
    "from transformers import AutoTokenizer\n",
    "from pymongo import MongoClient\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json\n",
    "\n",
    "# Import ClickHouse and Redis utilities from local copy\n",
    "# These are portable copies of KATO storage utilities\n",
    "from tools.kato_storage import (\n",
    "    get_clickhouse_client,\n",
    "    get_redis_client,\n",
    "    ClickHouseWriter,\n",
    "    RedisWriter\n",
    ")\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Chunk sizes: [8, 8, 8, 8]\n",
      "  Max predictions: [10, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - MODIFY THESE TO MATCH YOUR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "# Hierarchical configuration (MUST match training!)\n",
    "# chunk_sizes: How many tokens/patterns to process at each level\n",
    "# max_predictions: How many predictions in each ensemble sent to next level\n",
    "CHUNK_SIZES = [8, 8, 8, 8]      # [node0, node1, node2, node3]\n",
    "MAX_PREDICTIONS = [10, 10, 10, 10]  # [node0, node1, node2, node3]\n",
    "\n",
    "# Tokenizer (must match training)\n",
    "TOKENIZER_NAME = \"gpt2\"  # Options: \"gpt2\", \"bert-base-uncased\", \"roberta-base\", etc.\n",
    "\n",
    "# Recall threshold (pattern matching strictness)\n",
    "# Range: 0.0-1.0\n",
    "#   0.1 = permissive (many matches, lower quality)\n",
    "#   0.9 = strict (few matches, higher quality)\n",
    "# Default: 0.6 (balanced)\n",
    "RECALL_THRESHOLD_DEFAULT = 0.6\n",
    "\n",
    "# KATO server URL\n",
    "BASE_URL = \"http://kato:8000\"\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Chunk sizes: {CHUNK_SIZES}\")\n",
    "print(f\"  Max predictions: {MAX_PREDICTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing KATO clients...\n",
      "\n",
      "✓ All KATO clients ready for generation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KATO CLIENT INITIALIZATION - ONE PER NODE\n",
    "# ============================================================================\n",
    "\n",
    "# Create separate KATO clients for each hierarchical level\n",
    "# max_pattern_length=0: Prediction mode (no auto-learning)\n",
    "# recall_threshold: Pattern matching strictness\n",
    "# max_predictions: Number of predictions per ensemble (KATO config)\n",
    "# process_predictions=True: MUST enable predictions (may be disabled from training)\n",
    "\n",
    "print(\"Initializing KATO clients...\")\n",
    "\n",
    "# recall_threshold controls pattern matching strictness:\n",
    "#   High (0.7-0.9): Strict matching, fewer but higher-quality predictions\n",
    "#   Medium (0.4-0.6): Balanced (default: 0.6)\n",
    "#   Low (0.1-0.3): Permissive matching, more predictions (useful for novel inputs)\n",
    "#\n",
    "# max_predictions controls ensemble size:\n",
    "#   - KATO returns top N predictions per call\n",
    "#   - Entire ensemble sent as ONE event to next level\n",
    "#   - KATO's pattern matching handles missing/extra symbols gracefully\n",
    "#   - Higher values: more context but slower, potentially noisy\n",
    "#   - Lower values: faster but may miss important patterns\n",
    "#\n",
    "# process_predictions=False:\n",
    "#   - We set this to False so that every event doesn't trigger prediction processing.\n",
    "#   - The get_predictions API call will trigger processing when we need it.\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "node0 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    # Node identifiers (MUST match your training configuration)\n",
    "    # Check your MongoDB databases to find the correct node_ids\n",
    "    # Format: {node_id}_kato databases in MongoDB\n",
    "    node_id=\"node0\",\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=RECALL_THRESHOLD_DEFAULT,\n",
    "    max_predictions=MAX_PREDICTIONS[0],\n",
    "    process_predictions=False,\n",
    "    timeout=120\n",
    ")\n",
    "\n",
    "print(\"\\n✓ All KATO clients ready for generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_pattern_length': 0,\n",
       " 'persistence': 5,\n",
       " 'recall_threshold': 0.6,\n",
       " 'indexer_type': 'VI',\n",
       " 'max_predictions': 10,\n",
       " 'sort_symbols': True,\n",
       " 'process_predictions': False,\n",
       " 'use_token_matching': True,\n",
       " 'stm_mode': 'CLEAR',\n",
       " 'rank_sort_algo': 'potential',\n",
       " 'filter_pipeline': [],\n",
       " 'length_min_ratio': 0.5,\n",
       " 'length_max_ratio': 2.0,\n",
       " 'jaccard_threshold': 0.3,\n",
       " 'jaccard_min_overlap': 2,\n",
       " 'minhash_threshold': 0.7,\n",
       " 'minhash_bands': 20,\n",
       " 'minhash_rows': 5,\n",
       " 'minhash_num_hashes': 100,\n",
       " 'bloom_false_positive_rate': 0.01,\n",
       " 'max_candidates_per_stage': 100000,\n",
       " 'enable_filter_metrics': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get session configuration (replaces get_gene)\n",
    "node0.get_session_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Pipeline Algos\n",
    "\n",
    "\n",
    "Performance Comparison (1.2M patterns)\n",
    "\n",
    "  | Filter Pipeline        | Database Candidates      | Python Candidates | Est. Time | Accuracy |\n",
    "  |------------------------|--------------------------|-------------------|-----------|----------|\n",
    "  | [] (empty)             | 1,200,000                | 1,200,000         | TIMEOUT   | 100%     |\n",
    "  | [\"length\"]             | ~500,000 (not effective) | ~500,000          | Minutes   | 100%     |\n",
    "  | [\"jaccard\"]            | ~10,000-50,000           | ~10,000-50,000    | 5-30s     | 100%     |\n",
    "  | [\"minhash\"]            | ~1,000                   | ~1,000            | <1s       | ~98%     |\n",
    "  | [\"minhash\", \"jaccard\"] | ~500                     | ~500              | <1s       | ~98%     |\n",
    "\n",
    "  ---\n",
    "  Answer to Your Question\n",
    "\n",
    "  For your dataset (1.2M patterns, similar lengths):\n",
    "\n",
    "  1. Most performant: [\"minhash\"] - designed for billion-scale, handles 1.2M easily\n",
    "  2. Most accurate: [\"jaccard\", \"rapidfuzz\"] - exact matching, reasonable speed\n",
    "  3. Best balance: [\"minhash\"] with minhash_threshold: 0.5 - fast + accurate enough\n",
    "\n",
    "  Try Option 1 (MinHash only) first. It's specifically designed for this scenario.\n",
    "\n",
    "  Recommended Configuration\n",
    "\n",
    "  For your 1.2M pattern dataset with similar lengths:\n",
    "\n",
    "\n",
    "  {\n",
    "      \"filter_pipeline\": [\"minhash\"],\n",
    "      \"minhash_threshold\": 0.5,      # Lower = more recall (try 0.4-0.6)\n",
    "      \"recall_threshold\": 0.3,       # Final similarity threshold\n",
    "      \"enable_filter_metrics\": True\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'okay',\n",
       " 'message': 'Configuration updated',\n",
       " 'session_id': 'session-d660c2970736467a8b072fe3f8fa8c9c-1764770975190'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: rank_sort_algo may not be available as a session config parameter\n",
    "# Available session configs: max_pattern_length, persistence, recall_threshold, \n",
    "#                           stm_mode, indexer_type, max_predictions, sort_symbols,\n",
    "#                           process_predictions, use_token_matching\n",
    "\n",
    "node0.update_session_config({\n",
    "      'filter_pipeline': ['jaccard'],\n",
    "      'jaccard_threshold': 0.3,      # Token set overlap\n",
    "      'jaccard_min_overlap': 2,      # At least 2 shared tokens\n",
    "      # 'minhash_threshold': 0.1,      # Lower = more recall (try 0.4-0.6)\n",
    "      'recall_threshold': 0.3,       # Sequence similarity\n",
    "      'max_predictions': 10,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node0.check_session_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node0.get_session_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node0.clear_stm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stm': [],\n",
       " 'session_id': 'session-d660c2970736467a8b072fe3f8fa8c9c-1764770975190',\n",
       " 'length': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_stm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STM: [['ĠAmong'], ['Ġfl'], ['ukes'], [','], ['Ġthe'], ['Ġmost'], ['Ġcommon']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Test patterns known to be in KB\n",
    "# Chunks (12) with chunk_size=8:\n",
    "#   Chunk 0: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']  PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd\n",
    "#   Chunk 1: ['ĠNorth', 'ĠAmerican', 'Ġwolves', 'Ġis', 'ĠAl', 'aria', ',', 'Ġwhich'] PTRN|b9f48713626a92cd5c533ea1bafea4c1ac50bbf2\n",
    "#   Chunk 2: ['Ġinfect', 's', 'Ġsmall', 'Ġrodents', 'Ġand', 'Ġamphib', 'ians', 'Ġthat'] PTRN|7729f0ed56a13a9373fc1b1c17e34f61d4512ab4\n",
    "#   Chunk 3: ['Ġare', 'Ġeaten', 'Ġby', 'Ġwolves', '.', 'ĠUpon', 'Ġreaching', 'Ġmaturity'] PTRN|017e91585ea0db850a387c8756f67da5c1510254\n",
    "#   Chunk 4: [',', 'ĠAl', 'aria', 'Ġmigr', 'ates', 'Ġto', 'Ġthe', 'Ġwolf']\n",
    "#   Chunk 5: [\"'s\", 'Ġintestine', ',', 'Ġbut', 'Ġharms', 'Ġit', 'Ġlittle', '.']\n",
    "#   Chunk 6: ['ĠMet', 'or', 'ch', 'is', 'Ġconj', 'unct', 'us', ',']\n",
    "#   Chunk 7: ['Ġwhich', 'Ġenters', 'Ġwolves', 'Ġthrough', 'Ġeating', 'Ġfish', ',', 'Ġinfect']\n",
    "#   Chunk 8: ['s', 'Ġthe', 'Ġwolf', \"'s\", 'Ġliver', 'Ġor', 'Ġgall', 'Ġbladder']\n",
    "#   Chunk 9: [',', 'Ġcausing', 'Ġliver', 'Ġdisease', ',', 'Ġinflammation', 'Ġof', 'Ġthe']\n",
    "#   Chunk 10: ['Ġpanc', 're', 'as', ',', 'Ġand', 'Ġem', 'ac', 'iation']\n",
    "#   Chunk 11: ['.', 'ĠMost', 'Ġother', 'Ġfl', 'uke', 'Ġspecies']\n",
    "\n",
    "chunk = ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin'] \n",
    "# chunk = ['ĠNorth', 'ĠAmerican', 'Ġwolves', 'Ġis', 'ĠAl', 'aria', ',', 'Ġwhich']\n",
    "# chunk = ['Ġinfect', 's', 'Ġsmall', 'Ġrodents', 'Ġand', 'Ġamphib', 'ians', 'Ġthat']\n",
    "\n",
    "\n",
    "# # Clear STM and observe your test pattern\n",
    "node0.clear_stm()\n",
    "node0.observe(strings=[chunk[0]])\n",
    "node0.observe(strings=[chunk[1]])\n",
    "node0.observe(strings=[chunk[2]])\n",
    "node0.observe(strings=[chunk[3]])\n",
    "node0.observe(strings=[chunk[4]])\n",
    "node0.observe(strings=[chunk[5]])\n",
    "node0.observe(strings=[chunk[6]])\n",
    "# node0.observe(strings=[chunk[7]])\n",
    "\n",
    "\n",
    "# Verify STM\n",
    "stm = node0.get_stm()\n",
    "print(f\"STM: {stm['stm']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 predictions\n",
      "==========\n",
      "PTRN|acf6699f0326787929d6b615d7c744571ca0cff4\n",
      "matches: ['ĠAmong', 'Ġfl', 'ukes', 'Ġthe', 'Ġmost', 'Ġcommon']\n",
      "anomalies: - [[], [], [], ['Ġ,'], [], [], []]  + [[], [], [], [','], [], [], []]\n",
      "past: [['Ġto'], ['Ġmisc'], ['arry'], ['Ġthan'], ['Ġthose'], ['Ġnot'], ['Ġinfected'], ['Ġ.']]\n",
      "present: [['ĠAmong'], ['Ġfl'], ['ukes'], ['Ġ,'], ['Ġthe'], ['Ġmost'], ['Ġcommon']]\n",
      "future: []\n",
      "metrics:\n",
      "    frequency: 1\n",
      "    potential: 2.2932542141199668\n",
      "    normalized_potential: 1.0\n",
      "    similarity: 0.5454545454545454 \n",
      "    confidence: 0.8571428571428571 \n",
      "    evidence: 0.4 \n",
      "    predictive_information: 0.0\n",
      "    itfdf_similarity: 0.729517950383703\n",
      "    pattern_probability: 1.0\n",
      "    weighted_strength: 0.5454545454545454\n",
      "    fragmentation: 1.0\n",
      "    snr: 0.8461538461538461\n",
      "    entropy: 2.807354922057604\n",
      "    normalized_entropy: 0.18405623154276796\n",
      "    global_normalized_entropy: 0.039687079738586616\n",
      "    bayesian_posterior: 1.0\n",
      "    bayesian_prior: 1.0\n",
      "    bayesian_likelihood: 0.5454545454545454\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "preds = node0.get_predictions()['predictions']\n",
    "print(f\"There are {len(preds)} predictions\")\n",
    "\n",
    "total_potential = sum([p['potential'] for p in preds])\n",
    "for p in preds:\n",
    "    print(\"=\"*10)\n",
    "    print(f\"PTRN|{p['name']}\")\n",
    "    print(f\"matches: {p['matches']}\")\n",
    "    print(f\"anomalies: - {p['missing']}  + {p['extras']}\")\n",
    "    print(f\"past: {p['past']}\")\n",
    "    print(f\"present: {p['present']}\")\n",
    "    print(f\"future: {p['future']}\")\n",
    "    print(f'''metrics:\n",
    "    frequency: {p['frequency']}\n",
    "    potential: {p['potential']}\n",
    "    normalized_potential: {p['potential']/total_potential}\n",
    "    similarity: {p['similarity']} \n",
    "    confidence: {p['confidence']} \n",
    "    evidence: {p['evidence']} \n",
    "    predictive_information: {p['predictive_information']}\n",
    "    itfdf_similarity: {p['itfdf_similarity']}\n",
    "    pattern_probability: {p['pattern_probability']}\n",
    "    weighted_strength: {p['weighted_strength']}\n",
    "    fragmentation: {p['fragmentation']}\n",
    "    snr: {p['snr']}\n",
    "    entropy: {p['entropy']}\n",
    "    normalized_entropy: {p['normalized_entropy']}\n",
    "    global_normalized_entropy: {p['global_normalized_entropy']}\n",
    "    bayesian_posterior: {p['bayesian_posterior']}\n",
    "    bayesian_prior: {p['bayesian_prior']}\n",
    "    bayesian_likelihood: {p['bayesian_likelihood']}\n",
    "    \n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'prototypical',\n",
       " 'name': 'acf6699f0326787929d6b615d7c744571ca0cff4',\n",
       " 'frequency': 1,\n",
       " 'emotives': {},\n",
       " 'matches': ['ĠAmong', 'Ġfl', 'ukes', 'Ġthe', 'Ġmost', 'Ġcommon'],\n",
       " 'past': [['Ġto'],\n",
       "  ['Ġmisc'],\n",
       "  ['arry'],\n",
       "  ['Ġthan'],\n",
       "  ['Ġthose'],\n",
       "  ['Ġnot'],\n",
       "  ['Ġinfected'],\n",
       "  ['Ġ.']],\n",
       " 'present': [['ĠAmong'],\n",
       "  ['Ġfl'],\n",
       "  ['ukes'],\n",
       "  ['Ġ,'],\n",
       "  ['Ġthe'],\n",
       "  ['Ġmost'],\n",
       "  ['Ġcommon']],\n",
       " 'missing': [[], [], [], ['Ġ,'], [], [], []],\n",
       " 'extras': [[], [], [], [','], [], [], []],\n",
       " 'potential': 2.2932542141199668,\n",
       " 'evidence': 0.4,\n",
       " 'similarity': 0.5454545454545454,\n",
       " 'fragmentation': 1.0,\n",
       " 'snr': 0.8461538461538461,\n",
       " 'confluence': 4.007084525440979e-06,\n",
       " 'predictive_information': 0.0,\n",
       " 'sequence': [['Ġto'],\n",
       "  ['Ġmisc'],\n",
       "  ['arry'],\n",
       "  ['Ġthan'],\n",
       "  ['Ġthose'],\n",
       "  ['Ġnot'],\n",
       "  ['Ġinfected'],\n",
       "  ['Ġ.'],\n",
       "  ['ĠAmong'],\n",
       "  ['Ġfl'],\n",
       "  ['ukes'],\n",
       "  ['Ġ,'],\n",
       "  ['Ġthe'],\n",
       "  ['Ġmost'],\n",
       "  ['Ġcommon']],\n",
       " 'future': [],\n",
       " 'confidence': 0.8571428571428571,\n",
       " 'entropy': 2.807354922057604,\n",
       " 'normalized_entropy': 0.18405623154276796,\n",
       " 'global_normalized_entropy': 0.039687079738586616,\n",
       " 'itfdf_similarity': 0.729517950383703,\n",
       " 'tfidf_score': 1.2069848643118486,\n",
       " 'pattern_probability': 1.0,\n",
       " 'weighted_strength': 0.5454545454545454,\n",
       " 'bayesian_posterior': 1.0,\n",
       " 'bayesian_prior': 1.0,\n",
       " 'bayesian_likelihood': 0.5454545454545454}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pattern': {'status': 'okay',\n",
       "  'pattern': {'name': 'PTRN|acf6699f0326787929d6b615d7c744571ca0cff4',\n",
       "   'pattern_data': [['Ġto'],\n",
       "    ['Ġmisc'],\n",
       "    ['arry'],\n",
       "    ['Ġthan'],\n",
       "    ['Ġthose'],\n",
       "    ['Ġnot'],\n",
       "    ['Ġinfected'],\n",
       "    ['Ġ.'],\n",
       "    ['ĠAmong'],\n",
       "    ['Ġfl'],\n",
       "    ['ukes'],\n",
       "    ['Ġ,'],\n",
       "    ['Ġthe'],\n",
       "    ['Ġmost'],\n",
       "    ['Ġcommon']],\n",
       "   'length': 15,\n",
       "   'frequency': 1,\n",
       "   'emotives': [],\n",
       "   'metadata': {}}},\n",
       " 'node_id': 'node0_kato'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_pattern(\"PTRN|acf6699f0326787929d6b615d7c744571ca0cff4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Database Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE CONNECTIONS - MongoDB + ClickHouse + Redis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Connecting to databases...\")\n",
    "\n",
    "# Store in list for easy access by level index\n",
    "dbs = [db_node0]\n",
    "\n",
    "# ===== ClickHouse Connection (pattern data) =====\n",
    "clickhouse_client = get_clickhouse_client()\n",
    "if clickhouse_client:\n",
    "    clickhouse_writer = ClickHouseWriter(kb_id='node0_kato', clickhouse_client=clickhouse_client)\n",
    "    print(f\"  ✓ Connected to ClickHouse (kb_id='node0_kato')\")\n",
    "else:\n",
    "    clickhouse_writer = None\n",
    "    print(f\"  ⚠️  ClickHouse not available\")\n",
    "\n",
    "# ===== Redis Connection (metadata, frequency, emotives) =====\n",
    "redis_client = get_redis_client()\n",
    "if redis_client:\n",
    "    redis_writer = RedisWriter(kb_id='node0_kato', redis_client=redis_client)\n",
    "    print(f\"  ✓ Connected to Redis (kb_id='node0_kato')\")\n",
    "else:\n",
    "    redis_writer = None\n",
    "    print(f\"  ⚠️  Redis not available\")\n",
    "\n",
    "# ===== Pattern Counts =====\n",
    "if clickhouse_writer:\n",
    "    ch_count = clickhouse_writer.count_patterns()\n",
    "    print(f\"  ClickHouse (node0): {ch_count:,} patterns\")\n",
    "else:\n",
    "    print(f\"  ClickHouse (node0): N/A (not connected)\")\n",
    "\n",
    "if redis_writer:\n",
    "    redis_count = redis_writer.count_patterns()\n",
    "    print(f\"  Redis (node0):      {redis_count:,} patterns\")\n",
    "else:\n",
    "    print(f\"  Redis (node0):      N/A (not connected)\")\n",
    "\n",
    "print(\"\\n✓ Database connections ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PATTERN INSPECTION - ClickHouse + Redis Hybrid Query\n",
    "# ============================================================================\n",
    "\n",
    "level0_pattern_name = \"7729f0ed56a13a9373fc1b1c17e34f61d4512ab4\" # \"6850d8ef6abf023e778693c4d5d9986db464e5cd\"  \n",
    "## Expected: ['ĠAmong', 'Ġfl', 'ukes', 'Ġ,', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']\n",
    "## Translation: ' Among flukes , the most common in...'\n",
    "\n",
    "print(f\"Fetching pattern: {level0_pattern_name}\\n\")\n",
    "\n",
    "# ===== ClickHouse + Redis (hybrid) =====\n",
    "print(\"2️⃣  ClickHouse + Redis (hybrid architecture):\")\n",
    "\n",
    "if clickhouse_writer and redis_writer:\n",
    "    # Fetch pattern data from ClickHouse\n",
    "    pattern_ch = clickhouse_writer.get_pattern_data(level0_pattern_name)\n",
    "    \n",
    "    # Fetch metadata from Redis\n",
    "    pattern_redis = redis_writer.get_metadata(level0_pattern_name)\n",
    "    \n",
    "    if pattern_ch:\n",
    "        # Combine data from both sources (mimics MongoDB document structure)\n",
    "        pattern_doc = {\n",
    "            'name': level0_pattern_name,\n",
    "            'pattern_data': pattern_ch['pattern_data'],\n",
    "            'length': pattern_ch['length'],\n",
    "            'frequency': pattern_redis.get('frequency', 0),\n",
    "            'emotives': pattern_redis.get('emotives', {}),\n",
    "            'metadata': pattern_redis.get('metadata', {})\n",
    "        }\n",
    "        \n",
    "        print(f\"   ✓ Found in ClickHouse + Redis\")\n",
    "        print(f\"   Pattern data (ClickHouse): {pattern_doc['pattern_data']}\")\n",
    "        print(f\"   Length: {pattern_doc['length']}\")\n",
    "        print(f\"   Frequency (Redis): {pattern_doc['frequency']}\")\n",
    "        if pattern_doc['emotives']:\n",
    "            print(f\"   Emotives (Redis): {pattern_doc['emotives']}\")\n",
    "        if pattern_doc['metadata']:\n",
    "            print(f\"   Metadata (Redis): {pattern_doc['metadata']}\")\n",
    "    else:\n",
    "        print(f\"   ✗ Not found in ClickHouse\")\n",
    "        pattern_doc = None\n",
    "else:\n",
    "    print(f\"   ⚠️  Hybrid architecture not available (ClickHouse or Redis not connected)\")\n",
    "    pattern_doc = pattern_doc_mongo  # Fallback to MongoDB\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(pattern_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Close all connections when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close KATO client connections\n",
    "for i, node in enumerate(nodes):\n",
    "    node.close()\n",
    "    print(f\"✓ Closed node{i}\")\n",
    "\n",
    "\n",
    "print(\"\\n✓ All connections closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
