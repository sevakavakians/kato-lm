{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clickhouse-connect\n",
      "  Downloading clickhouse_connect-0.10.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting redis\n",
      "  Downloading redis-7.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting datasketch\n",
      "  Downloading datasketch-1.9.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2023.7.22)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2.0.7)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (2023.3.post1)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (0.21.0)\n",
      "Requirement already satisfied: lz4 in /opt/conda/lib/python3.11/site-packages (from clickhouse-connect) (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.11/site-packages (from datasketch) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from datasketch) (1.11.3)\n",
      "Downloading clickhouse_connect-0.10.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading redis-7.1.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.2/354.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasketch-1.9.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: redis, clickhouse-connect, datasketch\n",
      "Successfully installed clickhouse-connect-0.10.0 datasketch-1.9.0 redis-7.1.0\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies if not already installed\n",
    "# Uncomment these lines if running for the first time:\n",
    "!pip install clickhouse-connect redis datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from tools.kato_client import KATOClient\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import json\n",
    "\n",
    "# Import ClickHouse and Redis utilities from local copy\n",
    "# These are portable copies of KATO storage utilities\n",
    "from tools.kato_storage import (\n",
    "    get_clickhouse_client,\n",
    "    get_redis_client,\n",
    "    ClickHouseWriter,\n",
    "    RedisWriter\n",
    ")\n",
    "\n",
    "print(\"✓ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Chunk sizes: [8, 8, 8, 8]\n",
      "  Max predictions: [10, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - MODIFY THESE TO MATCH YOUR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "# Hierarchical configuration (MUST match training!)\n",
    "# chunk_sizes: How many tokens/patterns to process at each level\n",
    "# max_predictions: How many predictions in each ensemble sent to next level\n",
    "CHUNK_SIZES = [8, 8, 8, 8]      # [node0, node1, node2, node3]\n",
    "MAX_PREDICTIONS = [10, 10, 10, 10]  # [node0, node1, node2, node3]\n",
    "\n",
    "# Tokenizer (must match training)\n",
    "TOKENIZER_NAME = \"gpt2\"  # Options: \"gpt2\", \"bert-base-uncased\", \"roberta-base\", etc.\n",
    "\n",
    "# Recall threshold (pattern matching strictness)\n",
    "# Range: 0.0-1.0\n",
    "#   0.1 = permissive (many matches, lower quality)\n",
    "#   0.9 = strict (few matches, higher quality)\n",
    "# Default: 0.6 (balanced)\n",
    "RECALL_THRESHOLD_DEFAULT = 0.6\n",
    "\n",
    "# KATO server URL\n",
    "BASE_URL = \"http://kato:8000\"\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Chunk sizes: {CHUNK_SIZES}\")\n",
    "print(f\"  Max predictions: {MAX_PREDICTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing KATO clients...\n",
      "\n",
      "✓ All KATO clients ready for generation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KATO CLIENT INITIALIZATION - ONE PER NODE\n",
    "# ============================================================================\n",
    "\n",
    "# Create separate KATO clients for each hierarchical level\n",
    "# max_pattern_length=0: Prediction mode (no auto-learning)\n",
    "# recall_threshold: Pattern matching strictness\n",
    "# max_predictions: Number of predictions per ensemble (KATO config)\n",
    "# process_predictions=True: MUST enable predictions (may be disabled from training)\n",
    "\n",
    "print(\"Initializing KATO clients...\")\n",
    "\n",
    "# recall_threshold controls pattern matching strictness:\n",
    "#   High (0.7-0.9): Strict matching, fewer but higher-quality predictions\n",
    "#   Medium (0.4-0.6): Balanced (default: 0.6)\n",
    "#   Low (0.1-0.3): Permissive matching, more predictions (useful for novel inputs)\n",
    "#\n",
    "# max_predictions controls ensemble size:\n",
    "#   - KATO returns top N predictions per call\n",
    "#   - Entire ensemble sent as ONE event to next level\n",
    "#   - KATO's pattern matching handles missing/extra symbols gracefully\n",
    "#   - Higher values: more context but slower, potentially noisy\n",
    "#   - Lower values: faster but may miss important patterns\n",
    "#\n",
    "# process_predictions=False:\n",
    "#   - We set this to False so that every event doesn't trigger prediction processing.\n",
    "#   - The get_predictions API call will trigger processing when we need it.\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "node0 = KATOClient(\n",
    "    base_url=BASE_URL,\n",
    "    # Node identifiers (MUST match your training configuration)\n",
    "    # Check your MongoDB databases to find the correct node_ids\n",
    "    # Format: {node_id}_kato databases in MongoDB\n",
    "    node_id=\"node0\",\n",
    "    max_pattern_length=0,\n",
    "    recall_threshold=RECALL_THRESHOLD_DEFAULT,\n",
    "    max_predictions=MAX_PREDICTIONS[0],\n",
    "    process_predictions=False,\n",
    "    timeout=1200\n",
    ")\n",
    "\n",
    "print(\"\\n✓ All KATO clients ready for generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_pattern_length': 0,\n",
       " 'persistence': 5,\n",
       " 'recall_threshold': 0.6,\n",
       " 'indexer_type': 'VI',\n",
       " 'max_predictions': 10,\n",
       " 'sort_symbols': True,\n",
       " 'process_predictions': False,\n",
       " 'use_token_matching': True,\n",
       " 'stm_mode': 'CLEAR',\n",
       " 'rank_sort_algo': 'potential',\n",
       " 'filter_pipeline': [],\n",
       " 'length_min_ratio': 0.5,\n",
       " 'length_max_ratio': 2.0,\n",
       " 'jaccard_threshold': 0.3,\n",
       " 'jaccard_min_overlap': 2,\n",
       " 'minhash_threshold': 0.7,\n",
       " 'minhash_bands': 20,\n",
       " 'minhash_rows': 5,\n",
       " 'minhash_num_hashes': 100,\n",
       " 'bloom_false_positive_rate': 0.01,\n",
       " 'max_candidates_per_stage': 100000,\n",
       " 'enable_filter_metrics': True}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get session configuration\n",
    "node0.get_session_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Pipeline Algos\n",
    "\n",
    "\n",
    "Performance Comparison (1.2M patterns)\n",
    "\n",
    "  | Filter Pipeline        | Database Candidates      | Python Candidates | Est. Time | Accuracy |\n",
    "  |------------------------|--------------------------|-------------------|-----------|----------|\n",
    "  | [] (empty)             | 1,200,000                | 1,200,000         | TIMEOUT   | 100%     |\n",
    "  | [\"length\"]             | ~500,000 (not effective) | ~500,000          | Minutes   | 100%     |\n",
    "  | [\"jaccard\"]            | ~10,000-50,000           | ~10,000-50,000    | 5-30s     | 100%     |\n",
    "  | [\"minhash\"]            | ~1,000                   | ~1,000            | <1s       | ~98%     |\n",
    "  | [\"minhash\", \"jaccard\"] | ~500                     | ~500              | <1s       | ~98%     |\n",
    "\n",
    "  ---\n",
    "  Answer to Your Question\n",
    "\n",
    "  For your dataset (1.2M patterns, similar lengths):\n",
    "\n",
    "  1. Most performant: [\"minhash\"] - designed for billion-scale, handles 1.2M easily\n",
    "  2. Most accurate: [\"jaccard\", \"rapidfuzz\"] - exact matching, reasonable speed\n",
    "  3. Best balance: [\"minhash\"] with minhash_threshold: 0.5 - fast + accurate enough\n",
    "\n",
    "  Try Option 1 (MinHash only) first. It's specifically designed for this scenario.\n",
    "\n",
    "  Recommended Configuration\n",
    "\n",
    "  For your 1.2M pattern dataset with similar lengths:\n",
    "\n",
    "\n",
    "  {\n",
    "      \"filter_pipeline\": [\"minhash\"],\n",
    "      \"minhash_threshold\": 0.5,      # Lower = more recall (try 0.4-0.6)\n",
    "      \"recall_threshold\": 0.3,       # Final similarity threshold\n",
    "      \"enable_filter_metrics\": True\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'okay',\n",
       " 'message': 'Configuration updated',\n",
       " 'session_id': 'session-f1beae1adc06413eb1ceec702dfdc3e5-1770042201595'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: rank_sort_algo may not be available as a session config parameter\n",
    "# Available session configs: max_pattern_length, persistence, recall_threshold, \n",
    "#                           stm_mode, indexer_type, max_predictions, sort_symbols,\n",
    "#                           process_predictions, use_token_matching\n",
    "\n",
    "node0.update_session_config({\n",
    "      'use_token_matching': True,  # False = Character-level mode, True = Token-level mode\n",
    "      'filter_pipeline': ['jaccard'],\n",
    "      'jaccard_threshold': 0.3,      # Token set overlap\n",
    "      'jaccard_min_overlap': 2,      # At least 2 shared tokens\n",
    "      # 'minhash_threshold': 0.1,      # Lower = more recall (try 0.4-0.6)\n",
    "      'recall_threshold': 0.3,       # Sequence similarity\n",
    "      'max_predictions': 10,\n",
    "      'max_candidates_per_stage': 1000,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exists': True,\n",
       " 'expired': False,\n",
       " 'session_id': 'session-be3f4144b2b9489f8841fff77fbb14c0-1769178954180'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.check_session_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_id': 'session-f1beae1adc06413eb1ceec702dfdc3e5-1770042201595',\n",
       " 'node_id': 'node0',\n",
       " 'created_at': '2026-02-02T14:23:21.595433Z',\n",
       " 'expires_at': '2026-02-02T15:24:25.473293Z',\n",
       " 'ttl_seconds': 3599,\n",
       " 'metadata': {},\n",
       " 'session_config': {'max_pattern_length': 0,\n",
       "  'persistence': 5,\n",
       "  'recall_threshold': 0.3,\n",
       "  'indexer_type': 'VI',\n",
       "  'max_predictions': 10,\n",
       "  'sort_symbols': True,\n",
       "  'process_predictions': False,\n",
       "  'use_token_matching': True,\n",
       "  'stm_mode': 'CLEAR',\n",
       "  'rank_sort_algo': 'potential',\n",
       "  'filter_pipeline': ['jaccard'],\n",
       "  'length_min_ratio': 0.5,\n",
       "  'length_max_ratio': 2.0,\n",
       "  'jaccard_threshold': 0.3,\n",
       "  'jaccard_min_overlap': 2,\n",
       "  'minhash_threshold': 0.7,\n",
       "  'minhash_bands': 20,\n",
       "  'minhash_rows': 5,\n",
       "  'minhash_num_hashes': 100,\n",
       "  'bloom_false_positive_rate': 0.01,\n",
       "  'max_candidates_per_stage': 1000,\n",
       "  'enable_filter_metrics': True}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_session_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node0.clear_stm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stm': [],\n",
       " 'session_id': 'session-bfd6fb4fbbcb4b46b281dc4c2a68ce2d-1770043204341',\n",
       " 'length': 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_stm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STM: [['ĠAmong'], ['Ġfl'], ['ukes'], [','], ['Ġthe'], ['Ġmost'], ['Ġcommon']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Test patterns known to be in KB\n",
    "# Chunks (12) with chunk_size=8:\n",
    "#   Chunk 0: ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin']  PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd\n",
    "#   Chunk 1: ['ĠNorth', 'ĠAmerican', 'Ġwolves', 'Ġis', 'ĠAl', 'aria', ',', 'Ġwhich'] PTRN|b9f48713626a92cd5c533ea1bafea4c1ac50bbf2\n",
    "#   Chunk 2: ['Ġinfect', 's', 'Ġsmall', 'Ġrodents', 'Ġand', 'Ġamphib', 'ians', 'Ġthat'] PTRN|7729f0ed56a13a9373fc1b1c17e34f61d4512ab4\n",
    "#   Chunk 3: ['Ġare', 'Ġeaten', 'Ġby', 'Ġwolves', '.', 'ĠUpon', 'Ġreaching', 'Ġmaturity'] PTRN|017e91585ea0db850a387c8756f67da5c1510254\n",
    "#   Chunk 4: [',', 'ĠAl', 'aria', 'Ġmigr', 'ates', 'Ġto', 'Ġthe', 'Ġwolf']\n",
    "#   Chunk 5: [\"'s\", 'Ġintestine', ',', 'Ġbut', 'Ġharms', 'Ġit', 'Ġlittle', '.']\n",
    "#   Chunk 6: ['ĠMet', 'or', 'ch', 'is', 'Ġconj', 'unct', 'us', ',']\n",
    "#   Chunk 7: ['Ġwhich', 'Ġenters', 'Ġwolves', 'Ġthrough', 'Ġeating', 'Ġfish', ',', 'Ġinfect']\n",
    "#   Chunk 8: ['s', 'Ġthe', 'Ġwolf', \"'s\", 'Ġliver', 'Ġor', 'Ġgall', 'Ġbladder']\n",
    "#   Chunk 9: [',', 'Ġcausing', 'Ġliver', 'Ġdisease', ',', 'Ġinflammation', 'Ġof', 'Ġthe']\n",
    "#   Chunk 10: ['Ġpanc', 're', 'as', ',', 'Ġand', 'Ġem', 'ac', 'iation']\n",
    "#   Chunk 11: ['.', 'ĠMost', 'Ġother', 'Ġfl', 'uke', 'Ġspecies']\n",
    "\n",
    "chunk = ['ĠAmong', 'Ġfl', 'ukes', ',', 'Ġthe', 'Ġmost', 'Ġcommon', 'Ġin'] \n",
    "# chunk = ['ĠNorth', 'ĠAmerican', 'Ġwolves', 'Ġis', 'ĠAl', 'aria', ',', 'Ġwhich']\n",
    "# chunk = ['Ġinfect', 's', 'Ġsmall', 'Ġrodents', 'Ġand', 'Ġamphib', 'ians', 'Ġthat']\n",
    "\n",
    "# chunk = [\"Ġleague\",\"Ġminimum\",\"Ġ,\",\"Ġpasing\",\"Ġupt\",\"Ġa\",\"Ġsuperior\",\"Ġcontract\"]\n",
    "\n",
    "# # Clear STM and observe your test pattern\n",
    "node0.clear_stm()\n",
    "node0.observe(strings=[chunk[0]])\n",
    "node0.observe(strings=[chunk[1]])\n",
    "node0.observe(strings=[chunk[2]])\n",
    "node0.observe(strings=[chunk[3]])\n",
    "node0.observe(strings=[chunk[4]])\n",
    "node0.observe(strings=[chunk[5]])\n",
    "node0.observe(strings=[chunk[6]])\n",
    "# node0.observe(strings=[chunk[7]])\n",
    "\n",
    "\n",
    "# Verify STM\n",
    "stm = node0.get_stm()\n",
    "print(f\"STM: {stm['stm']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 predictions\n",
      "==========\n",
      "PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd\n",
      "matches: ['ĠAmong', 'Ġfl', 'ukes', 'Ġthe', 'Ġmost', 'Ġcommon']\n",
      "anomalies: - [[], [], [], ['Ġ,'], [], [], []]  + [[], [], [], [','], [], [], []]\n",
      "past: []\n",
      "present: [['ĠAmong'], ['Ġfl'], ['ukes'], ['Ġ,'], ['Ġthe'], ['Ġmost'], ['Ġcommon']]\n",
      "future: [['Ġin']]\n",
      "metrics:\n",
      "    frequency: 1\n",
      "    potential: 2.6004464772045273\n",
      "    normalized_potential: 1.0\n",
      "    similarity: 0.8 \n",
      "    confidence: 0.8571428571428571 \n",
      "    evidence: 0.75 \n",
      "    predictive_information: 0.8\n",
      "    itfdf_similarity: 0.7405563673144173\n",
      "    pattern_probability: 1.0\n",
      "    weighted_strength: 0.8\n",
      "    fragmentation: 1.0\n",
      "    snr: 0.8461538461538461\n",
      "    entropy: 2.807354922057604\n",
      "    normalized_entropy: 0.18308131158888039\n",
      "    global_normalized_entropy: 0.03882654021718058\n",
      "    bayesian_posterior: 1.0\n",
      "    bayesian_prior: 1.0\n",
      "    bayesian_likelihood: 0.8\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "preds = node0.get_predictions()['predictions']\n",
    "print(f\"There are {len(preds)} predictions\")\n",
    "\n",
    "total_potential = sum([p['potential'] for p in preds])\n",
    "for p in preds:\n",
    "    print(\"=\"*10)\n",
    "    print(f\"PTRN|{p['name']}\")\n",
    "    print(f\"matches: {p['matches']}\")\n",
    "    print(f\"anomalies: - {p['missing']}  + {p['extras']}\")\n",
    "    print(f\"past: {p['past']}\")\n",
    "    print(f\"present: {p['present']}\")\n",
    "    print(f\"future: {p['future']}\")\n",
    "    print(f'''metrics:\n",
    "    frequency: {p['frequency']}\n",
    "    potential: {p['potential']}\n",
    "    normalized_potential: {p['potential']/total_potential}\n",
    "    similarity: {p['similarity']} \n",
    "    confidence: {p['confidence']} \n",
    "    evidence: {p['evidence']} \n",
    "    predictive_information: {p['predictive_information']}\n",
    "    itfdf_similarity: {p['itfdf_similarity']}\n",
    "    pattern_probability: {p['pattern_probability']}\n",
    "    weighted_strength: {p['weighted_strength']}\n",
    "    fragmentation: {p['fragmentation']}\n",
    "    snr: {p['snr']}\n",
    "    entropy: {p['entropy']}\n",
    "    normalized_entropy: {p['normalized_entropy']}\n",
    "    global_normalized_entropy: {p['global_normalized_entropy']}\n",
    "    bayesian_posterior: {p['bayesian_posterior']}\n",
    "    bayesian_prior: {p['bayesian_prior']}\n",
    "    bayesian_likelihood: {p['bayesian_likelihood']}\n",
    "    \n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'prototypical',\n",
       " 'name': '6850d8ef6abf023e778693c4d5d9986db464e5cd',\n",
       " 'frequency': 1,\n",
       " 'emotives': {},\n",
       " 'matches': ['ĠAmong', 'Ġfl', 'ukes', 'Ġthe', 'Ġmost', 'Ġcommon'],\n",
       " 'past': [],\n",
       " 'present': [['ĠAmong'],\n",
       "  ['Ġfl'],\n",
       "  ['ukes'],\n",
       "  ['Ġ,'],\n",
       "  ['Ġthe'],\n",
       "  ['Ġmost'],\n",
       "  ['Ġcommon']],\n",
       " 'missing': [[], [], [], ['Ġ,'], [], [], []],\n",
       " 'extras': [[], [], [], [','], [], [], []],\n",
       " 'anomalies': [],\n",
       " 'potential': 2.6004464772045273,\n",
       " 'evidence': 0.75,\n",
       " 'similarity': 0.8,\n",
       " 'fragmentation': 1.0,\n",
       " 'snr': 0.8461538461538461,\n",
       " 'confluence': 1.2433388123130329e-06,\n",
       " 'predictive_information': 0.8,\n",
       " 'sequence': [['ĠAmong'],\n",
       "  ['Ġfl'],\n",
       "  ['ukes'],\n",
       "  ['Ġ,'],\n",
       "  ['Ġthe'],\n",
       "  ['Ġmost'],\n",
       "  ['Ġcommon'],\n",
       "  ['Ġin']],\n",
       " 'future': [['Ġin']],\n",
       " 'confidence': 0.8571428571428571,\n",
       " 'entropy': 2.807354922057604,\n",
       " 'normalized_entropy': 0.18308131158888039,\n",
       " 'global_normalized_entropy': 0.03882654021718058,\n",
       " 'itfdf_similarity': 0.7405563673144173,\n",
       " 'tfidf_score': 1.3280445895742103,\n",
       " 'pattern_probability': 1.0,\n",
       " 'weighted_strength': 0.8,\n",
       " 'bayesian_posterior': 1.0,\n",
       " 'bayesian_prior': 1.0,\n",
       " 'bayesian_likelihood': 0.8}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pattern': {'status': 'okay',\n",
       "  'pattern': {'name': 'PTRN|fd54bd662706befc7a4b6588f8a2e8903682a451',\n",
       "   'pattern_data': [['Ġvocals'],\n",
       "    ['Ġ,'],\n",
       "    ['ĠEpic'],\n",
       "    ['l'],\n",
       "    ['oud'],\n",
       "    ['Ġappeared'],\n",
       "    ['Ġon'],\n",
       "    ['Ġseveral']],\n",
       "   'length': 8,\n",
       "   'frequency': 2,\n",
       "   'emotives': [],\n",
       "   'metadata': {}}},\n",
       " 'node_id': 'node0_kato'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_pattern(\"PTRN|fd54bd662706befc7a4b6588f8a2e8903682a451\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pattern': {'status': 'okay',\n",
       "  'pattern': {'name': 'PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd',\n",
       "   'pattern_data': [['ĠAmong'],\n",
       "    ['Ġfl'],\n",
       "    ['ukes'],\n",
       "    ['Ġ,'],\n",
       "    ['Ġthe'],\n",
       "    ['Ġmost'],\n",
       "    ['Ġcommon'],\n",
       "    ['Ġin']],\n",
       "   'length': 8,\n",
       "   'frequency': 1,\n",
       "   'emotives': [],\n",
       "   'metadata': {}}},\n",
       " 'node_id': 'node0_kato'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_pattern(\"PTRN|6850d8ef6abf023e778693c4d5d9986db464e5cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pattern': {'status': 'error',\n",
       "  'message': 'Pattern PTRN|acf6699f0326787929d6b615d7c744571ca0cff4 not found'},\n",
       " 'node_id': 'node0_kato'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node0.get_pattern(\"PTRN|acf6699f0326787929d6b615d7c744571ca0cff4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Close all connections when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close KATO client connections\n",
    "for i, node in enumerate(nodes):\n",
    "    node.close()\n",
    "    print(f\"✓ Closed node{i}\")\n",
    "\n",
    "\n",
    "print(\"\\n✓ All connections closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
